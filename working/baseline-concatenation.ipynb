{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6df5f5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n",
      "BEGIN TRAINING...\n",
      "EPOCH  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1001/1001 [00:17<00:00, 56.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  133.52577859609872\n",
      "Running Average TRAIN F1-Score :  0.16652627472396378\n",
      "Running Average VAL Loss :  130.06730086462838\n",
      "Running Average VAL F1-Score :  0.18029049171933106\n",
      "\n",
      "\n",
      "EPOCH  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1001/1001 [00:17<00:00, 58.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  130.43866453828153\n",
      "Running Average TRAIN F1-Score :  0.18519467959454008\n",
      "Running Average VAL Loss :  128.55258069719588\n",
      "Running Average VAL F1-Score :  0.18270099382581456\n",
      "\n",
      "\n",
      "EPOCH  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1001/1001 [00:17<00:00, 56.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  129.3222176916711\n",
      "Running Average TRAIN F1-Score :  0.18816738566020866\n",
      "Running Average VAL Loss :  128.22666393007552\n",
      "Running Average VAL F1-Score :  0.18348183296620846\n",
      "\n",
      "\n",
      "EPOCH  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1001/1001 [00:18<00:00, 54.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  128.55858844524616\n",
      "Running Average TRAIN F1-Score :  0.18859252501855958\n",
      "Running Average VAL Loss :  127.8585694858006\n",
      "Running Average VAL F1-Score :  0.1810517647702779\n",
      "\n",
      "\n",
      "EPOCH  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1001/1001 [00:17<00:00, 56.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  127.88731513085304\n",
      "Running Average TRAIN F1-Score :  0.1869535091427061\n",
      "Running Average VAL Loss :  127.98071098327637\n",
      "Running Average VAL F1-Score :  0.17886019085666963\n",
      "\n",
      "\n",
      "EPOCH  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1001/1001 [00:17<00:00, 57.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  127.34622472101873\n",
      "Running Average TRAIN F1-Score :  0.1847792199769101\n",
      "Running Average VAL Loss :  127.45680856704712\n",
      "Running Average VAL F1-Score :  0.1758612064378602\n",
      "\n",
      "\n",
      "EPOCH  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1001/1001 [00:17<00:00, 57.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  126.81494780091734\n",
      "Running Average TRAIN F1-Score :  0.18301985278115288\n",
      "Running Average VAL Loss :  128.3714177267892\n",
      "Running Average VAL F1-Score :  0.17309936614973204\n",
      "\n",
      "\n",
      "EPOCH  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1001/1001 [00:17<00:00, 56.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  126.34226155924154\n",
      "Running Average TRAIN F1-Score :  0.181399076216883\n",
      "Running Average VAL Loss :  127.6075541632516\n",
      "Running Average VAL F1-Score :  0.17029622962166155\n",
      "\n",
      "\n",
      "EPOCH  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1001/1001 [00:17<00:00, 56.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  124.89644815633585\n",
      "Running Average TRAIN F1-Score :  0.18421163075751476\n",
      "Running Average VAL Loss :  127.03648349217006\n",
      "Running Average VAL F1-Score :  0.1728574221155473\n",
      "\n",
      "\n",
      "EPOCH  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1001/1001 [00:17<00:00, 56.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  124.36522939036061\n",
      "Running Average TRAIN F1-Score :  0.18668044159760128\n",
      "Running Average VAL Loss :  126.93043967655727\n",
      "Running Average VAL F1-Score :  0.17365932052156755\n",
      "\n",
      "\n",
      "TRAINING FINISHED\n",
      "FINAL TRAINING SCORE :  0.18668044159760128\n",
      "FINAL VALIDATION SCORE :  0.17365932052156755\n",
      "FINAL TRAINING MAX F1 SCORE :  0.18859252501855958\n",
      "FINAL VALIDATION MAX F1 SCORE :  0.18348183296620846\n",
      "GENERATE PREDICTION FOR TEST SET...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "141865it [02:50, 830.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTIONS DONE\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pdb\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "from torch.utils.data import Dataset\n",
    "import torch;\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchmetrics.classification import MultilabelF1Score\n",
    "from torchmetrics.classification import MultilabelAccuracy\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "# 1.数据预处理，使用T5（ProtTrans）、ems2、protein与训练模型生成embedding;\n",
    "# 2.配置读取embedding文件 & 构建数据集;\n",
    "MAIN_DIR = \"../input/cafa-5-protein-function-prediction\"\n",
    "\n",
    "class config:\n",
    "    train_labels_path = MAIN_DIR + \"/Train/train_terms.tsv\"\n",
    "    test_sequences_path = MAIN_DIR + \"/Test (Targets)/testsuperset.fasta\"\n",
    "    train_sequences_path = MAIN_DIR  + \"/Train/train_sequences.fasta\"\n",
    "    \n",
    "    num_labels = 500\n",
    "    n_epochs = 10\n",
    "    batch_size = 128\n",
    "    lr = 0.001 \n",
    "    # for Mac\n",
    "    device = torch.device('mps') \n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "    print(device)\n",
    "   \n",
    "# Directories for the different embedding vectors : \n",
    "embeds_map = {\n",
    "    \"T5\" : \"cafa-5-t5-embeddings-numpy\",\n",
    "    \"ProtBERT\" : \"cafa-5-protbert-embeddings-numpy\",\n",
    "    \"EMS2\" : \"cafa-5-ems2-embeddings-numpy\"\n",
    "}\n",
    "\n",
    "# Length of the different embedding vectors :\n",
    "embeds_dim = {\n",
    "    \"T5\" : 1024,\n",
    "    \"ProtBERT\" : 1024,\n",
    "    \"EMS2\" : 1280\n",
    "}\n",
    "\n",
    "class ProteinSequenceDataset(Dataset):\n",
    "    \n",
    "#     def __init__(self, datatype, embeddings_sources):\n",
    "#         super(ProteinSequenceDataset).__init__()\n",
    "#         self.datatype = datatype\n",
    "\n",
    "#         # 加载嵌入并进行拼接\n",
    "#         embeddings_list = []\n",
    "#         for embeddings_source in embeddings_sources:\n",
    "#             embeds_path = f\"../input/{embeds_map[embeddings_source]}/{datatype}_embeddings.npy\"\n",
    "#             embeds = np.load(embeds_path)\n",
    "#             print(f\"{embeddings_source} embeddings shape: {embeds.shape}\")\n",
    "#             embeddings_list.append(embeds)\n",
    "\n",
    "#         # 使用numpy的hstack函数进行水平拼接\n",
    "#         combined_embeds = np.hstack(embeddings_list)\n",
    "\n",
    "#         # 加载IDs\n",
    "#         ids_path = f\"../input/{list(embeds_map.values())[0]}/{datatype}_ids.npy\"  # 假设所有ID文件相同\n",
    "#         ids = np.load(ids_path)\n",
    "\n",
    "#         # 创建包含所有嵌入和ID的DataFrame\n",
    "#         embeds_list = [combined_embeds[i, :] for i in range(combined_embeds.shape[0])]\n",
    "#         self.df = pd.DataFrame(data={\"EntryID\": ids, \"embed\": embeds_list})\n",
    "        \n",
    "#         if datatype == \"train\":\n",
    "#             np_labels = np.load(f\"../input/train-targets-top{config.num_labels}/train_targets_top{config.num_labels}.npy\")\n",
    "#             df_labels = pd.DataFrame(self.df['EntryID'])\n",
    "#             df_labels['labels_vect'] = [row for row in np_labels]\n",
    "#             self.df = self.df.merge(df_labels, on=\"EntryID\")\n",
    "    def __init__(self, datatype, embeddings_sources):\n",
    "        super(ProteinSequenceDataset).__init__()\n",
    "        self.datatype = datatype\n",
    "\n",
    "        # 加载嵌入并进行拼接\n",
    "        embeddings_list = []\n",
    "        for embeddings_source in embeddings_sources:\n",
    "            embeds_path = f\"../input/{embeds_map[embeddings_source]}/{datatype}_embeddings.npy\"\n",
    "            embeds = np.load(embeds_path)\n",
    "            embeddings_list.append(embeds)\n",
    "\n",
    "        # 调整嵌入向量的大小，使它们在拼接维度上具有相同的大小\n",
    "        max_size = max(embed.shape[0] for embed in embeddings_list)\n",
    "        for i, embed in enumerate(embeddings_list):\n",
    "            if embed.shape[0] < max_size:\n",
    "                pad_size = max_size - embed.shape[0]\n",
    "                embeddings_list[i] = np.pad(embed, ((0, pad_size), (0, 0)), mode='constant', constant_values=0)\n",
    "\n",
    "        # 使用numpy的hstack函数进行水平拼接\n",
    "        combined_embeds = np.hstack(embeddings_list)\n",
    "\n",
    "        # 加载IDs\n",
    "        ids_path = f\"../input/{list(embeds_map.values())[0]}/{datatype}_ids.npy\"  # 假设所有ID文件相同\n",
    "        ids = np.load(ids_path)\n",
    "\n",
    "        # 创建包含所有嵌入和ID的DataFrame\n",
    "        embeds_list = [combined_embeds[i, :] for i in range(combined_embeds.shape[0])]\n",
    "        self.df = pd.DataFrame(data={\"EntryID\": ids, \"embed\": embeds_list})\n",
    "\n",
    "        if datatype == \"train\":\n",
    "            np_labels = np.load(f\"../input/train-targets-top{config.num_labels}/train_targets_top{config.num_labels}.npy\")\n",
    "            df_labels = pd.DataFrame(self.df['EntryID'])\n",
    "            df_labels['labels_vect'] = [row for row in np_labels]\n",
    "            self.df = self.df.merge(df_labels, on=\"EntryID\")\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        embed = torch.tensor(self.df.iloc[index][\"embed\"], dtype=torch.float32)\n",
    "        if self.datatype == \"train\":\n",
    "            targets = torch.tensor(self.df.iloc[index][\"labels_vect\"], dtype=torch.float32)\n",
    "            return embed, targets\n",
    "        if self.datatype == \"test\":\n",
    "            id = self.df.iloc[index][\"EntryID\"]\n",
    "            return embed, id\n",
    "\n",
    "\n",
    "# class MultiLayerPerceptron(nn.Module):\n",
    "#     def __init__(self, input_dim, num_classes):\n",
    "#         super(MultiLayerPerceptron, self).__init__()\n",
    "\n",
    "#         # 减少层数\n",
    "#         self.linear1 = nn.Linear(input_dim, 2048)\n",
    "#         self.bn1 = nn.BatchNorm1d(2048)\n",
    "#         self.activation1 = nn.ReLU()\n",
    "#         self.dropout1 = nn.Dropout(0.5)\n",
    "#         self.linear2 = nn.Linear(2048, 1024)\n",
    "#         self.bn2 = nn.BatchNorm1d(1024)\n",
    "#         self.activation2 = nn.ReLU()\n",
    "#         self.dropout2 = nn.Dropout(0.5)\n",
    "#         self.linear3 = nn.Linear(1024, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.linear1(x)\n",
    "#         x = self.bn1(x)\n",
    "#         x = self.activation1(x)\n",
    "#         x = self.dropout1(x)\n",
    "#         x = self.linear2(x)\n",
    "#         x = self.bn2(x)\n",
    "#         x = self.activation2(x)\n",
    "#         x = self.dropout2(x)\n",
    "#         x = self.linear3(x)\n",
    "#         return x\n",
    "\n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "\n",
    "        # 减少层数\n",
    "        self.linear1 = nn.Linear(input_dim, 2048)\n",
    "        self.bn1 = nn.BatchNorm1d(2048)\n",
    "        self.activation1 = nn.GELU()\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.linear2 = nn.Linear(2048, 1024)\n",
    "        self.bn2 = nn.BatchNorm1d(1024)\n",
    "        self.activation2 = nn.GELU()\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.linear3 = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "    \n",
    "# class CNN1D(nn.Module):\n",
    "#     def __init__(self, input_dim, num_classes):\n",
    "#         super(CNN1D, self).__init__()\n",
    "#         # (batch_size, channels, embed_size)  \n",
    "#         # Multi-dialted Layer\n",
    "#         #  某一层：1x3 conv dilate1\n",
    "#         #  某一层：1x3 conv dilate1 + 1x3 conv dilat2 + 1x7 conv dilate1  + 1x7 conv dilate2 \n",
    "#         #  最直观的改进！！同一层有不同的感受野！！！\n",
    "#         self.conv1 = nn.Conv1d(in_channels=1, out_channels=3, kernel_size=3, dilation=1, padding=1, stride=1)\n",
    "#         # (batch_size, 3, embed_size)\n",
    "#         self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "#         # (batch_size, 3, embed_size/2 = 512)\n",
    "#         self.conv2 = nn.Conv1d(in_channels=3, out_channels=8, kernel_size=3, dilation=1, padding=1, stride=1)\n",
    "#         # (batch_size, 8, embed_size/2 = 512)\n",
    "#         self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "#         # (batch_size, 8, embed_size/4 = 256)\n",
    "#         self.fc1 = nn.Linear(in_features=int(8 * input_dim/4), out_features=128)\n",
    "#         self.fc2 = nn.Linear(in_features=128, out_features=num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.reshape(x.shape[0], 1, x.shape[1])\n",
    "#         x = self.pool1(nn.functional.relu(self.conv1(x)))\n",
    "#         x = self.pool2(nn.functional.relu(self.conv2(x)))\n",
    "#         x = torch.flatten(x, 1)\n",
    "#         x = nn.functional.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "###############################################################\n",
    "########## >>>>> 第二层 扩大感受野 <<<<<<  ##########\n",
    "###############################################################\n",
    "class CNN1D(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(CNN1D, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=3, kernel_size=3, dilation=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(3)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv2a = nn.Conv1d(in_channels=3, out_channels=8, kernel_size=3, dilation=1, padding=1)\n",
    "        self.bn2a = nn.BatchNorm1d(8)\n",
    "        self.conv2b = nn.Conv1d(in_channels=3, out_channels=8, kernel_size=5, dilation=1, padding=2)\n",
    "        self.bn2b = nn.BatchNorm1d(8)\n",
    "        self.conv2c = nn.Conv1d(in_channels=3, out_channels=8, kernel_size=3, dilation=2, padding=2)\n",
    "        self.bn2c = nn.BatchNorm1d(8)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=int(8 * input_dim/4), out_features=128)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.shape[0], 1, x.shape[1])\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = F.relu(self.bn2a(self.conv2a(x))) + F.relu(self.bn2b(self.conv2b(x))) + F.relu(self.bn2c(self.conv2c(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "###############################################################\n",
    "########## >>>>> 增加注意力机制 <<<<<<  ##########\n",
    "###############################################################\n",
    "# class AttentionModule(nn.Module):\n",
    "#     def __init__(self, channels):\n",
    "#         super(AttentionModule, self).__init__()\n",
    "#         self.attention_weights = nn.Sequential(\n",
    "#             nn.Conv1d(channels, channels, kernel_size=3, padding=1),  # 可以是更复杂的结构\n",
    "#             nn.ReLU(),\n",
    "#             nn.BatchNorm1d(channels),\n",
    "#             nn.Conv1d(channels, 1, kernel_size=3, padding=1),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         weights = self.attention_weights(x)\n",
    "#         return x * weights\n",
    "\n",
    "# class CNN1D(nn.Module):\n",
    "#     def __init__(self, input_dim, num_classes):\n",
    "#         super(CNN1D, self).__init__()\n",
    "#         self.conv1 = nn.Conv1d(in_channels=1, out_channels=3, kernel_size=3, dilation=1, padding=1, stride=1)\n",
    "#         self.attention1 = AttentionModule(3)\n",
    "#         self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "#         self.conv2 = nn.Conv1d(in_channels=3, out_channels=8, kernel_size=3, dilation=1, padding=1, stride=1)\n",
    "#         self.attention2 = AttentionModule(8)\n",
    "#         self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "#         self.fc1 = nn.Linear(in_features=int(8 * input_dim / 4), out_features=128)\n",
    "#         self.fc2 = nn.Linear(in_features=128, out_features=num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.reshape(x.shape[0], 1, x.shape[1])\n",
    "#         x = self.attention1(self.conv1(x))\n",
    "#         x = self.pool1(F.relu(x))\n",
    "#         x = self.attention2(self.conv2(x))\n",
    "#         x = self.pool2(F.relu(x))\n",
    "#         x = torch.flatten(x, 1)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "# class Attention(nn.Module):\n",
    "#     def __init__(self, input_dim, hidden_dim):\n",
    "#         super(Attention, self).__init__()\n",
    "#         self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "#         self.linear2 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.linear1(x))\n",
    "#         x = self.linear2(x)\n",
    "#         alpha = F.softmax(x, dim=1)\n",
    "#         return alpha\n",
    "\n",
    "# class CNN1D(nn.Module):\n",
    "#     def __init__(self, input_dim, num_classes, attention_hidden_dim=64):\n",
    "#         super(CNN1D, self).__init__()\n",
    "#         self.conv1 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=5, padding=2)\n",
    "#         self.bn1 = nn.BatchNorm1d(64)\n",
    "#         self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "#         self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "#         self.bn2 = nn.BatchNorm1d(128)\n",
    "#         self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "#         self.conv3 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "#         self.bn3 = nn.BatchNorm1d(256)\n",
    "#         self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "#         self.attention = Attention(256, attention_hidden_dim)\n",
    "\n",
    "#         self.fc1 = nn.Linear(in_features=256, out_features=512)\n",
    "#         self.fc2 = nn.Linear(in_features=512, out_features=num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.unsqueeze(1)\n",
    "#         x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "#         x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "#         x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
    "#         alpha = self.attention(x.permute(0, 2, 1))  # Attention weights\n",
    "#         x = torch.bmm(alpha.permute(0, 2, 1), x).squeeze()  # Weighted sum\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# class OptimizedCNN1D(nn.Module):\n",
    "#     def __init__(self, input_dim, num_classes):\n",
    "#         super(OptimizedCNN1D, self).__init__()\n",
    "#         self.conv1 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=5, padding=2)\n",
    "#         self.bn1 = nn.BatchNorm1d(64)\n",
    "#         self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "#         self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "#         self.bn2 = nn.BatchNorm1d(128)\n",
    "#         self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "#         self.conv3 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "#         self.bn3 = nn.BatchNorm1d(256)\n",
    "#         self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "#         self.fc1 = nn.Linear(in_features=256 * (input_dim // 8), out_features=512)\n",
    "#         self.fc2 = nn.Linear(in_features=512, out_features=num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.unsqueeze(1)  # Add channel dimension\n",
    "#         x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "#         x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "#         x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
    "#         x = x.view(x.size(0), -1)  # Flatten\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "    \n",
    "model_type = \"convolutional\"\n",
    "def train_model(embeddings_source, model_type = model_type, train_size=0.9):\n",
    "\n",
    "    train_dataset = ProteinSequenceDataset(datatype=\"train\", embeddings_sources=embeddings_source)\n",
    "    train_set, val_set = random_split(train_dataset, lengths = [int(len(train_dataset)*train_size), len(train_dataset)-int(len(train_dataset)*train_size)])\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=config.batch_size, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_set, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "    total_input_dim = sum(embeds_dim[src] for src in embeddings_sources)\n",
    "    model = MultiLayerPerceptron(input_dim=total_input_dim, num_classes=config.num_labels).to(config.device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = config.lr)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, factor=0.1, patience=1)\n",
    "    CrossEntropy = torch.nn.CrossEntropyLoss()\n",
    "    f1_score = MultilabelF1Score(num_labels=config.num_labels).to(config.device)\n",
    "    n_epochs = config.n_epochs\n",
    "\n",
    "\n",
    "    print(\"BEGIN TRAINING...\")\n",
    "    train_loss_history=[]\n",
    "    val_loss_history=[]\n",
    "    \n",
    "    train_f1score_history=[]\n",
    "    val_f1score_history=[]\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"EPOCH \", epoch+1)\n",
    "        ## TRAIN PHASE :\n",
    "        losses = []\n",
    "        scores = []\n",
    "        for embed, targets in tqdm(train_dataloader):\n",
    "            embed, targets = embed.to(config.device), targets.to(config.device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(embed)\n",
    "            loss= CrossEntropy(preds, targets)\n",
    "            score=f1_score(preds, targets)\n",
    "            losses.append(loss.item()) \n",
    "            scores.append(score.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        avg_loss = np.mean(losses)\n",
    "        avg_score = np.mean(scores)\n",
    "        print(\"Running Average TRAIN Loss : \", avg_loss)\n",
    "        print(\"Running Average TRAIN F1-Score : \", avg_score)\n",
    "        train_loss_history.append(avg_loss)\n",
    "        train_f1score_history.append(avg_score)\n",
    "        \n",
    "        ## VALIDATION PHASE : \n",
    "        losses = []\n",
    "        scores = []\n",
    "        for embed, targets in val_dataloader:\n",
    "            embed, targets = embed.to(config.device), targets.to(config.device)\n",
    "            preds = model(embed)\n",
    "            loss= CrossEntropy(preds, targets)\n",
    "            score=f1_score(preds, targets)\n",
    "            losses.append(loss.item())\n",
    "            scores.append(score.item())\n",
    "\n",
    "        avg_loss = np.mean(losses)\n",
    "        avg_score = np.mean(scores)\n",
    "        print(\"Running Average VAL Loss : \", avg_loss)\n",
    "        print(\"Running Average VAL F1-Score : \", avg_score)\n",
    "        val_loss_history.append(avg_loss)\n",
    "        val_f1score_history.append(avg_score)\n",
    "        \n",
    "        scheduler.step(avg_loss)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    print(\"TRAINING FINISHED\")\n",
    "    print(\"FINAL TRAINING SCORE : \", train_f1score_history[-1])\n",
    "    print(\"FINAL VALIDATION SCORE : \", val_f1score_history[-1])\n",
    "    print(\"FINAL TRAINING MAX F1 SCORE : \", max(train_f1score_history))\n",
    "    print(\"FINAL VALIDATION MAX F1 SCORE : \", max(val_f1score_history))\n",
    "\n",
    "    losses_history = {\"train\" : train_loss_history, \"val\" : val_loss_history}\n",
    "    scores_history = {\"train\" : train_f1score_history, \"val\" : val_f1score_history}\n",
    "    \n",
    "    return model, losses_history, scores_history\n",
    "\n",
    "\n",
    "# 训练模型\n",
    "embeddings_sources = [\"T5\", \"ProtBERT\", \"EMS2\"]\n",
    "t5_model, t5_losses, t5_scores = train_model(embeddings_source = embeddings_sources,model_type=\"linear\")\n",
    "# t5_model, t5_losses, t5_scores = train_model(embeddings_source = embeddings_sources,model_type=\"convolutional\")\n",
    "\n",
    "# # 指定保存路径\n",
    "# model_path = \"t5_model.pth\"\n",
    "\n",
    "# # 保存模型\n",
    "# torch.save(t5_model.state_dict(), model_path)\n",
    "\n",
    "# print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# 4.模型预测\n",
    "\n",
    "def predict(embeddings_sources):\n",
    "    test_dataset = ProteinSequenceDataset(datatype=\"test\", embeddings_sources=embeddings_sources)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "    \n",
    "    model = t5_model\n",
    "    model.eval()\n",
    "    \n",
    "    labels = pd.read_csv(config.train_labels_path, sep = \"\\t\")\n",
    "    top_terms = labels.groupby(\"term\")[\"EntryID\"].count().sort_values(ascending=False)\n",
    "    labels_names = top_terms[:config.num_labels].index.values\n",
    "    print(\"GENERATE PREDICTION FOR TEST SET...\")\n",
    "\n",
    "    ids_ = np.empty(shape=(len(test_dataloader)*config.num_labels,), dtype=object)\n",
    "    go_terms_ = np.empty(shape=(len(test_dataloader)*config.num_labels,), dtype=object)\n",
    "    confs_ = np.empty(shape=(len(test_dataloader)*config.num_labels,), dtype=np.float32)\n",
    "\n",
    "    for i, (embed, id) in tqdm(enumerate(test_dataloader)):\n",
    "        embed = embed.to(config.device)\n",
    "        confs_[i*config.num_labels:(i+1)*config.num_labels] = torch.nn.functional.sigmoid(model(embed)).squeeze().detach().cpu().numpy()\n",
    "        ids_[i*config.num_labels:(i+1)*config.num_labels] = id[0]\n",
    "        go_terms_[i*config.num_labels:(i+1)*config.num_labels] = labels_names\n",
    "\n",
    "    submission_df = pd.DataFrame(data={\"Id\" : ids_, \"GO term\" : go_terms_, \"Confidence\" : confs_})\n",
    "    print(\"PREDICTIONS DONE\")\n",
    "    return submission_df\n",
    "submission_df = predict([\"T5\", \"ProtBERT\", \"EMS2\"])\n",
    "submission_df.to_csv('submission.tsv', sep='\\t', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2044afdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#预测结果融合\n",
    "test_pred_df_foldseek = pd.read_csv('/kaggle/input/foldseek-cafa/foldseek_submission.tsv',\n",
    "    sep='\\t', header=None, names=[1, 2, 3])\n",
    "test_pred_df_foldseek = test_pred_df_foldseek[test_pred_df_foldseek[3] > 0.6]\n",
    "\n",
    "submission_best_public = pd.read_csv('/kaggle/input/cafa5-tuning-merge-datasets/submission.tsv',\n",
    "    sep='\\t', header=None, names=['Id', 'GO term', 'Confidence'])\n",
    "\n",
    "submissions_merged = submission_best_public.merge(test_pred_df_foldseek, left_on=['Id', 'GO term'], \n",
    "                                                  right_on=[1, 2], how='outer')\n",
    "submissions_merged.drop([1, 2], axis=1, inplace=True)\n",
    "submissions_merged['confidence_combined'] = submissions_merged.apply(lambda row: row['Confidence'] if not np.isnan(row['Confidence']) else row[3], axis=1)\n",
    "submissions_merged[['Id', 'GO term', 'confidence_combined']].to_csv('submission.tsv',\n",
    "    sep='\\t', header=False, index=False)\n",
    "\n",
    "\n",
    "\n",
    "final_submission.loc[final_submission.confidence_combined<0.22,\"confidence_combined\"]=0\n",
    "final_submission.loc[final_submission.confidence_combined>0.8,\"confidence_combined\"]=1\n",
    "final_submission.confidence_combined.hist(range=[0, 1], bins=100)\n",
    "final_submission.to_csv('submission.tsv', sep='\\t', header=False, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
