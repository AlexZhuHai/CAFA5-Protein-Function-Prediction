{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b13b884-4c22-42fb-945b-d67869e0d417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch\n",
    "# !pip install torchmetrics\n",
    "# !pip install wandb\n",
    "# !pip install pytorch_lightning\n",
    "# !pip install cafaeval\n",
    "import pandas as pd\n",
    "import pdb\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "from torch.utils.data import Dataset\n",
    "import torch;\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchmetrics.classification import MultilabelF1Score\n",
    "from torchmetrics.classification import MultilabelAccuracy\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f27258ac-6123-4a28-a961-05fb637ee14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "# 1.数据预处理，使用T5（ProtTrans）、ems2、protein与训练模型生成embedding;\n",
    "# 2.配置读取embedding文件 & 构建数据集;\n",
    "\n",
    "# 查看最终生成评估文件的模版格式\n",
    "# sub = pd.read_csv(\"../input/cafa-5-protein-function-prediction/sample_submission.tsv\", sep= \"\\t\", header = None)\n",
    "# sub.columns = [\"The Protein ID\", \"The GO ID\", \"Predictedin Protein\"]\n",
    "# print(sub.head(5))\n",
    "MAIN_DIR = \"../input/cafa-5-protein-function-prediction\"\n",
    "\n",
    "class config:\n",
    "    train_labels_path = MAIN_DIR + \"/Train/train_terms.tsv\"\n",
    "    test_sequences_path = MAIN_DIR + \"/Test (Targets)/testsuperset.fasta\"\n",
    "    train_sequences_path = MAIN_DIR  + \"/Train/train_sequences.fasta\"\n",
    "    \n",
    "    num_labels = 500\n",
    "    n_epochs = 20\n",
    "    batch_size = 128\n",
    "    lr = 0.001 \n",
    "    # for Mac\n",
    "    device = torch.device('mps') \n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "    print(device)\n",
    "   \n",
    "# Directories for the different embedding vectors : \n",
    "embeds_map = {\n",
    "    \"T5\" : \"cafa-5-t5-embeddings-numpy\",\n",
    "    \"ProtBERT\" : \"cafa-5-protbert-embeddings-numpy\",\n",
    "    \"EMS2\" : \"cafa-5-ems2-embeddings-numpy\"\n",
    "}\n",
    "\n",
    "# Length of the different embedding vectors :\n",
    "embeds_dim = {\n",
    "    \"T5\" : 1024,\n",
    "    \"ProtBERT\" : 1024,\n",
    "    \"EMS2\" : 1280\n",
    "}\n",
    "\n",
    "\n",
    "class ProteinSequenceDataset(Dataset): \n",
    "#     def __init__(self, datatype, embeddings_sources):\n",
    "#     super(ProteinSequenceDataset).__init__()\n",
    "#     self.datatype = datatype\n",
    "\n",
    "#     # 加载嵌入并进行拼接\n",
    "#     embeddings_list = []\n",
    "#     for embeddings_source in embeddings_sources:\n",
    "#         embeds_path = f\"../input/{embeds_map[embeddings_source]}/{datatype}_embeddings.npy\"\n",
    "#         embeds = np.load(embeds_path)\n",
    "#         embeddings_list.append(embeds)\n",
    "\n",
    "#     # 调整嵌入向量的大小，使它们在拼接维度上具有相同的大小\n",
    "#     max_size = max(embed.shape[0] for embed in embeddings_list)\n",
    "#     for i, embed in enumerate(embeddings_list):\n",
    "#         if embed.shape[0] < max_size:\n",
    "#             pad_size = max_size - embed.shape[0]\n",
    "#             embeddings_list[i] = np.pad(embed, ((0, pad_size), (0, 0)), mode='constant', constant_values=0)\n",
    "\n",
    "#     # 使用numpy的hstack函数进行水平拼接\n",
    "#     combined_embeds = np.hstack(embeddings_list)\n",
    "\n",
    "#     # 加载IDs\n",
    "#     ids_path = f\"../input/{list(embeds_map.values())[0]}/{datatype}_ids.npy\"  # 假设所有ID文件相同\n",
    "#     ids = np.load(ids_path)\n",
    "\n",
    "#     # 创建包含所有嵌入和ID的DataFrame\n",
    "#     embeds_list = [combined_embeds[i, :] for i in range(combined_embeds.shape[0])]\n",
    "#     self.df = pd.DataFrame(data={\"EntryID\": ids, \"embed\": embeds_list})\n",
    "    \n",
    "#     if datatype == \"train\":\n",
    "#         np_labels = np.load(f\"../input/train-targets-top{config.num_labels}/train_targets_top{config.num_labels}.npy\")\n",
    "#         df_labels = pd.DataFrame(self.df['EntryID'])\n",
    "#         df_labels['labels_vect'] = [row for row in np_labels]\n",
    "#         self.df = self.df.merge(df_labels, on=\"EntryID\")\n",
    "\n",
    "    \n",
    "    def __init__(self, datatype, embeddings_source):\n",
    "        super(ProteinSequenceDataset).__init__()\n",
    "        self.datatype = datatype\n",
    "        embeds = np.load(\"../input/\"+embeds_map[embeddings_source]+\"/\"+datatype+\"_embeddings.npy\")\n",
    "        ids = np.load(\"../input/\"+embeds_map[embeddings_source]+\"/\"+datatype+\"_ids.npy\")\n",
    "\n",
    "        embeds_list = []\n",
    "        for l in range(embeds.shape[0]):\n",
    "            embeds_list.append(embeds[l,:])\n",
    "        self.df = pd.DataFrame(data={\"EntryID\": ids, \"embed\" : embeds_list})\n",
    "        \n",
    "        if datatype==\"train\":\n",
    "            np_labels = np.load(\n",
    "                \"../input/train-targets-top\"+str(config.num_labels)+ \\\n",
    "                \"/train_targets_top\"+str(config.num_labels)+\".npy\")\n",
    "            df_labels = pd.DataFrame(self.df['EntryID'])\n",
    "            df_labels['labels_vect']=[row for row in np_labels]\n",
    "            self.df = self.df.merge(df_labels, on=\"EntryID\")\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index): \n",
    "        embed = torch.tensor(self.df.iloc[index][\"embed\"] , dtype = torch.float32)\n",
    "        if self.datatype==\"train\":\n",
    "            targets = torch.tensor(self.df.iloc[index][\"labels_vect\"], dtype = torch.float32)\n",
    "            return embed, targets\n",
    "        if self.datatype==\"test\":\n",
    "            id = self.df.iloc[index][\"EntryID\"]\n",
    "            return embed, id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "805e8c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.定义分类模型 - MLP\n",
    "\n",
    "\n",
    "###############################################################\n",
    "########## >>>>> Baseline <<<<<<  ##########\n",
    "###############################################################\n",
    "# class MultiLayerPerceptron(torch.nn.Module): # member function \n",
    "    \n",
    "#     def __init__(self, input_dim, num_classes):\n",
    "#         super(MultiLayerPerceptron, self).__init__()\n",
    "\n",
    "#         self.linear1 = torch.nn.Linear(input_dim, 1012)\n",
    "#         self.activation1 = torch.nn.Tanh()\n",
    "#         self.linear2 = torch.nn.Linear(1012, 864)\n",
    "#         self.activation2 = torch.nn.Tanh()\n",
    "#         self.linear3 = torch.nn.Linear(864, 712)\n",
    "#         self.activation3 = torch.nn.Tanh()\n",
    "#         self.linear4 = torch.nn.Linear(712, num_classes)\n",
    "      \n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.linear1(x)\n",
    "#         x = self.activation1(x)\n",
    "#         x = self.linear2(x)\n",
    "#         x = self.activation2(x)\n",
    "#         x = self.linear3(x)\n",
    "#         x = self.activation3(x)\n",
    "#         x = self.linear4(x)\n",
    "#         return x\n",
    "\n",
    "###############################################################\n",
    "########## >>>>> 调整隐藏层神经元数量 2048<<<<<<  ##########\n",
    "###############################################################\n",
    "# class MultiLayerPerceptron(nn.Module):\n",
    "#     def __init__(self, input_dim, num_classes):\n",
    "#         super(MultiLayerPerceptron, self).__init__()\n",
    "\n",
    "#         # 增加第一个隐藏层的神经元数量到2048，逐层减少\n",
    "#         self.linear1 = nn.Linear(input_dim, 2048)\n",
    "#         self.activation1 = nn.Tanh()\n",
    "#         self.linear2 = nn.Linear(2048, 1024)\n",
    "#         self.activation2 = nn.Tanh()\n",
    "#         self.linear3 = nn.Linear(1024, 768)\n",
    "#         self.activation3 = nn.Tanh()\n",
    "#         self.linear4 = nn.Linear(768, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.linear1(x)\n",
    "#         x = self.activation1(x)\n",
    "#         x = self.linear2(x)\n",
    "#         x = self.activation2(x)\n",
    "#         x = self.linear3(x)\n",
    "#         x = self.activation3(x)\n",
    "#         x = self.linear4(x)\n",
    "#         return x\n",
    "\n",
    "###############################################################\n",
    "########## >>>>> 调修改激活函数 ReLU<<<<<<  ##########\n",
    "###############################################################\n",
    "# class MultiLayerPerceptron(nn.Module):\n",
    "#     def __init__(self, input_dim, num_classes):\n",
    "#         super(MultiLayerPerceptron, self).__init__()\n",
    "\n",
    "#         # 使用ReLU激活函数\n",
    "#         self.linear1 = nn.Linear(input_dim, 2048)\n",
    "#         self.activation1 = nn.ReLU()\n",
    "#         self.linear2 = nn.Linear(2048, 1024)\n",
    "#         self.activation2 = nn.ReLU()\n",
    "#         self.linear3 = nn.Linear(1024, 768)\n",
    "#         self.activation3 = nn.ReLU()\n",
    "#         self.linear4 = nn.Linear(768, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.linear1(x)\n",
    "#         x = self.activation1(x)\n",
    "#         x = self.linear2(x)\n",
    "#         x = self.activation2(x)\n",
    "#         x = self.linear3(x)\n",
    "#         x = self.activation3(x)\n",
    "#         x = self.linear4(x)\n",
    "#         return x\n",
    "        \n",
    "###############################################################\n",
    "########## >>>>> 添加正则化层 Dropout(0.5)<<<<<<  ##########\n",
    "###############################################################\n",
    "# class MultiLayerPerceptron(nn.Module):\n",
    "#     def __init__(self, input_dim, num_classes):\n",
    "#         super(MultiLayerPerceptron, self).__init__()\n",
    "\n",
    "#         # 添加Dropout和Batch Normalization\n",
    "#         self.linear1 = nn.Linear(input_dim, 2048)\n",
    "#         self.bn1 = nn.BatchNorm1d(2048)\n",
    "#         self.activation1 = nn.ReLU()\n",
    "#         self.dropout1 = nn.Dropout(0.5)\n",
    "#         self.linear2 = nn.Linear(2048, 1024)\n",
    "#         self.bn2 = nn.BatchNorm1d(1024)\n",
    "#         self.activation2 = nn.ReLU()\n",
    "#         self.dropout2 = nn.Dropout(0.5)\n",
    "#         self.linear3 = nn.Linear(1024, 768)\n",
    "#         self.bn3 = nn.BatchNorm1d(768)\n",
    "#         self.activation3 = nn.ReLU()\n",
    "#         self.dropout3 = nn.Dropout(0.5)\n",
    "#         self.linear4 = nn.Linear(768, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.linear1(x)\n",
    "#         x = self.bn1(x)\n",
    "#         x = self.activation1(x)\n",
    "#         x = self.dropout1(x)\n",
    "#         x = self.linear2(x)\n",
    "#         x = self.bn2(x)\n",
    "#         x = self.activation2(x)\n",
    "#         x = self.dropout2(x)\n",
    "#         x = self.linear3(x)\n",
    "#         x = self.bn3(x)\n",
    "#         x = self.activation3(x)\n",
    "#         x = self.dropout3(x)\n",
    "#         x = self.linear4(x)\n",
    "#         return x\n",
    "\n",
    "###############################################################\n",
    "########## >>>>> 增加层数 5层<<<<<<  ##########\n",
    "###############################################################\n",
    "# class MultiLayerPerceptron(nn.Module):\n",
    "#     def __init__(self, input_dim, num_classes):\n",
    "#         super(MultiLayerPerceptron, self).__init__()\n",
    "\n",
    "#         # 增加层数\n",
    "#         self.linear1 = nn.Linear(input_dim, 2048)\n",
    "#         self.activation1 = nn.ReLU()\n",
    "#         self.linear2 = nn.Linear(2048, 1536)\n",
    "#         self.activation2 = nn.ReLU()\n",
    "#         self.linear3 = nn.Linear(1536, 1024)\n",
    "#         self.activation3 = nn.ReLU()\n",
    "#         self.linear4 = nn.Linear(1024, 768)\n",
    "#         self.activation4 = nn.ReLU()\n",
    "#         self.linear5 = nn.Linear(768, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.linear1(x)\n",
    "#         x = self.activation1(x)\n",
    "#         x = self.linear2(x)\n",
    "#         x = self.activation2(x)\n",
    "#         x = self.linear3(x)\n",
    "#         x = self.activation3(x)\n",
    "#         x = self.linear4(x)\n",
    "#         x = self.activation4(x)\n",
    "#         x = self.linear5(x)\n",
    "#         return x\n",
    "\n",
    "###############################################################\n",
    "########## >>>>> 减少层数 3层<<<<<<  ##########\n",
    "###############################################################\n",
    "# class MultiLayerPerceptron(nn.Module):\n",
    "#     def __init__(self, input_dim, num_classes):\n",
    "#         super(MultiLayerPerceptron, self).__init__()\n",
    "\n",
    "#         # 减少层数\n",
    "#         self.linear1 = nn.Linear(input_dim, 2048)\n",
    "#         self.activation1 = nn.ReLU()\n",
    "#         self.linear2 = nn.Linear(2048, 1024)\n",
    "#         self.activation2 = nn.ReLU()\n",
    "#         self.linear3 = nn.Linear(1024, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.linear1(x)\n",
    "#         x = self.activation1(x)\n",
    "#         x = self.linear2(x)\n",
    "#         x = self.activation2(x)\n",
    "#         x = self.linear3(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "###############################################################\n",
    "########## >>>>> 尝试增加注意力机制 <<<<<<  ##########\n",
    "###############################################################\n",
    "# class AttentionModule(nn.Module):\n",
    "#     def __init__(self, feature_dim):\n",
    "#         super(AttentionModule, self).__init__()\n",
    "#         self.attention_weights = nn.Sequential(\n",
    "#             nn.Linear(feature_dim, feature_dim),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(feature_dim, 1),\n",
    "#             nn.Softmax(dim=1)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         weights = self.attention_weights(x)  # 计算每个特征的重要性\n",
    "#         return x * weights  # 将重要性权重应用到特征上\n",
    "\n",
    "# class MultiLayerPerceptron(nn.Module):\n",
    "#     def __init__(self, input_dim, num_classes):\n",
    "#         super(MultiLayerPerceptron, self).__init__()\n",
    "#         self.linear1 = nn.Linear(input_dim, 2048)\n",
    "#         self.activation1 = nn.ReLU()\n",
    "#         self.attention1 = AttentionModule(2048)  # 注意力层加在第一层后\n",
    "#         self.linear2 = nn.Linear(2048, 1024)\n",
    "#         self.activation2 = nn.ReLU()\n",
    "#         self.attention2 = AttentionModule(1024)  # 注意力层加在第二层后\n",
    "#         self.linear3 = nn.Linear(1024, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.linear1(x)\n",
    "#         x = self.activation1(x)\n",
    "#         x = self.attention1(x)  # 应用注意力机制\n",
    "#         x = self.linear2(x)\n",
    "#         x = self.activation2(x)\n",
    "#         x = self.attention2(x)  # 应用注意力机制\n",
    "#         x = self.linear3(x)\n",
    "#         return x\n",
    "\n",
    "###############################################################\n",
    "########## >>>>> 尝试增加多头注意力机制 <<<<<<  ##########\n",
    "###############################################################\n",
    "# class MultiHeadAttention(nn.Module):\n",
    "#     def __init__(self, feature_dim, num_heads):\n",
    "#         super(MultiHeadAttention, self).__init__()\n",
    "#         self.num_heads = num_heads\n",
    "#         self.feature_dim = feature_dim\n",
    "#         self.dim_per_head = feature_dim // num_heads\n",
    "\n",
    "#         assert self.dim_per_head * num_heads == feature_dim, \"Feature dimension must be divisible by number of heads\"\n",
    "\n",
    "#         self.linear_layers = nn.ModuleList([nn.Linear(feature_dim, feature_dim) for _ in range(3)])\n",
    "#         self.output_linear = nn.Linear(feature_dim, feature_dim)\n",
    "#         self.dropout = nn.Dropout(0.0)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         batch_size = x.size(0)\n",
    "\n",
    "#         query, key, value = [l(x).view(batch_size, -1, self.num_heads, self.dim_per_head).transpose(1, 2) for l in self.linear_layers]\n",
    "\n",
    "#         scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.dim_per_head)\n",
    "#         attention = F.softmax(scores, dim=-1)\n",
    "#         attention = self.dropout(attention)\n",
    "\n",
    "#         weighted_value = torch.matmul(attention, value)\n",
    "#         weighted_value = weighted_value.transpose(1, 2).contiguous().view(batch_size, -1, self.feature_dim)\n",
    "\n",
    "#         return self.output_linear(weighted_value)\n",
    "    \n",
    "# class MultiLayerPerceptron(nn.Module):\n",
    "#     def __init__(self, input_dim, num_classes):\n",
    "#         super(MultiLayerPerceptron, self).__init__()\n",
    "#         self.linear1 = nn.Linear(input_dim, 2048)\n",
    "#         self.dropout1 = nn.Dropout(0.0)  # 增加Dropout\n",
    "#         self.attention1 = MultiHeadAttention(2048, 16)\n",
    "#         self.linear2 = nn.Linear(2048, 1024)\n",
    "#         self.dropout2 = nn.Dropout(0.0)  # 增加Dropout\n",
    "#         self.attention2 = MultiHeadAttention(1024, 16)\n",
    "#         self.linear3 = nn.Linear(1024, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.linear1(x))\n",
    "#         x = self.dropout1(x)  # 应用Dropout\n",
    "#         x = self.attention1(x)\n",
    "#         x = F.relu(self.linear2(x))\n",
    "#         x = self.dropout2(x)  # 应用Dropout\n",
    "#         x = self.attention2(x)\n",
    "#         x = self.linear3(x)\n",
    "#         x = x.squeeze()\n",
    "#         return x\n",
    "\n",
    "\n",
    "###############################################################\n",
    "########## >>>>> 神经元4096、10层、增加残差网络 <<<<<<  ##########\n",
    "###############################################################\n",
    "# class MultiLayerPerceptron(nn.Module):\n",
    "#     def __init__(self, input_dim, num_classes, layers=10, features=4096):\n",
    "#         super(MultiLayerPerceptron, self).__init__()\n",
    "#         self.layers = nn.ModuleList()\n",
    "#         self.batch_norms = nn.ModuleList()\n",
    "\n",
    "#         self.layers.append(nn.Linear(input_dim, features))\n",
    "#         self.batch_norms.append(nn.BatchNorm1d(features))\n",
    "\n",
    "#         if input_dim != features:\n",
    "#             self.residual_adapt = nn.Linear(input_dim, features)\n",
    "#         else:\n",
    "#             self.residual_adapt = None\n",
    "\n",
    "#         # Intermediate layers\n",
    "#         for _ in range(1, layers - 1):\n",
    "#             self.layers.append(nn.Linear(features, features))\n",
    "#             self.batch_norms.append(nn.BatchNorm1d(features))\n",
    "\n",
    "#         # Final layer\n",
    "#         self.layers.append(nn.Linear(features, num_classes))\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         initial_input = x  # Save initial input for residual connection adaptation\n",
    "\n",
    "#         x = F.relu(self.batch_norms[0](self.layers[0](x)))\n",
    "\n",
    "#         if self.residual_adapt:\n",
    "#             residual = self.residual_adapt(initial_input)  # Adapt the initial input dimension\n",
    "#         else:\n",
    "#             residual = initial_input\n",
    "\n",
    "#         for i in range(1, len(self.layers) - 1):\n",
    "#             y = F.relu(self.batch_norms[i](self.layers[i](x)))\n",
    "#             x = y + residual  # Apply residual connection\n",
    "#             residual = x  # Update residual to the latest x\n",
    "\n",
    "#         x = self.layers[-1](x)\n",
    "#         return x\n",
    "\n",
    "###############################################################\n",
    "########## >>>>> 3层、Dropout(0.5) <<<<<<  ##########\n",
    "###############################################################\n",
    "# class MultiLayerPerceptron(nn.Module):\n",
    "#     def __init__(self, input_dim, num_classes):\n",
    "#         super(MultiLayerPerceptron, self).__init__()\n",
    "\n",
    "#         # 减少层数\n",
    "#         self.linear1 = nn.Linear(input_dim, 2048)\n",
    "#         self.bn1 = nn.BatchNorm1d(2048)\n",
    "#         self.activation1 = nn.ReLU()\n",
    "#         self.dropout1 = nn.Dropout(0.5)\n",
    "#         self.linear2 = nn.Linear(2048, 1024)\n",
    "#         self.bn2 = nn.BatchNorm1d(1024)\n",
    "#         self.activation2 = nn.ReLU()\n",
    "#         self.dropout2 = nn.Dropout(0.5)\n",
    "#         self.linear3 = nn.Linear(1024, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.linear1(x)\n",
    "#         x = self.bn1(x)\n",
    "#         x = self.activation1(x)\n",
    "#         x = self.dropout1(x)\n",
    "#         x = self.linear2(x)\n",
    "#         x = self.bn2(x)\n",
    "#         x = self.activation2(x)\n",
    "#         x = self.dropout2(x)\n",
    "#         x = self.linear3(x)\n",
    "#         return x\n",
    "    \n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self,input_dim,num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.activation = nn.PReLU()\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm1d(input_dim)\n",
    "        self.fc1 = nn.Linear(input_dim, 800)\n",
    "        self.ln1 = nn.LayerNorm(800, elementwise_affine=True)\n",
    "        \n",
    "        self.bn2 = nn.BatchNorm1d(800)\n",
    "        self.fc2 = nn.Linear(800, 600)\n",
    "        self.ln2 = nn.LayerNorm(600, elementwise_affine=True)\n",
    "        \n",
    "        self.bn3 = nn.BatchNorm1d(600)\n",
    "        self.fc3 = nn.Linear(600, 400)\n",
    "        self.ln3 = nn.LayerNorm(400, elementwise_affine=True)\n",
    "        \n",
    "        self.bn4 = nn.BatchNorm1d(1200)\n",
    "        self.fc4 = nn.Linear(1200, num_classes)\n",
    "        self.ln4 = nn.LayerNorm(num_classes, elementwise_affine=True)\n",
    "        \n",
    "        self.sigm = nn.Sigmoid()\n",
    "    def forward(self,inputs):\n",
    "\n",
    "        fc1_out = self.bn1(inputs)\n",
    "        fc1_out = self.ln1(self.fc1(inputs))\n",
    "        fc1_out = self.activation(fc1_out)\n",
    "        \n",
    "        x = self.bn2(fc1_out)\n",
    "        \n",
    "        x = self.ln2(self.fc2(x))\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = self.bn3(x)\n",
    "        \n",
    "        x = self.ln3(self.fc3(x))\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        x = torch.cat([x, fc1_out], axis = -1)\n",
    "        \n",
    "        x = self.bn4(x)\n",
    "        \n",
    "        x = self.ln4(self.fc4(x))\n",
    "        out = self.sigm(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ea7a277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.定义分类模型 - 1DCNN\n",
    "\n",
    "###############################################################\n",
    "########## >>>>> baseline <<<<<<  ##########\n",
    "###############################################################\n",
    "class CNN1D(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(CNN1D, self).__init__()\n",
    "        # (batch_size, channels, embed_size)  \n",
    "        # Multi-dialted Layer\n",
    "        #  某一层：1x3 conv dilate1\n",
    "        #  某一层：1x3 conv dilate1 + 1x3 conv dilat2 + 1x7 conv dilate1  + 1x7 conv dilate2 \n",
    "        #  最直观的改进！！同一层有不同的感受野！！！\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=3, kernel_size=3, dilation=1, padding=1, stride=1)\n",
    "        # (batch_size, 3, embed_size)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        # (batch_size, 3, embed_size/2 = 512)\n",
    "        self.conv2 = nn.Conv1d(in_channels=3, out_channels=8, kernel_size=3, dilation=1, padding=1, stride=1)\n",
    "        # (batch_size, 8, embed_size/2 = 512)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        # (batch_size, 8, embed_size/4 = 256)\n",
    "        self.fc1 = nn.Linear(in_features=int(8 * input_dim/4), out_features=128)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.shape[0], 1, x.shape[1])\n",
    "        x = self.pool1(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool2(nn.functional.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "###############################################################\n",
    "########## >>>>> 第二层 扩大感受野 <<<<<<  ##########\n",
    "###############################################################\n",
    "# class CNN1D(nn.Module):\n",
    "#     def __init__(self, input_dim, num_classes):\n",
    "#         super(CNN1D, self).__init__()\n",
    "#         self.conv1 = nn.Conv1d(in_channels=1, out_channels=3, kernel_size=3, dilation=1, padding=1)\n",
    "#         self.bn1 = nn.BatchNorm1d(3)\n",
    "#         self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "#         self.conv2a = nn.Conv1d(in_channels=3, out_channels=8, kernel_size=3, dilation=1, padding=1)\n",
    "#         self.bn2a = nn.BatchNorm1d(8)\n",
    "#         self.conv2b = nn.Conv1d(in_channels=3, out_channels=8, kernel_size=5, dilation=1, padding=2)\n",
    "#         self.bn2b = nn.BatchNorm1d(8)\n",
    "#         self.conv2c = nn.Conv1d(in_channels=3, out_channels=8, kernel_size=3, dilation=2, padding=2)\n",
    "#         self.bn2c = nn.BatchNorm1d(8)\n",
    "#         self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "#         self.fc1 = nn.Linear(in_features=int(8 * input_dim/4), out_features=128)\n",
    "#         self.fc2 = nn.Linear(in_features=128, out_features=num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.reshape(x.shape[0], 1, x.shape[1])\n",
    "#         x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "#         x = F.relu(self.bn2a(self.conv2a(x))) + F.relu(self.bn2b(self.conv2b(x))) + F.relu(self.bn2c(self.conv2c(x)))\n",
    "#         x = self.pool2(x)\n",
    "#         x = torch.flatten(x, 1)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "###############################################################\n",
    "########## >>>>> 增加注意力机制 <<<<<<  ##########\n",
    "###############################################################\n",
    "# class AttentionModule(nn.Module):\n",
    "#     def __init__(self, channels):\n",
    "#         super(AttentionModule, self).__init__()\n",
    "#         self.attention_weights = nn.Sequential(\n",
    "#             nn.Conv1d(channels, channels, kernel_size=3, padding=1),  # 可以是更复杂的结构\n",
    "#             nn.ReLU(),\n",
    "#             nn.BatchNorm1d(channels),\n",
    "#             nn.Conv1d(channels, 1, kernel_size=3, padding=1),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         weights = self.attention_weights(x)\n",
    "#         return x * weights\n",
    "\n",
    "# class CNN1D(nn.Module):\n",
    "#     def __init__(self, input_dim, num_classes):\n",
    "#         super(CNN1D, self).__init__()\n",
    "#         self.conv1 = nn.Conv1d(in_channels=1, out_channels=3, kernel_size=3, dilation=1, padding=1, stride=1)\n",
    "#         self.attention1 = AttentionModule(3)\n",
    "#         self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "#         self.conv2 = nn.Conv1d(in_channels=3, out_channels=8, kernel_size=3, dilation=1, padding=1, stride=1)\n",
    "#         self.attention2 = AttentionModule(8)\n",
    "#         self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "#         self.fc1 = nn.Linear(in_features=int(8 * input_dim / 4), out_features=128)\n",
    "#         self.fc2 = nn.Linear(in_features=128, out_features=num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.reshape(x.shape[0], 1, x.shape[1])\n",
    "#         x = self.attention1(self.conv1(x))\n",
    "#         x = self.pool1(F.relu(x))\n",
    "#         x = self.attention2(self.conv2(x))\n",
    "#         x = self.pool2(F.relu(x))\n",
    "#         x = torch.flatten(x, 1)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02fce8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN TRAINING...\n",
      "EPOCH  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1001/1001 [00:15<00:00, 66.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  148.11298829954225\n",
      "Running Average TRAIN F1-Score :  0.18063766602333728\n",
      "Running Average VAL Loss :  147.3607474054609\n",
      "Running Average VAL F1-Score :  0.2115955469198525\n",
      "\n",
      "\n",
      "EPOCH  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1001/1001 [00:14<00:00, 67.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  145.64049745725467\n",
      "Running Average TRAIN F1-Score :  0.224618124050813\n",
      "Running Average VAL Loss :  145.84388262884957\n",
      "Running Average VAL F1-Score :  0.23656268923410348\n",
      "\n",
      "\n",
      "EPOCH  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1001/1001 [00:14<00:00, 67.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  144.6687542499005\n",
      "Running Average TRAIN F1-Score :  0.24698475315854265\n",
      "Running Average VAL Loss :  145.61247335161482\n",
      "Running Average VAL F1-Score :  0.2509148513366069\n",
      "\n",
      "\n",
      "EPOCH  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1001/1001 [00:14<00:00, 67.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  144.04660862666387\n",
      "Running Average TRAIN F1-Score :  0.26306359560696874\n",
      "Running Average VAL Loss :  144.64057963235038\n",
      "Running Average VAL F1-Score :  0.26017869829333257\n",
      "\n",
      "\n",
      "EPOCH  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1001/1001 [00:14<00:00, 67.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  143.62687552773156\n",
      "Running Average TRAIN F1-Score :  0.2764860439401764\n",
      "Running Average VAL Loss :  144.6617626462664\n",
      "Running Average VAL F1-Score :  0.26680476364812683\n",
      "\n",
      "\n",
      "EPOCH  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1001/1001 [00:14<00:00, 67.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  143.29244340406908\n",
      "Running Average TRAIN F1-Score :  0.286810508260241\n",
      "Running Average VAL Loss :  144.97399650301253\n",
      "Running Average VAL F1-Score :  0.27583765438092606\n",
      "\n",
      "\n",
      "EPOCH  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1001/1001 [00:14<00:00, 67.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  142.77964465649097\n",
      "Running Average TRAIN F1-Score :  0.3066443005850265\n",
      "Running Average VAL Loss :  145.0096802030291\n",
      "Running Average VAL F1-Score :  0.28551822820944445\n",
      "\n",
      "\n",
      "EPOCH  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1001/1001 [00:14<00:00, 67.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  142.5869755854497\n",
      "Running Average TRAIN F1-Score :  0.3159130912769091\n",
      "Running Average VAL Loss :  144.4905960219247\n",
      "Running Average VAL F1-Score :  0.28764244967273306\n",
      "\n",
      "\n",
      "EPOCH  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1001/1001 [00:14<00:00, 67.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  142.49004821320037\n",
      "Running Average TRAIN F1-Score :  0.3205737567448116\n",
      "Running Average VAL Loss :  145.09972967420305\n",
      "Running Average VAL F1-Score :  0.2906730242871812\n",
      "\n",
      "\n",
      "EPOCH  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1001/1001 [00:14<00:00, 67.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  142.411641520101\n",
      "Running Average TRAIN F1-Score :  0.3251487248308294\n",
      "Running Average VAL Loss :  144.40837580817086\n",
      "Running Average VAL F1-Score :  0.2911611261910626\n",
      "\n",
      "\n",
      "EPOCH  11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1001/1001 [00:14<00:00, 67.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  142.34831311724164\n",
      "Running Average TRAIN F1-Score :  0.32797124122942123\n",
      "Running Average VAL Loss :  144.8645897592817\n",
      "Running Average VAL F1-Score :  0.29322261549532413\n",
      "\n",
      "\n",
      "EPOCH  12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1001/1001 [00:14<00:00, 67.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  142.28521574555816\n",
      "Running Average TRAIN F1-Score :  0.3315605051391251\n",
      "Running Average VAL Loss :  144.313166482108\n",
      "Running Average VAL F1-Score :  0.2925274439954332\n",
      "\n",
      "\n",
      "EPOCH  13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1001/1001 [00:14<00:00, 67.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  142.20899585338978\n",
      "Running Average TRAIN F1-Score :  0.3350775029573526\n",
      "Running Average VAL Loss :  144.32341289520264\n",
      "Running Average VAL F1-Score :  0.2945912509624447\n",
      "\n",
      "\n",
      "EPOCH  14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1001/1001 [00:14<00:00, 67.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  142.12568899301382\n",
      "Running Average TRAIN F1-Score :  0.3377521744855753\n",
      "Running Average VAL Loss :  144.17862789971488\n",
      "Running Average VAL F1-Score :  0.293881769957287\n",
      "\n",
      "\n",
      "EPOCH  15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1001/1001 [00:14<00:00, 67.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  142.08073331545162\n",
      "Running Average TRAIN F1-Score :  0.34018241927459403\n",
      "Running Average VAL Loss :  144.4891800880432\n",
      "Running Average VAL F1-Score :  0.29564752536160605\n",
      "\n",
      "\n",
      "EPOCH  16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1001/1001 [00:14<00:00, 67.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  142.01919361308856\n",
      "Running Average TRAIN F1-Score :  0.3427799721578737\n",
      "Running Average VAL Loss :  144.45540741511755\n",
      "Running Average VAL F1-Score :  0.29692351658429417\n",
      "\n",
      "\n",
      "EPOCH  17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1001/1001 [00:14<00:00, 67.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  141.8925131036566\n",
      "Running Average TRAIN F1-Score :  0.34698231043396416\n",
      "Running Average VAL Loss :  144.81054060799735\n",
      "Running Average VAL F1-Score :  0.29672463797032833\n",
      "\n",
      "\n",
      "EPOCH  18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1001/1001 [00:14<00:00, 67.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  141.87232024567228\n",
      "Running Average TRAIN F1-Score :  0.3481591928761441\n",
      "Running Average VAL Loss :  144.62272753034318\n",
      "Running Average VAL F1-Score :  0.29825897780912264\n",
      "\n",
      "\n",
      "EPOCH  19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1001/1001 [00:14<00:00, 67.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  141.86771812781944\n",
      "Running Average TRAIN F1-Score :  0.34881879755488404\n",
      "Running Average VAL Loss :  144.2957844734192\n",
      "Running Average VAL F1-Score :  0.29718715072210344\n",
      "\n",
      "\n",
      "EPOCH  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1001/1001 [00:14<00:00, 67.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Average TRAIN Loss :  141.94673776388407\n",
      "Running Average TRAIN F1-Score :  0.34855694123557757\n",
      "Running Average VAL Loss :  144.46160418646676\n",
      "Running Average VAL F1-Score :  0.296888583738889\n",
      "\n",
      "\n",
      "TRAINING FINISHED\n",
      "FINAL TRAINING SCORE :  0.34855694123557757\n",
      "FINAL VALIDATION SCORE :  0.296888583738889\n",
      "FINAL TRAINING MAX F1 SCORE :  0.34881879755488404\n",
      "FINAL VALIDATION MAX F1 SCORE :  0.29825897780912264\n"
     ]
    }
   ],
   "source": [
    "# 4.模型训练\n",
    "\n",
    "model_type = \"convolutional\"\n",
    "def train_model(embeddings_source, model_type = model_type, train_size=0.9):\n",
    "    \n",
    "    train_dataset = ProteinSequenceDataset(datatype = \"train\", embeddings_source = embeddings_source)\n",
    "    train_set, val_set = random_split(train_dataset, lengths = [int(len(train_dataset)*train_size), len(train_dataset)-int(len(train_dataset)*train_size)])\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=config.batch_size, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_set, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "    if model_type == \"linear\":\n",
    "        model = MultiLayerPerceptron(input_dim=embeds_dim[embeddings_source], num_classes=config.num_labels).to(config.device)\n",
    "    if model_type == \"convolutional\":\n",
    "        model = CNN1D(input_dim=embeds_dim[embeddings_source], num_classes=config.num_labels).to(config.device)\n",
    "\n",
    "        \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = config.lr)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, factor=0.1, patience=1)\n",
    "    CrossEntropy = torch.nn.CrossEntropyLoss()\n",
    "    f1_score = MultilabelF1Score(num_labels=config.num_labels).to(config.device)\n",
    "    n_epochs = config.n_epochs\n",
    "\n",
    "\n",
    "    print(\"BEGIN TRAINING...\")\n",
    "    train_loss_history=[]\n",
    "    val_loss_history=[]\n",
    "    \n",
    "    train_f1score_history=[]\n",
    "    val_f1score_history=[]\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"EPOCH \", epoch+1)\n",
    "        ## TRAIN PHASE :\n",
    "        losses = []\n",
    "        scores = []\n",
    "        for embed, targets in tqdm(train_dataloader):\n",
    "            embed, targets = embed.to(config.device), targets.to(config.device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(embed)\n",
    "            loss= CrossEntropy(preds, targets)\n",
    "            score=f1_score(preds, targets)\n",
    "            losses.append(loss.item()) \n",
    "            scores.append(score.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        avg_loss = np.mean(losses)\n",
    "        avg_score = np.mean(scores)\n",
    "        print(\"Running Average TRAIN Loss : \", avg_loss)\n",
    "        print(\"Running Average TRAIN F1-Score : \", avg_score)\n",
    "        train_loss_history.append(avg_loss)\n",
    "        train_f1score_history.append(avg_score)\n",
    "        \n",
    "        ## VALIDATION PHASE : \n",
    "        losses = []\n",
    "        scores = []\n",
    "        for embed, targets in val_dataloader:\n",
    "            embed, targets = embed.to(config.device), targets.to(config.device)\n",
    "            preds = model(embed)\n",
    "            loss= CrossEntropy(preds, targets)\n",
    "            score=f1_score(preds, targets)\n",
    "            losses.append(loss.item())\n",
    "            scores.append(score.item())\n",
    "\n",
    "        avg_loss = np.mean(losses)\n",
    "        avg_score = np.mean(scores)\n",
    "        print(\"Running Average VAL Loss : \", avg_loss)\n",
    "        print(\"Running Average VAL F1-Score : \", avg_score)\n",
    "        val_loss_history.append(avg_loss)\n",
    "        val_f1score_history.append(avg_score)\n",
    "        \n",
    "        scheduler.step(avg_loss)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    print(\"TRAINING FINISHED\")\n",
    "    print(\"FINAL TRAINING SCORE : \", train_f1score_history[-1])\n",
    "    print(\"FINAL VALIDATION SCORE : \", val_f1score_history[-1])\n",
    "    print(\"FINAL TRAINING MAX F1 SCORE : \", max(train_f1score_history))\n",
    "    print(\"FINAL VALIDATION MAX F1 SCORE : \", max(val_f1score_history))\n",
    "\n",
    "    losses_history = {\"train\" : train_loss_history, \"val\" : val_loss_history}\n",
    "    scores_history = {\"train\" : train_f1score_history, \"val\" : val_f1score_history}\n",
    "    \n",
    "    return model, losses_history, scores_history\n",
    "\n",
    "\n",
    "# 训练模型\n",
    "t5_model, t5_losses, t5_scores = train_model(embeddings_source=\"T5\",model_type=\"linear\")\n",
    "# protbert_model, protbert_losses, protbert_scores = train_model(embeddings_source=\"ProtBERT\",model_type=\"linear\")\n",
    "# ems2_model, ems2_losses, ems2_scores = train_model(embeddings_source=\"EMS2\",model_type=\"convolutional\")\n",
    "\n",
    "# plt.figure(figsize = (10, 4))\n",
    "# #plt.plot(ems2_losses[\"val\"], label = \"EMS2\")\n",
    "# plt.plot(t5_losses[\"val\"], label = \"T5\")\n",
    "# #plt.plot(protbert_losses[\"val\"], label = \"ProtBERT\") \n",
    "# plt.title(\"Validation Losses for # Vector Embeddings\")\n",
    "# plt.xlabel(\"Epochs\")\n",
    "# plt.ylabel(\"Average Loss\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize = (10, 4))\n",
    "# #plt.plot(ems2_scores[\"val\"], label = \"EMS2\")\n",
    "# plt.plot(t5_scores[\"val\"], label = \"T5\")\n",
    "# #plt.plot(protbert_scores[\"val\"], label = \"ProtBERT\")\n",
    "# plt.title(\"Validation F1-Scores for # Vector Embeddings\")\n",
    "# plt.xlabel(\"Epochs\")\n",
    "# plt.ylabel(\"Average F1-Score\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8597bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATE PREDICTION FOR TEST SET...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "141864it [02:14, 1054.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTIONS DONE\n",
      "              Id     GO term  Confidence\n",
      "0         Q9ZSA8  GO:0005575    0.932083\n",
      "1         Q9ZSA8  GO:0008150    0.928279\n",
      "2         Q9ZSA8  GO:0110165    0.930866\n",
      "3         Q9ZSA8  GO:0003674    0.915635\n",
      "4         Q9ZSA8  GO:0005622    0.912028\n",
      "...          ...         ...         ...\n",
      "70931995  P0AG74  GO:0044403    0.188066\n",
      "70931996  P0AG74  GO:0042803    0.180563\n",
      "70931997  P0AG74  GO:0016829    0.162771\n",
      "70931998  P0AG74  GO:0090596    0.171527\n",
      "70931999  P0AG74  GO:0051640    0.190207\n",
      "\n",
      "[70932000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# 4.模型预测\n",
    "\n",
    "def predict(embeddings_source):\n",
    "    \n",
    "    test_dataset = ProteinSequenceDataset(datatype=\"test\", embeddings_source = embeddings_source)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "    \n",
    "    if embeddings_source == \"T5\":\n",
    "        model = t5_model\n",
    "    if embeddings_source == \"ProtBERT\":\n",
    "        model = protbert_model\n",
    "    if embeddings_source == \"EMS2\":\n",
    "        model = ems2_model\n",
    "        \n",
    "    model.eval()\n",
    "    \n",
    "    labels = pd.read_csv(config.train_labels_path, sep = \"\\t\")\n",
    "    top_terms = labels.groupby(\"term\")[\"EntryID\"].count().sort_values(ascending=False)\n",
    "    labels_names = top_terms[:config.num_labels].index.values\n",
    "    print(\"GENERATE PREDICTION FOR TEST SET...\")\n",
    "\n",
    "    ids_ = np.empty(shape=(len(test_dataloader)*config.num_labels,), dtype=object)\n",
    "    go_terms_ = np.empty(shape=(len(test_dataloader)*config.num_labels,), dtype=object)\n",
    "    confs_ = np.empty(shape=(len(test_dataloader)*config.num_labels,), dtype=np.float32)\n",
    "\n",
    "    for i, (embed, id) in tqdm(enumerate(test_dataloader)):\n",
    "        embed = embed.to(config.device)\n",
    "        confs_[i*config.num_labels:(i+1)*config.num_labels] = torch.nn.functional.sigmoid(model(embed)).squeeze().detach().cpu().numpy()\n",
    "        ids_[i*config.num_labels:(i+1)*config.num_labels] = id[0]\n",
    "        go_terms_[i*config.num_labels:(i+1)*config.num_labels] = labels_names\n",
    "\n",
    "    submission_df = pd.DataFrame(data={\"Id\" : ids_, \"GO term\" : go_terms_, \"Confidence\" : confs_})\n",
    "    print(\"PREDICTIONS DONE\")\n",
    "    return submission_df\n",
    "\n",
    "\n",
    "# submission_df = predict(\"T5\")\n",
    "# submission_df.to_csv('submission-t5.tsv', sep='\\t', header=False, index=False)\n",
    "# submission_df = predict(\"ProtBERT\")\n",
    "# submission_df.to_csv('submission-protbert.tsv', sep='\\t', header=False, index=False)\n",
    "submission_df = predict(\"EMS2\")\n",
    "submission_df.to_csv('submission-ems2.tsv', sep='\\t', header=False, index=False)\n",
    "print(submission_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37962565-c664-4c45-be1e-9ec029c4e604",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'submission_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m###############################################################\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m########## >>>>> ENSEMBLING WITH TOP PUBLIC SUBMISSION <<<<<<  ##########\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m###############################################################\u001b[39;00m\n\u001b[1;32m      4\u001b[0m submission_best_public2 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../input/cafa-5-053818-pred/submission (3).tsv\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m     sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mId2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGO term2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConfidence2\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 8\u001b[0m submissions \u001b[38;5;241m=\u001b[39m submission_best_public2\u001b[38;5;241m.\u001b[39mmerge(submission_df, left_on\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mId2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGO term2\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m      9\u001b[0m                                                   right_on\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mId\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGO term\u001b[39m\u001b[38;5;124m'\u001b[39m], how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mouter\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m submissions\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mId\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGO term\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m submissions[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfidence_combined\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m submissions\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConfidence2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConfidence2\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01melse\u001b[39;00m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConfidence\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'submission_df' is not defined"
     ]
    }
   ],
   "source": [
    "###############################################################\n",
    "########## >>>>> ENSEMBLING WITH TOP PUBLIC SUBMISSION <<<<<<  ##########\n",
    "###############################################################\n",
    "submission_best_public2 = pd.read_csv('../input/cafa-5-053818-pred/submission (3).tsv',\n",
    "    sep='\\t', header=None, names=['Id2', 'GO term2', 'Confidence2'])\n",
    "\n",
    "\n",
    "submissions = submission_best_public2.merge(submission_df, left_on=['Id2', 'GO term2'], \n",
    "                                                  right_on=['Id', 'GO term'], how='outer')\n",
    "\n",
    "submissions.drop(['Id', 'GO term'], axis=1, inplace=True)\n",
    "submissions['confidence_combined'] = submissions.apply(lambda row: row['Confidence2'] if not np.isnan(row['Confidence2']) else row['Confidence'], axis=1)\n",
    "\n",
    "submissions[['Id2', 'GO term2', 'confidence_combined']].to_csv('submission.tsv', sep='\\t', header=False, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
