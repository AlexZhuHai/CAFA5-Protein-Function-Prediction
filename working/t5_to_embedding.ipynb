{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2128cc3a-4a53-41f0-8331-309966c19122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Downloading sentencepiece-0.2.0-cp311-cp311-macosx_11_0_arm64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m582.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Load the tokenizer\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m T5Tokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRostlab/prot_t5_xl_half_uniref50-enc\u001b[39m\u001b[38;5;124m'\u001b[39m, do_lower_case\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;66;03m#.to(device)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Load the model\u001b[39;00m\n\u001b[1;32m     27\u001b[0m model \u001b[38;5;241m=\u001b[39m T5EncoderModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRostlab/prot_t5_xl_half_uniref50-enc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device);\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/utils/import_utils.py:1412\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1412\u001b[0m requires_backends(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_backends)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/utils/import_utils.py:1400\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1398\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[1;32m   1399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[0;32m-> 1400\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "# !pip install obonet\n",
    "# !pip install pyvis\n",
    "# !pip install biopython\n",
    "# !pip install transformers\n",
    "# !pip install sentencepiece\n",
    "import os\n",
    "import json\n",
    "from typing import Dict\n",
    "from collections import Counter\n",
    "\n",
    "import random\n",
    "import obonet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "\n",
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "import torch\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_half_uniref50-enc', do_lower_case=False) #.to(device)\n",
    "\n",
    "# Load the model\n",
    "model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_half_uniref50-enc\").to(device);\n",
    "\n",
    "# only GPUs support half-precision currently; if you want to run on CPU use full-precision (not recommended, much slower)\n",
    "#model.full() if device=='cpu' else model.half()\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input/cafa-5-protein-function-prediction'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "from pathlib import Path\n",
    "path = Path('/input/cafa-5-protein-function-prediction')\n",
    "!head {path}/'Test (Targets)/testsuperset-taxon-list.tsv'\n",
    "!head {path}/'Test (Targets)/testsuperset.fasta'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6a8d8e-d250-49ac-a463-0ca6013939bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def get_embeddings(seq):\n",
    "    sequence_examples = [\" \".join(list(re.sub(r\"[UZOB]\", \"X\", seq)))]\n",
    "\n",
    "    ids = tokenizer.batch_encode_plus(sequence_examples, add_special_tokens=True, padding=\"longest\")\n",
    "\n",
    "    input_ids = torch.tensor(ids['input_ids']).to(device)\n",
    "    attention_mask = torch.tensor(ids['attention_mask']).to(device)\n",
    "\n",
    "    # generate embeddings\n",
    "    with torch.no_grad():\n",
    "        embedding_repr = model(input_ids=input_ids,\n",
    "                               attention_mask=attention_mask)\n",
    "\n",
    "    # extract residue embeddings for the first ([0,:]) sequence in the batch and remove padded & special tokens ([0,:7]) \n",
    "    emb_0 = embedding_repr.last_hidden_state[0]\n",
    "    emb_0_per_protein = emb_0.mean(dim=0)\n",
    "    \n",
    "    return emb_0_per_protein\n",
    "\n",
    "get_embeddings('MTMDKSELVQKAKLAEQAERYDDMAAAMKAVTEQGHELSNEERNLLSVAYKNVVGARRSS')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2b3417-e880-4bc8-a4b0-9117b97bd937",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = '/kaggle/input/cafa-5-protein-function-prediction/Train/train_sequences.fasta'\n",
    "print(\"Sequence example:\\n\\n\", next(iter(SeqIO.parse(fn, \"fasta\"))))\n",
    "sequences = SeqIO.parse(fn, \"fasta\")\n",
    "num_sequences = sum(1 for seq in sequences)\n",
    "print()\n",
    "print(\"Number of sequences in train:\", num_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4bb53d-2f09-4186-a177-afb0ac44823a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "fn = '/kaggle/input/cafa-5-protein-function-prediction/Train/train_sequences.fasta'\n",
    "\n",
    "sequences = SeqIO.parse(fn, \"fasta\")\n",
    "\n",
    "ids = []\n",
    "embeds = np.zeros((num_sequences, 1024))\n",
    "i = 0\n",
    "for seq in tqdm.tqdm(sequences):\n",
    "    ids.append(seq.id)\n",
    "    embeds[i] = get_embeddings(str(seq.seq)).detach().cpu().numpy()\n",
    "    i += 1\n",
    "    print('train_sequences embeds = ' , embeds)\n",
    "    print('train_sequences ids = ' , ids)  \n",
    "    break #remove it for full calculation\n",
    "    \n",
    "np.save('train_embeds.npy', embeds)\n",
    "np.save('train_ids.npy', np.array(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70470d4f-1b6f-47b1-8016-eb281d794912",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = '/kaggle/input/cafa-5-protein-function-prediction/Test (Targets)/testsuperset.fasta'\n",
    "\n",
    "sequences = SeqIO.parse(fn, \"fasta\")\n",
    "num_sequences = sum(1 for seq in sequences)\n",
    "print(\"Number of sequences in test:\", num_sequences)\n",
    "sequences = SeqIO.parse(fn, \"fasta\")\n",
    "\n",
    "\n",
    "ids = []\n",
    "embeds = np.zeros((num_sequences, 1024))\n",
    "i = 0\n",
    "for seq in tqdm.tqdm(sequences):\n",
    "    ids.append(seq.id)\n",
    "    embeds[i] = get_embeddings(str(seq.seq)).detach().cpu().numpy()\n",
    "    i += 1\n",
    "    print('test_embeds embeds = ' , embeds)\n",
    "    print('test_ids ids = ' , ids)  \n",
    "    break #remove it for full calculation\n",
    "    \n",
    "np.save('test_embeds.npy', embeds)\n",
    "np.save('test_ids.npy', np.array(ids))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
