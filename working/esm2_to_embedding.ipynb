{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530f0621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q fair-esm\n",
    "import pathlib\n",
    "import torch\n",
    "from esm import FastaBatchedDataset, pretrained\n",
    "\n",
    "\n",
    "def extract_embeddings(model_name, fasta_file, output_dir, tokens_per_batch=4096, seq_length=1022,repr_layers=[33]):\n",
    "    \n",
    "    model, alphabet = pretrained.load_model_and_alphabet(model_name)\n",
    "    model.eval()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        \n",
    "    dataset = FastaBatchedDataset.from_file(fasta_file)\n",
    "    batches = dataset.get_batch_indices(tokens_per_batch, extra_toks_per_seq=1)\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, \n",
    "        collate_fn=alphabet.get_batch_converter(seq_length), \n",
    "        batch_sampler=batches\n",
    "    )\n",
    "\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (labels, strs, toks) in enumerate(data_loader):\n",
    "\n",
    "            print(f'Processing batch {batch_idx + 1} of {len(batches)}')\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                toks = toks.to(device=\"cuda\", non_blocking=True)\n",
    "\n",
    "            out = model(toks, repr_layers=repr_layers, return_contacts=False)\n",
    "\n",
    "            logits = out[\"logits\"].to(device=\"cpu\")\n",
    "            representations = {layer: t.to(device=\"cpu\") for layer, t in out[\"representations\"].items()}\n",
    "            \n",
    "            for i, label in enumerate(labels):\n",
    "                entry_id = label.split()[0]\n",
    "                \n",
    "                filename = output_dir / f\"{entry_id}.pt\"\n",
    "                truncate_len = min(seq_length, len(strs[i]))\n",
    "\n",
    "                result = {\"entry_id\": entry_id}\n",
    "                result[\"mean_representations\"] = {\n",
    "                        layer: t[i, 1 : truncate_len + 1].mean(0).clone()\n",
    "                        for layer, t in representations.items()\n",
    "                    }\n",
    "\n",
    "                torch.save(result, filename)\n",
    "\n",
    "# Process train data                 \n",
    "model_name = 'esm2_t33_650M_UR50D'\n",
    "fasta_file = pathlib.Path('/kaggle/input/cafa-5-fasta-files/train_sequences.fasta')\n",
    "output_dir = pathlib.Path('train_embeddings')\n",
    "extract_embeddings(model_name, fasta_file, output_dir)\n",
    "\n",
    "\n",
    "# Process test data   \n",
    "model_name = 'esm2_t33_650M_UR50D'\n",
    "fasta_file = pathlib.Path('/kaggle/input/cafa-5-fasta-files/test_sequences.fasta')\n",
    "output_dir = pathlib.Path('test_embeddings')\n",
    "extract_embeddings(model_name, fasta_file, output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
