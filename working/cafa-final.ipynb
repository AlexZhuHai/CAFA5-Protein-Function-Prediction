{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T00:16:13.789460Z","iopub.status.busy":"2024-05-23T00:16:13.788398Z","iopub.status.idle":"2024-05-23T00:17:02.710864Z","shell.execute_reply":"2024-05-23T00:17:02.708792Z","shell.execute_reply.started":"2024-05-23T00:16:13.789409Z"},"trusted":true},"outputs":[],"source":["%%capture \n","%%time\n","######################################################################################\n","###############  Preparation And Configuration\n","######################################################################################\n","!pip install torchmetrics\n","!pip install torchsummary\n","import time\n","import numpy as np \n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import os\n","import psutil\n","import datetime\n","from torchmetrics import AUROC as torch_AUCROC\n","from torchmetrics import F1Score as torch_F1Score\n","from torchsummary import summary as torchsummary\n","from sklearn.model_selection import train_test_split\n","from tqdm.notebook import tqdm\n","import torch\n","import warnings\n","import random \n","import os\n","import numpy as np\n","from scipy import sparse\n","import matplotlib.pyplot as plt\n","import gc\n","warnings.filterwarnings('ignore')\n","tqdm.pandas()\n","import torch.nn.functional as F\n","import torch.nn as nn\n","import gc\n","import time \n","import numpy as np\n","try: \n","    from sklearn.metrics import roc_auc_score\n","    from sklearn.metrics import f1_score\n","    from sklearn.linear_model import Ridge\n","    from sklearn.neural_network import MLPRegressor\n","    import lightgbm as lgbm\n","    from sklearn.multioutput import MultiOutputRegressor\n","    from sklearn.multioutput import MultiOutputClassifier  \n","    \n","    from catboost import CatBoostRegressor\n","    from catboost import CatBoostClassifier    \n","except Exception as e:\n","    print(f'Exception importing models {e} ')\n","    pass\n","import keras\n","from keras.models import Sequential, Model\n","from keras.layers import Dense, Input,  Concatenate, Dropout, BatchNormalization, Activation\n","from keras.layers import LayerNormalization\n","\n","t0start = time.time()\n","RANDOM_SEED = None\n","# 获取可用RAM\n","def get_available_ram():\n","    virtual_memory = psutil.virtual_memory()\n","    available_ram = virtual_memory.available\n","    return available_ram\n","\n","# 记录可用RAM\n","def log_available_ram(str_for_logging_optional=None):\n","    try:\n","        virtual_memory = psutil.virtual_memory()\n","        available_ram_bytes = virtual_memory.available\n","        available_ram_gigabytes = available_ram_bytes / (1024 ** 3)\n","\n","        if str_for_logging_optional is not None:\n","            print(str_for_logging_optional)\n","        current_datetime = datetime.datetime.now()\n","        str1 = f\"Available RAM: {available_ram_gigabytes:.2f} G  Current datetime: {current_datetime}\"\n","        print(str1) \n","        \n","        with open(logs_file_path, 'a') as file:\n","            if str_for_logging_optional is not None:\n","                file.write(str_for_logging_optional + '\\n')\n","            file.write(str1 + '\\n')\n","    except Exception as e:\n","        print(f\"Error while appending data: {e}\")        \n","\n","log_available_ram('On start')\n","\n","# 设定随机种子函数\n","def seed_all(RANDOM_SEED):\n","    if RANDOM_SEED is not None: \n","        try:\n","            SEED = RANDOM_SEED\n","            random.seed(SEED)\n","            np.random.seed(SEED)\n","            torch.manual_seed(SEED)\n","            torch.cuda.manual_seed_all(SEED)\n","            torch.backends.cudnn.deterministic = True\n","            torch.backends.cudnn.benchmark = False\n","\n","        except Exception as e:\n","            print(f\"Exception: {e}\")\n","            \n","seed_all(RANDOM_SEED)      \n","\n","# 安装 Sophia 优化器\n","!git clone https://github.com/kyegomez/Sophia.git\n","! python Sophia/setup.py install\n","!rm Sophia/Sophia/__init__.py\n","from Sophia.Sophia.Sophia import SophiaG\n","\n","# 选择设备 - GPU或CPU\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","\n","# 配置参数\n","n_samples_to_consider = 142246\n","n_labels_to_consider = 500  # 最大到 31466，但超过 8000 可能会导致内存崩溃\n","n_folds_to_process = 3\n","\n","# 模型配置\n","list_main_config_model_feature_etc = []\n","cfg2 = {\n","    'model': {\n","        'id': 'pMLP_Andrey',\n","        'n_selfblend': 2,\n","        'epochs': 2,\n","        'custom_model_fit_function_name': 'model_fit_pytorch_Sophia_Andrey1'\n","    }\n","}\n","list_main_config_model_feature_etc.append(cfg2)\n","\n","# 特征集\n","list_features_id = ['t5', 'esm2S1280']\n","\n","# 计算得分的标志\n","flag_compute_oof_predictions = True\n","flag_compute_stat_for_each_model = flag_compute_oof_predictions and False\n","flag_compute_cafa_f1_for_each_model = flag_compute_oof_predictions and flag_compute_stat_for_each_model and True\n","flag_compute_each_blend_stat = flag_compute_oof_predictions and False\n","flag_compute_cafa_f1_for_each_blend = flag_compute_oof_predictions and flag_compute_each_blend_stat and True\n","flag_compute_final_model_stat = flag_compute_oof_predictions and True\n","\n","# 预测值小于 cutoff_threshold_low 将被设置为零\n","cutoff_threshold_low = 0.1\n","mode_submit = True\n","\n","# 保存预测结果的标志\n","flag_save_final_submit_file = mode_submit and True\n","flag_save_numpy_Y_pred_oof_blend = flag_compute_oof_predictions and True\n","flag_save_numpy_Y_submit = mode_submit and True\n","\n","mode_downsample_train_default = '43k'\n","logs_file_path = 'logs.txt'"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T00:19:24.910976Z","iopub.status.busy":"2024-05-23T00:19:24.909507Z","iopub.status.idle":"2024-05-23T00:21:00.191726Z","shell.execute_reply":"2024-05-23T00:21:00.190833Z","shell.execute_reply.started":"2024-05-23T00:19:24.910919Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['t5', 'esm2S1280']\n","/kaggle/input/cafa5-features-etc/T5_train_embeds_float32.npy\n","(142246, 1024)\n","[[ 0.04948843 -0.03293516  0.03247323]\n"," [-0.04461636  0.06492499 -0.08026284]]\n","protein_ids.shape: (142246,)\n","protein_ids[:15]: ['P20536' 'O73864' 'O95231' 'A0A0B4J1F4' 'P54366' 'P33681' 'P77596'\n"," 'Q16787' 'Q59VP0' 'P13508' 'Q96S79' 'Q9VSA3' 'O94652' 'P35669' 'Q5MNZ6']\n","/kaggle/input/23468234/train_embeds_esm2_t33_650M_UR50D.npy\n","(142246, 2304)\n","[[ 0.04948843 -0.03293516  0.03247323]\n"," [-0.04461636  0.06492499 -0.08026284]]\n","protein_ids.shape: (142246,)\n","protein_ids[:15]: ['P20536' 'O73864' 'O95231' 'A0A0B4J1F4' 'P54366' 'P33681' 'P77596'\n"," 'Q16787' 'Q59VP0' 'P13508' 'Q96S79' 'Q9VSA3' 'O94652' 'P35669' 'Q5MNZ6']\n","Y (142246, 31466) loaded\n","Y (142246, 500) truncated\n","(500,)\n","['GO:0005575' 'GO:0008150' 'GO:0110165' 'GO:0003674' 'GO:0005622'\n"," 'GO:0009987' 'GO:0043226' 'GO:0043229' 'GO:0005488' 'GO:0043227'\n"," 'GO:0005737' 'GO:0043231' 'GO:0005515' 'GO:0065007' 'GO:0050789'\n"," 'GO:0050794' 'GO:0050896' 'GO:0008152' 'GO:0032501' 'GO:0005634']\n","X 内存占用 (MB): 1250.208984375\n","Y 内存占用 (MB): 271.3127136230469\n","43189 [1, 2, 3, 5, 131079, 131080, 9, 131081, 13, 14]\n","(142246,) 5\n","0 28442\n","1 28449\n","2 28458\n","3 28467\n","4 28430\n","CPU times: user 1min 32s, sys: 2.15 s, total: 1min 34s\n","Wall time: 1min 35s\n"]}],"source":["%%time \n","######################################################################################\n","###############  Load embeddings and targets ,transformed in 0,1 multi-target task.\n","######################################################################################\n","\n","# 根据特征ID获取特征路径的函数\n","def get_paths_to_features(features_id):\n","    # 基础目录映射\n","    base_dir_map = {\n","        'esm2S2560': '../input/4637427/',\n","        't5': '../input/cafa5-features-etc/',\n","        'protbert': '../input/protbert-embeddings-for-cafa5/',\n","        'esm2S1280': '../input/23468234/',\n","        'esm2S320': '../input/315701375/',\n","        'esm2S640': '../input/8947923/',\n","        'esm2S480': '../input/3023750/'\n","    }\n","\n","    # 文件名映射\n","    file_name_map = {\n","        'esm2S2560': ('train_embeds_esm2_t36_3B_UR50D.npy', 'train_ids_esm2_t36_3B_UR50D.npy', 'test_embeds_esm2_t36_3B_UR50D.npy', 'test_ids_esm2_t36_3B_UR50D.npy'),\n","        't5': ('T5_train_embeds_float32.npy', 'train_ids.npy', 'T5_test_embeds_float32.npy', 'test_ids.npy'),\n","        'protbert': ('train_embeddings.npy', 'train_ids.npy', 'test_embeddings.npy', 'test_ids.npy'),\n","        'esm2S1280': ('train_embeds_esm2_t33_650M_UR50D.npy', 'train_ids_esm2_t33_650M_UR50D.npy', 'test_embeds_esm2_t33_650M_UR50D.npy', 'test_ids_esm2_t33_650M_UR50D.npy'),\n","        'esm2S320': ('train_embeds_esm2_t6_8M_UR50D.npy', 'train_ids_esm2_t6_8M_UR50D.npy', 'test_embeds_esm2_t6_8M_UR50D.npy', 'test_ids_esm2_t6_8M_UR50D.npy'),\n","        'esm2S640': ('train_embeds_esm2_t30_150M_UR50D.npy', 'train_ids_esm2_t30_150M_UR50D.npy', 'test_embeds_esm2_t30_150M_UR50D.npy', 'test_ids_esm2_t30_150M_UR50D.npy'),\n","        'esm2S480': ('train_embeds_esm2_t12_35M_UR50D.npy', 'train_ids_esm2_t12_35M_UR50D.npy', 'test_embeds_esm2_t12_35M_UR50D.npy', 'test_ids_esm2_t12_35M_UR50D.npy')\n","    }\n","\n","    dn = base_dir_map.get(features_id)\n","    fn_X, fn_protein_ids, fn_X_submit, fn_submit_protein_ids = file_name_map.get(features_id)\n","    \n","    return os.path.join(dn, fn_X), os.path.join(dn, fn_protein_ids), os.path.join(dn, fn_X_submit), os.path.join(dn, fn_submit_protein_ids)\n","\n","# 根据特征ID列表加载特征的函数\n","def get_features(list_features_id, n_samples_to_consider, mode_submit=False, verbose=0):\n","    if verbose >= 100: \n","        print(list_features_id)\n","\n","    X_submit, submit_protein_ids = None, None\n","    for i0, features_id in enumerate(list_features_id):\n","        fn_X, fn_protein_ids, fn_X_submit, fn_submit_protein_ids = get_paths_to_features(features_id)\n","        \n","        if verbose >= 100: \n","            print(fn_X)\n","        \n","        if i0 == 0:\n","            X = np.load(fn_X).astype(np.float32)[:n_samples_to_consider, :]\n","        else:\n","            X = np.concatenate([X, np.load(fn_X).astype(np.float32)[:n_samples_to_consider, :]], axis=1)\n","        \n","        if verbose >= 100: \n","            print(X.shape)\n","            print(X[:2, :3])\n","        \n","        protein_ids = np.load(fn_protein_ids)[:n_samples_to_consider]\n","        vec_train_protein_ids = protein_ids\n","        \n","        if verbose >= 100:\n","            print('protein_ids.shape:', protein_ids.shape)\n","            print('protein_ids[:15]:', protein_ids[:15])\n","\n","        if mode_submit:\n","            if verbose >= 100: \n","                print(fn_X_submit)\n","            \n","            if i0 == 0:\n","                X_submit = np.load(fn_X_submit).astype(np.float32)\n","            else:\n","                X_submit = np.concatenate([X_submit, np.load(fn_X_submit).astype(np.float32)], axis=1)\n","            \n","            if verbose >= 100:\n","                print(X_submit.shape)\n","                print(X_submit[:2, :3])\n","            \n","            submit_protein_ids = np.load(fn_submit_protein_ids)\n","            if verbose >= 100:\n","                print(submit_protein_ids.shape, submit_protein_ids[:10])\n","    \n","    return X, vec_train_protein_ids, X_submit, submit_protein_ids\n","\n","# 加载特征\n","X, vec_train_protein_ids, X_submit, submit_protein_ids = get_features(list_features_id, n_samples_to_consider, verbose=100)\n","\n","# 加载目标和其ID\n","fn_Y = '/kaggle/input/cafa5-features-etc/Y_31466_sparse_float32.npz'\n","Y = sparse.load_npz(fn_Y)\n","print('Y', Y.shape, 'loaded')\n","Y = Y[:n_samples_to_consider, :n_labels_to_consider].toarray()\n","print('Y', Y.shape, 'truncated')\n","n_labels_to_consider = Y.shape[1]\n","\n","fn_Y_labels = '/kaggle/input/cafa5-features-etc/Y_31466_labels.npy'\n","Y_labels = np.load(fn_Y_labels, allow_pickle=True)[:n_labels_to_consider]\n","print(Y_labels.shape)\n","print(Y_labels[:20])\n","gc.collect()\n","print('X 内存占用 (MB):', X.nbytes / 1024 / 1024)\n","print('Y 内存占用 (MB):', Y.nbytes / 1024 / 1024)\n","try:\n","    print('X_submit 内存占用 (MB):', X_submit.nbytes / 1024 / 1024)\n","except:\n","    pass\n","\n","# 准备可能的训练集下采样\n","dict_set_allowed_train_indexes = {}\n","fn_allowed_train_ids = '/kaggle/input/cafa5-features-etc/train_ids_cut43k.npy'\n","allowed_train_ids = np.load(fn_allowed_train_ids)\n","vec_allowed_train_indexes_43k = [ix for ix in range(len(vec_train_protein_ids)) if vec_train_protein_ids[ix] in allowed_train_ids]\n","dict_set_allowed_train_indexes['43k'] = set(vec_allowed_train_indexes_43k)\n","print(len(dict_set_allowed_train_indexes['43k']), list(dict_set_allowed_train_indexes['43k'])[:10])\n","\n","# 获取下采样的训练索引的函数\n","def get_downsampled_IX_train(IX_train, mode_downsample_train):\n","    if mode_downsample_train in dict_set_allowed_train_indexes:\n","        set_allowed_train_indexes = dict_set_allowed_train_indexes[mode_downsample_train]\n","        IX_train = [t for t in IX_train if t in set_allowed_train_indexes]\n","    elif 'random_subsample_percent' in str(mode_downsample_train):\n","        random_subsample_percent = float(str(mode_downsample_train).split('_')[-1])\n","        IX_train = np.random.permutation(IX_train)[:int(len(IX_train) * random_subsample_percent / 100)]\n","    return IX_train\n","\n","# 加载交叉验证的折叠分区\n","fn_folds = '/kaggle/input/cafa5-features-etc/random_folds/folds_gkf.npy'\n","folds = np.load(fn_folds)[:n_samples_to_consider]\n","print(folds.shape, len(set(folds)))\n","for k in set(folds):\n","    m = folds == k\n","    print(k, m.sum())"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T00:21:05.229056Z","iopub.status.busy":"2024-05-23T00:21:05.228619Z","iopub.status.idle":"2024-05-23T00:21:05.240945Z","shell.execute_reply":"2024-05-23T00:21:05.239572Z","shell.execute_reply.started":"2024-05-23T00:21:05.229018Z"},"trusted":true},"outputs":[],"source":["######################################################################################\n","###############  Define neural networks\n","######################################################################################\n","class Model5(nn.Module):\n","    def __init__(self,input_features,output_features):\n","        super().__init__()\n","        \n","        self.activation = nn.PReLU()\n","        \n","        self.bn1 = nn.BatchNorm1d(input_features)\n","        self.fc1 = nn.Linear(input_features, 800)\n","        self.ln1 = nn.LayerNorm(800, elementwise_affine=True)\n","        \n","        self.bn2 = nn.BatchNorm1d(800)\n","        self.fc2 = nn.Linear(800, 600)\n","        self.ln2 = nn.LayerNorm(600, elementwise_affine=True)\n","        \n","        self.bn3 = nn.BatchNorm1d(600)\n","        self.fc3 = nn.Linear(600, 400)\n","        self.ln3 = nn.LayerNorm(400, elementwise_affine=True)\n","        \n","        self.bn4 = nn.BatchNorm1d(1200)\n","        self.fc4 = nn.Linear(1200, output_features)\n","        self.ln4 = nn.LayerNorm(output_features, elementwise_affine=True)\n","        \n","        self.sigm = nn.Sigmoid()\n","    def forward(self,inputs):\n","\n","        fc1_out = self.bn1(inputs)\n","        fc1_out = self.ln1(self.fc1(inputs))\n","        fc1_out = self.activation(fc1_out)\n","        \n","        x = self.bn2(fc1_out)\n","        \n","        x = self.ln2(self.fc2(x))\n","        x = self.activation(x)\n","        \n","        x = self.bn3(x)\n","        \n","        x = self.ln3(self.fc3(x))\n","        x = self.activation(x)\n","        \n","        x = torch.cat([x, fc1_out], axis = -1)\n","        \n","        x = self.bn4(x)\n","        \n","        x = self.ln4(self.fc4(x))\n","        out = self.sigm(x)\n","        return out"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T00:21:09.010356Z","iopub.status.busy":"2024-05-23T00:21:09.009851Z","iopub.status.idle":"2024-05-23T00:21:09.058561Z","shell.execute_reply":"2024-05-23T00:21:09.057401Z","shell.execute_reply.started":"2024-05-23T00:21:09.010237Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 84 µs, sys: 0 ns, total: 84 µs\n","Wall time: 88 µs\n"]}],"source":["%%time\n","######################################################################################\n","###############  Get model And Define fit&predict function\n","######################################################################################\n","import torch\n","from torch import nn\n","from sklearn.linear_model import Ridge\n","from sklearn.neural_network import MLPRegressor\n","from sklearn.multioutput import MultiOutputClassifier, MultiOutputRegressor\n","from catboost import CatBoostClassifier, CatBoostRegressor\n","import lightgbm as lgbm\n","from keras.models import Sequential\n","from keras.layers import Dense, BatchNormalization, Dropout\n","import keras\n","\n","# 获取模型的函数\n","def get_model(model_config):\n","    str_model_id = str(model_config['id'])\n","    \n","    # pMLP模型\n","    if model_config['id'] == 'pMLP_Andrey':\n","        model = Model5(X.shape[1], Y.shape[1])\n","        model.to(device)\n","    \n","    # Ridge回归模型\n","    elif model_config['id'] == 'Ridge':\n","        alpha = model_config.get('alpha', 10)\n","        str_model_id = 'Ridge' + str(alpha)\n","        model = Ridge(alpha=alpha)\n","    \n","    # skMLP模型\n","    elif model_config['id'] == 'skMLP':\n","        str_model_id = 'skMLP'\n","        max_iter = model_config.get('max_iter', 500)\n","        str_model_id += '_MI' + str(max_iter)\n","        random_state = model_config.get('random_state', np.random.randint(0, 100))\n","        str_model_id += '_RS' + str(random_state)\n","        hidden_layer_sizes = model_config.get('Layers', (100,))\n","        str_model_id += '_HL' + str(hidden_layer_sizes)\n","        alpha = model_config.get('alpha', 1e-4)\n","        str_model_id += '_alpha' + str(alpha)\n","        learning_rate_init = model_config.get('LR', 0.001)\n","        str_model_id += '_LR' + str(learning_rate_init)\n","        model = MLPRegressor(max_iter=max_iter, random_state=random_state, hidden_layer_sizes=hidden_layer_sizes, alpha=alpha, learning_rate_init=learning_rate_init)\n","    \n","    # 简单Keras多层感知机模型\n","    elif model_config['id'] == 'KMLP_simple':\n","        model = Sequential()\n","        str_model_id = 'KMLP_simple'\n","        \n","        nfeats = X.shape[1]\n","        nlabels = Y.shape[1]\n","        \n","        # 第一层\n","        layer_dim = 500\n","        model.add(Dense(layer_dim, activation='relu', input_dim=nfeats))\n","        str_model_id += '_L1_' + str(layer_dim)\n","        model.add(BatchNormalization())\n","        str_model_id += '_BN'\n","        dropout_rate = 0.1\n","        model.add(Dropout(dropout_rate))\n","        str_model_id += '_DR' + str(np.round(dropout_rate, 2))\n","        \n","        # 第二层\n","        layer_dim = 800\n","        model.add(Dense(layer_dim, activation='relu'))\n","        str_model_id += '_L2_' + str(layer_dim)\n","        model.add(BatchNormalization())\n","        str_model_id += '_BN'\n","        model.add(Dropout(dropout_rate))\n","        str_model_id += '_DR' + str(np.round(dropout_rate, 2))\n","        \n","        # 输出层\n","        model.add(Dense(nlabels, activation='sigmoid'))\n","        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[keras.metrics.AUC()])\n","    \n","    # 复杂Keras多层感知机模型\n","    elif model_config['id'] == 'KMLP':\n","        model = Sequential()\n","        str_model_id = 'KMLP'\n","        \n","        layers_sizes = model_config['Layers']\n","        list_dropouts = model_config.get('Dropouts', [])\n","        list_batchnormalization = model_config.get('BatchNormalizations', [])\n","        \n","        nfeats = X.shape[1]\n","        nlabels = Y.shape[1]\n","        \n","        # 第一层\n","        i_layer = 0\n","        layer_dim = layers_sizes[i_layer]\n","        model.add(Dense(layer_dim, activation='relu', input_dim=nfeats))\n","        str_model_id += '_L1_' + str(layer_dim)\n","        if len(list_batchnormalization) > i_layer and list_batchnormalization[i_layer]:\n","            model.add(BatchNormalization())\n","            str_model_id += '_BN'\n","        if len(list_dropouts) > i_layer and list_dropouts[i_layer] is not None:\n","            dropout_rate = list_dropouts[i_layer]\n","            model.add(Dropout(dropout_rate))\n","            str_model_id += '_DR' + str(np.round(dropout_rate, 2))\n","        \n","        # 中间层\n","        for i_layer in range(1, len(layers_sizes)):\n","            layer_dim = layers_sizes[i_layer]\n","            if layer_dim is None:\n","                break\n","            model.add(Dense(layer_dim, activation='relu'))\n","            str_model_id += '_L' + str(i_layer + 1) + '_' + str(layer_dim)\n","            if len(list_batchnormalization) > i_layer and list_batchnormalization[i_layer]:\n","                model.add(BatchNormalization())\n","                str_model_id += '_BN'\n","            if len(list_dropouts) > i_layer and list_dropouts[i_layer] is not None:\n","                dropout_rate = list_dropouts[i_layer]\n","                model.add(Dropout(dropout_rate))\n","                str_model_id += '_DR' + str(np.round(dropout_rate, 2))\n","        \n","        # 输出层\n","        model.add(Dense(nlabels, activation='sigmoid'))\n","        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[keras.metrics.AUC()])\n","    \n","    # GPU Logistic Regression\n","    elif model_config['id'] == 'gpuLogReg':\n","        if torch.cuda.is_available():\n","            from cuml.linear_model import LogisticRegression as CULogisticRegression\n","            model = MultiOutputClassifier(estimator=CULogisticRegression())\n","            str_model_id = 'gpuLogReg'\n","    \n","    # GPU CatBoost Classifier 默认模型\n","    elif model_config['id'] == 'gpuCatBClasdefault':\n","        model = MultiOutputClassifier(estimator=CatBoostClassifier(task_type='GPU', verbose=0))\n","        str_model_id = model_cfg[0]\n","    \n","    # GPU CatBoost Regressor 默认模型\n","    elif model_config['id'] == 'gpuCatBdefault':\n","        model = MultiOutputRegressor(estimator=CatBoostRegressor(task_type='GPU', verbose=0))\n","        str_model_id = model_cfg[0]\n","    \n","    # CatBoost Regressor 默认模型\n","    elif model_config['id'] == 'CatBdefault':\n","        model = MultiOutputRegressor(estimator=CatBoostRegressor(verbose=0))\n","        str_model_id = model_cfg[0]\n","    \n","    # LightGBM Regressor 默认模型\n","    elif model_config['id'] == 'LGBdefault':\n","        model = MultiOutputRegressor(estimator=lgbm.LGBMRegressor())\n","        str_model_id = 'LGBdefault'\n","    \n","    # 添加后缀名\n","    namepostfix = model_config.get('namepostfix', \"\")\n","    str_model_id += namepostfix\n","    \n","    return model, str_model_id\n","\n","\n","# 定义模型训练函数\n","\n","from torch.utils.data import TensorDataset, DataLoader\n","from Sophia.Sophia.Sophia import SophiaG\n","\n","def model_fit(model, X_train, Y_train, model_config, str_model_id='', verbose=1000):\n","    print('model_fit')\n","    if verbose >= 100:\n","        print('model_fit', str_model_id, model_config)\n","\n","    # 检查是否有自定义的模型训练函数\n","    if 'custom_model_fit_function_name' in model_config.keys():\n","        str_func_name = model_config['custom_model_fit_function_name']\n","        if verbose >= 1000:\n","            print('str_func_name', str_func_name)\n","        globals()[str_func_name](model, X_train, Y_train, model_config, str_model_id='', verbose=1000)\n","\n","    # 检查是否为pMLP模型\n","    elif 'pMLP' in str(model_config['id']):\n","        model_fit_pytorch(model, X_train, Y_train, model_config, str_model_id, verbose)\n","    else:\n","        dict_prm_for_fit = {t: model_config[t] for t in ['epochs', 'batch_size', 'verbose'] if t in model_config.keys()}\n","        if len(dict_prm_for_fit) == 0:\n","            model.fit(X_train, Y_train)\n","        else:\n","            model.fit(X_train, Y_train, **dict_prm_for_fit)\n","\n","# 定义PyTorch模型训练函数\n","def model_fit_pytorch(model, X_train, Y_train, model_config, str_model_id='', verbose=1000):\n","    print('model_fit_pytorch')\n","    criterion = model_config.get('criterion', nn.BCELoss())\n","    max_epoch = model_config.get('epochs', 10)\n","    BATCH_SIZE = model_config.get('batch_size', 128)\n","    LEARNING_RATE = model_config.get('LR', 0.001)\n","    optimizer = model_config.get('optimizer', torch.optim.Adam(model.parameters(), lr=LEARNING_RATE))\n","    lr_sched = model_config.get('lr_scheduler', None)\n","\n","    # 将数据转换为PyTorch张量并移动到设备上\n","    X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n","    Y_train = torch.tensor(Y_train, dtype=torch.float32).to(device)\n","    train_dataset = TensorDataset(X_train, Y_train)\n","    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","\n","    if verbose >= 100:\n","        print(str_model_id, 'Start model training. X_train.shape, Y_train.shape', X_train.shape, Y_train.shape)\n","    log_available_ram(f'After X_train initialization. Right before model train {str_model_id}')\n","\n","    # 模型训练\n","    for epoch in range(max_epoch):\n","        t0_epoch = time.time()\n","        model.train()\n","        for i_batch, (x_batch, y_batch) in enumerate(train_dataloader):\n","            optimizer.zero_grad()\n","            preds = model(x_batch)\n","            loss = criterion(preds, y_batch)\n","            loss.backward()\n","            optimizer.step()\n","\n","        if lr_sched is not None:\n","            lr_sched.step()\n","\n","        if (verbose >= 10) and (i_batch % 100 == 0):\n","            print(str_model_id, f'Epoch: {epoch}, batch: {i_batch}, train loss on batch: {loss.item():12.5f}, time: {time.time() - t0_epoch:.1f}')\n","\n","# 使用Sophia优化器的PyTorch模型训练函数\n","def model_fit_pytorch_Sophia_Andrey1(model, X_train, Y_train, model_config, str_model_id='', verbose=1000):\n","    print('model_fit_pytorch_Sophia_Andrey1')\n","    \n","    criterion = model_config.get('criterion', nn.BCELoss())\n","    max_epoch = model_config.get('epochs', 50)\n","    BATCH_SIZE = model_config.get('batch_size', 128)\n","    LEARNING_RATE = model_config.get('LR', 0.001)\n","    optimizer = SophiaG(model.parameters(), lr=LEARNING_RATE, betas=(0.965, 0.99), rho=0.01, weight_decay=1e-1)\n","\n","    # 将数据转换为PyTorch张量并移动到设备上\n","    X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n","    Y_train = torch.tensor(Y_train, dtype=torch.float32).to(device)\n","    train_dataset = TensorDataset(X_train, Y_train)\n","    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","\n","    if verbose >= 100:\n","        print(str_model_id, 'Start model training. X_train.shape, Y_train.shape', X_train.shape, Y_train.shape)\n","    log_available_ram(f'After X_train initialization. Right before model train {str_model_id}')\n","\n","    # 模型训练\n","    for epoch in range(max_epoch):\n","        print('epoch ======= ', epoch)\n","        t0_epoch = time.time()\n","        model.train()\n","        for i_batch, (x_batch, y_batch) in enumerate(train_dataloader):\n","            optimizer.zero_grad()\n","            preds = model(x_batch)\n","            loss = criterion(preds, y_batch)\n","            loss.backward()\n","            optimizer.step()\n","\n","        if (verbose >= 10) and (i_batch % 100 == 0):\n","            print(str_model_id, f'Epoch: {epoch}, batch: {i_batch}, train loss on batch: {loss.item():12.5f}, time: {time.time() - t0_epoch:.1f}')\n","\n","# 定义模型预测函数\n","def model_predict(model, XX, model_config, str_model_id='', verbose=1000):\n","    if verbose >= 100:\n","        print('model_predict', str_model_id, model_config)\n","\n","    if 'pMLP' in str(model_config['id']):\n","        Y_pred = model_predict_pytorch(model, XX, model_config, str_model_id, verbose)\n","    else:\n","        Y_pred = model.predict(XX)\n","\n","    return Y_pred\n","\n","# 定义PyTorch模型预测函数\n","def model_predict_pytorch(model, XX, model_config, str_model_id='', verbose=1000):\n","    print('model_predict_pytorch======',model)\n","    \n","    print('model_predict_pytorch======',XX)\n","    \n","    t0_submit = time.time()\n","    XX = torch.tensor(XX, dtype=torch.float32).to(device)\n","    model.eval()\n","    with torch.no_grad():\n","        Y_pred = model(XX).cpu().numpy()\n","\n","    if verbose >= 100:\n","        print(str_model_id, f'Y_pred.shape: {Y_pred.shape}, type(Y_pred): {type(Y_pred)}, predict on submit time: {time.time() - t0_submit:.1f}')\n","\n","    return Y_pred\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T00:21:19.205594Z","iopub.status.busy":"2024-05-23T00:21:19.205144Z","iopub.status.idle":"2024-05-23T00:21:25.796363Z","shell.execute_reply":"2024-05-23T00:21:25.794830Z","shell.execute_reply.started":"2024-05-23T00:21:19.205560Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/cafa-5-protein-function-prediction/Train/train_terms.tsv\n","(5363863, 3)\n","trainTerms memory_usage Mb: 128.73284\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>EntryID</th>\n","      <th>term</th>\n","      <th>aspect</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>A0A009IHW8</td>\n","      <td>GO:0008152</td>\n","      <td>BPO</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>A0A009IHW8</td>\n","      <td>GO:0034655</td>\n","      <td>BPO</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>A0A009IHW8</td>\n","      <td>GO:0072523</td>\n","      <td>BPO</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      EntryID        term aspect\n","0  A0A009IHW8  GO:0008152    BPO\n","1  A0A009IHW8  GO:0034655    BPO\n","2  A0A009IHW8  GO:0072523    BPO"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["CPU times: user 4.57 s, sys: 787 ms, total: 5.35 s\n","Wall time: 6.51 s\n"]}],"source":["%%time\n","######################################################################################\n","###############  CAFA5 metric\n","######################################################################################\n","\n","\n","# Evaluation for CAFA \n","# https://github.com/BioComputingUP/CAFA-evaluator\n","flag_correct_metric_computation_bug_found_by_Anton = True\n","# https://www.kaggle.com/competitions/cafa-5-protein-function-prediction/discussion/420241 - Anton Vakhrushev - correcting error in the initial code of the metric computation - please upvote \n","import numpy as np\n","import pandas as pd\n","import multiprocessing as mp\n","import copy\n","import logging\n","\n","class Graph:\n","    \"\"\"\n","    Ontology class. One ontology == one namespace\n","    DAG is the adjacence matrix (sparse) which represent a Directed Acyclic Graph where\n","    DAG(i,j) == 1 means that the go term i is_a (or is part_of) j\n","    Parents that are in a different namespace are discarded\n","    \"\"\"\n","    def __init__(self, namespace, terms_dict, ia_dict=None, orphans=False):\n","        \"\"\"\n","        terms_dict = {term: {name: , namespace: , def: , alt_id: , rel:}}\n","        \"\"\"\n","        self.namespace = namespace\n","        self.dag = []  # [[], ...] terms (rows, axis 0) x parents (columns, axis 1)\n","        self.terms_dict = {}  # {term: {index: , name: , namespace: , def: }  used to assign term indexes in the gt\n","        self.terms_list = []  # [{id: term, name:, namespace: , def:, adg: [], children: []}, ...]\n","        self.idxs = None  # Number of terms\n","        self.order = None\n","        self.toi = None\n","        self.ia = None\n","\n","        rel_list = []\n","        for self.idxs, (term_id, term) in enumerate(terms_dict.items()):\n","            rel_list.extend([[term_id, rel, term['namespace']] for rel in term['rel']])\n","            self.terms_list.append({'id': term_id, 'name': term['name'], 'namespace': namespace, 'def': term['def'],\n","                                 'adj': [], 'children': []})\n","            self.terms_dict[term_id] = {'index': self.idxs, 'name': term['name'], 'namespace': namespace, 'def': term['def']}\n","            for a_id in term['alt_id']:\n","                self.terms_dict[a_id] = copy.copy(self.terms_dict[term_id])\n","        self.idxs += 1\n","\n","        self.dag = np.zeros((self.idxs, self.idxs), dtype='bool')\n","\n","        # id1 term (row, axis 0), id2 parent (column, axis 1)\n","        for id1, id2, ns in rel_list:\n","            if self.terms_dict.get(id2):\n","                i = self.terms_dict[id1]['index']\n","                j = self.terms_dict[id2]['index']\n","                self.dag[i, j] = 1\n","                self.terms_list[i]['adj'].append(j)\n","                self.terms_list[j]['children'].append(i)\n","                logging.debug(\"i,j {},{} {},{}\".format(i, j, id1, id2))\n","            else:\n","                logging.debug('Skipping branch to external namespace: {}'.format(id2))\n","        logging.debug(\"dag {}\".format(self.dag))\n","        # Topological sorting\n","        self.top_sort()\n","        logging.debug(\"order sorted {}\".format(self.order))\n","\n","        if orphans:\n","            self.toi = np.arange(self.dag.shape[0])  # All terms, also those without parents\n","        else:\n","            self.toi = np.nonzero(self.dag.sum(axis=1) > 0)[0]  # Only terms with parents\n","        logging.debug(\"toi {}\".format(self.toi))\n","\n","        if ia_dict is not None:\n","            self.set_ia(ia_dict)\n","\n","        return\n","\n","    def top_sort(self):\n","        \"\"\"\n","        Takes a sparse matrix representing a DAG and returns an array with nodes indexes in topological order\n","        https://en.wikipedia.org/wiki/Topological_sorting\n","        \"\"\"\n","        indexes = []\n","        visited = 0\n","        (rows, cols) = self.dag.shape\n","\n","        # create a vector containing the in-degree of each node\n","        in_degree = self.dag.sum(axis=0)\n","        # logging.debug(\"degree {}\".format(in_degree))\n","\n","        # find the nodes with in-degree 0 (leaves) and add them to the queue\n","        queue = np.nonzero(in_degree == 0)[0].tolist()\n","        # logging.debug(\"queue {}\".format(queue))\n","\n","        # for each element of the queue increment visits, add them to the list of ordered nodes\n","        # and decrease the in-degree of the neighbor nodes\n","        # and add them to the queue if they reach in-degree == 0\n","        while queue:\n","            visited += 1\n","            idx = queue.pop(0)\n","            indexes.append(idx)\n","            in_degree[idx] -= 1\n","            l = self.terms_list[idx]['adj']\n","            if len(l) > 0:\n","                for j in l:\n","                    in_degree[j] -= 1\n","                    if in_degree[j] == 0:\n","                        queue.append(j)\n","\n","        # if visited is equal to the number of nodes in the graph then the sorting is complete\n","        # otherwise the graph can't be sorted with topological order\n","        if visited == rows:\n","            self.order = indexes\n","        else:\n","            raise Exception(\"The sparse matrix doesn't represent an acyclic graph\")\n","\n","    def set_ia(self, ia_dict):\n","        self.ia = np.zeros(self.idxs, dtype='float')\n","        for term_id in self.terms_dict:\n","            if ia_dict.get(term_id):\n","                self.ia[self.terms_dict[term_id]['index']] = ia_dict.get(term_id)\n","            else:\n","                logging.debug('Missing IA for term: {}'.format(term_id))\n","        # Convert inf to zero\n","        np.nan_to_num(self.ia, copy=False, nan=0, posinf=0, neginf=0)\n","        self.toi = np.nonzero(self.ia > 0)[0]\n","\n","\n","class Prediction:\n","    \"\"\"\n","    The score matrix contains the scores given by the predictor for every node of the ontology\n","    \"\"\"\n","    def __init__(self, ids, matrix, idx, namespace=None):\n","        self.ids = ids\n","        self.matrix = matrix  # scores\n","        self.next_idx = idx\n","        # self.n_pred_seq = idx + 1\n","        self.namespace = namespace\n","\n","    def __str__(self):\n","        return \"\\n\".join([\"{}\\t{}\\t{}\".format(index, self.matrix[index], self.namespace) for index, _id in enumerate(self.ids)])\n","\n","\n","class GroundTruth:\n","    def __init__(self, ids, matrix, namespace=None):\n","        self.ids = ids\n","        self.matrix = matrix\n","        self.namespace = namespace\n","\n","\n","def propagate(matrix, ont, order, mode='max'):\n","    \"\"\"\n","    Update inplace the score matrix (proteins x terms) up to the root taking the max between children and parents\n","    \"\"\"\n","    if matrix.shape[0] == 0:\n","        raise Exception(\"Empty matrix\")\n","\n","    deepest = np.where(np.sum(matrix[:, order], axis=0) > 0)[0][0]\n","    if deepest.size == 0:\n","        raise Exception(\"The matrix is empty\")\n","\n","    # Remove leaves\n","    order_ = np.delete(order, [range(0, deepest)])\n","\n","    for i in order_:\n","        # Get direct children\n","        children = np.where(ont.dag[:, i] != 0)[0]\n","        if children.size > 0:\n","            cols = np.concatenate((children, [i]))\n","            if mode == 'max':\n","                matrix[:, i] = matrix[:, cols].max(axis=1)\n","            elif mode == 'fill':\n","                rows = np.where(matrix[:, i] == 0)[0]\n","                if rows.size:\n","                    idx = np.ix_(rows, cols)\n","                    if flag_correct_metric_computation_bug_found_by_Anton:\n","                        # Corrected way (see https://www.kaggle.com/competitions/cafa-5-protein-function-prediction/discussion/420241 )\n","                        matrix[rows, i] = matrix[idx].max(axis=1) #  matrix[idx].max(axis=1)[0] # Correction: https://www.kaggle.com/competitions/cafa-5-protein-function-prediction/discussion/420241\n","                    else:\n","                        # Old way - not corrected\n","                        matrix[rows, i] = matrix[idx].max(axis=1)[0] # Correction: https://www.kaggle.com/competitions/cafa-5-protein-function-prediction/discussion/420241\n","                        \n","    return\n","\n","\n","def obo_parser(obo_file, valid_rel=(\"is_a\", \"part_of\")):\n","    \"\"\"\n","    Parse a OBO file and returns a list of ontologies, one for each namespace.\n","    Obsolete terms are excluded as well as external namespaces.\n","    \"\"\"\n","    term_dict = {}\n","    term_id = None\n","    namespace = None\n","    name = None\n","    term_def = None\n","    alt_id = []\n","    rel = []\n","    obsolete = True\n","    with open(obo_file) as f:\n","        for line in f:\n","            line = line.strip().split(\": \")\n","            if line and len(line) > 1:\n","                k = line[0]\n","                v = \": \".join(line[1:])\n","                if k == \"id\":\n","                    # Populate the dictionary with the previous entry\n","                    if term_id is not None and obsolete is False and namespace is not None:\n","                        term_dict.setdefault(namespace, {})[term_id] = {'name': name,\n","                                                                       'namespace': namespace,\n","                                                                       'def': term_def,\n","                                                                       'alt_id': alt_id,\n","                                                                       'rel': rel}\n","                    # Assign current term ID\n","                    term_id = v\n","\n","                    # Reset optional fields\n","                    alt_id = []\n","                    rel = []\n","                    obsolete = False\n","                    namespace = None\n","\n","                elif k == \"alt_id\":\n","                    alt_id.append(v)\n","                elif k == \"name\":\n","                    name = v\n","                elif k == \"namespace\" and v != 'external':\n","                    namespace = v\n","                elif k == \"def\":\n","                    term_def = v\n","                elif k == 'is_obsolete':\n","                    obsolete = True\n","                elif k == \"is_a\" and k in valid_rel:\n","                    s = v.split('!')[0].strip()\n","                    rel.append(s)\n","                elif k == \"relationship\" and v.startswith(\"part_of\") and \"part_of\" in valid_rel:\n","                    s = v.split()[1].strip()\n","                    rel.append(s)\n","\n","        # Last record\n","        if obsolete is False and namespace is not None:\n","            term_dict.setdefault(namespace, {})[term_id] = {'name': name,\n","                                                          'namespace': namespace,\n","                                                          'def': term_def,\n","                                                          'alt_id': alt_id,\n","                                                          'rel': rel}\n","    return term_dict\n","\n","\n","def gt_parser(gt_file, ontologies):\n","    \"\"\"\n","    Parse ground truth file. Discard terms not included in the ontology.\n","    \"\"\"\n","    gt_dict = {}\n","    with open(gt_file) as f:\n","        for line in f:\n","            line = line.strip().split()\n","            if line:\n","                p_id, term_id = line[:2]\n","                for ont in ontologies:\n","                    if term_id in ont.terms_dict:\n","                        gt_dict.setdefault(ont.namespace, {}).setdefault(p_id, []).append(term_id)\n","                        break\n","\n","    gts = {}\n","    for ont in ontologies:\n","        if gt_dict.get(ont.namespace):\n","            matrix = np.zeros((len(gt_dict[ont.namespace]), ont.idxs), dtype='bool')\n","            ids = {}\n","            for i, p_id in enumerate(gt_dict[ont.namespace]):\n","                ids[p_id] = i\n","                for term_id in gt_dict[ont.namespace][p_id]:\n","                    matrix[i, ont.terms_dict[term_id]['index']] = 1\n","            propagate(matrix, ont, ont.order, mode='max')\n","            gts[ont.namespace] = GroundTruth(ids, matrix, ont.namespace)\n","\n","    return gts\n","\n","\n","def pred_parser(f, ontologies, gts, prop_mode, max_terms=None):\n","    \"\"\"\n","    Parse a prediction file and returns a list of prediction objects, one for each namespace.\n","    If a predicted is predicted multiple times for the same target, it stores the max.\n","    This is the slow step if the input file is huge, ca. 1 minute for 5GB input on SSD disk.\n","    \"\"\"\n","    ids = {}\n","    matrix = {}\n","    ns_dict = {}  # {namespace: term}\n","    onts = {ont.namespace: ont for ont in ontologies}\n","    for ns in gts:\n","        matrix[ns] = np.zeros(gts[ns].matrix.shape, dtype='float')\n","        ids[ns] = {}\n","        for term in onts[ns].terms_dict:\n","            ns_dict[term] = ns\n","\n","    for line in f:\n","        p_id, term_id, prob = line\n","        ns = ns_dict.get(term_id)\n","        if ns in gts and p_id in gts[ns].ids:\n","            i = gts[ns].ids[p_id]\n","            if max_terms is None or np.count_nonzero(matrix[ns][i]) <= max_terms:\n","                j = onts[ns].terms_dict.get(term_id)['index']\n","                ids[ns][p_id] = i\n","                matrix[ns][i, j] = max(matrix[ns][i, j], float(prob))\n","\n","    predictions = []\n","    for ns in ids:\n","        if ids[ns]:\n","            propagate(matrix[ns], onts[ns], onts[ns].order, mode=prop_mode)\n","            predictions.append(Prediction(ids[ns], matrix[ns], len(ids[ns]), ns))\n","\n","    if not predictions:\n","        raise Exception(\"Empty prediction, check format\")\n","\n","    return predictions\n","\n","\n","def ia_parser(file):\n","    ia_dict = {}\n","    with open(file) as f:\n","        for line in f:\n","            if line:\n","                term, ia = line.strip().split()\n","                ia_dict[term] = float(ia)\n","    return ia_dict\n","\n","# Computes the root terms in the dag\n","def get_roots_idx(dag):\n","    return np.where(dag.sum(axis=1) == 0)[0]\n","\n","\n","# Computes the leaf terms in the dag\n","def get_leafs_idx(dag):\n","    return np.where(dag.sum(axis=0) == 0)[0]\n","\n","\n","# Return a mask for all the predictions (matrix) >= tau\n","def solidify_prediction(pred, tau):\n","    return pred >= tau\n","\n","\n","# computes the f metric for each precision and recall in the input arrays\n","def compute_f(pr, rc):\n","    n = 2 * pr * rc\n","    d = pr + rc\n","    return np.divide(n, d, out=np.zeros_like(n, dtype=float), where=d != 0)\n","\n","\n","def compute_s(ru, mi):\n","    return np.sqrt(ru**2 + mi**2)\n","    # return np.where(np.isnan(ru), mi, np.sqrt(ru + np.nan_to_num(mi)))\n","\n","import time\n","from scipy.sparse import csr_matrix\n","\n","def compute_metrics_(tau_arr, g, pred, toi, n_gt, wn_gt=None, ic_arr=None):\n","\n","    verbose = 0;         \n","\n","    if verbose >= 10:\n","        t0 = time.time()\n","    \n","    metrics = np.zeros((len(tau_arr), 7), dtype='float')  # cov, pr, rc, wpr, wrc, ru, mi\n","\n","    if verbose >= 10:\n","        print('type(toi), toi', type(toi), toi )\n","    tmp = pred.matrix[:, toi]\n","    if verbose >= 10:\n","        print('type(tmp), tmp.shape', type(tmp), tmp.shape )\n","    p_s = csr_matrix(tmp )\n","    ic_arr_toi = ic_arr[toi]\n","    if verbose >= 10:\n","        print('type(ic_arr_toi), ic_arr_toi.shape', type(ic_arr_toi), ic_arr_toi.shape )\n","\n","    \n","    g_s = csr_matrix( g )\n","    \n","    if verbose >= 10:\n","        print( 'csr_matrix done %.1f'%(time.time( ) - t0 ), 'p_s.shape, g_s.shape:', p_s.shape, g_s.shape )    \n","\n","\n","    \n","    for i, tau in enumerate(tau_arr):\n","\n","        if verbose >= 100:\n","            t0 = time.time()\n","            print()\n","            print(i, tau, 'Start %.1f'%(time.time( ) - t0 ) )\n","        p = p_s > tau # solidify_prediction(p, tau)\n","        if verbose >= 100:\n","            print(i, tau, 'solidify done %.1f'%(time.time( ) - t0 ), 'p.shape:', p.shape,  )\n","\n","        # number of proteins with at least one term predicted with score >= tau\n","        metrics[i, 0] = (p.sum(axis=1) > 0).sum()\n","\n","        # Terms subsets\n","        intersection = p.multiply( g_s)  # TP\n","\n","\n","        if ic_arr is not None:\n","            \n","            # Weighted precision, recall\n","            wn_pred = p.dot( ic_arr_toi)\n","            wn_intersection =  intersection.dot( ic_arr_toi )\n","            \n","            if verbose >= 100:\n","                print(i, tau, 'After w_pred wn_intersection  %.1f'%(time.time( ) - t0 ) )\n","            \n","            metrics[i, 3] = np.divide(wn_intersection, wn_pred, out=np.zeros( wn_intersection.shape, dtype='float'),\n","                                      where=wn_pred > 0).sum()\n","            metrics[i, 4] = np.divide(wn_intersection, wn_gt, out=np.zeros(wn_intersection.shape, dtype='float'),\n","                                      where=n_gt > 0).sum()\n","            if verbose >= 100:\n","                print(i, tau, 'After metrics 3,4   %.1f'%(time.time( ) - t0 ) )\n","\n","\n","    return metrics\n","\n","\n","def compute_metrics(pred, gt, toi, tau_arr, ic_arr=None, n_cpu=0):\n","    \"\"\"\n","    Takes the prediction and the ground truth and for each threshold in tau_arr\n","    calculates the confusion matrix and returns the coverage,\n","    precision, recall, remaining uncertainty and misinformation.\n","    Toi is the list of terms (indexes) to be considered\n","    \"\"\"\n","    g = gt.matrix[:, toi]\n","    n_gt = g.sum(axis=1)\n","    wn_gt = None\n","    if ic_arr is not None:\n","        wn_gt = (g * ic_arr[toi]).sum(axis=1)\n","\n","    # Parallelization\n","    if n_cpu == 0:\n","        n_cpu = mp.cpu_count()\n","\n","    arg_lists = [[tau_arr, g, pred, toi, n_gt, wn_gt, ic_arr] for tau_arr in np.array_split(tau_arr, n_cpu)]\n","    if 0:\n","        # Original parallel way (# It does not work on Kaggle)\n","        arg_lists = [[tau_arr, g, pred, toi, n_gt, wn_gt, ic_arr] for tau_arr in np.array_split(tau_arr, n_cpu)]\n","        with mp.Pool(processes=n_cpu) as pool:\n","            metrics = np.concatenate(pool.starmap(compute_metrics_, arg_lists), axis=0)\n","    else: \n","        # no-parallel: \n","        metrics = compute_metrics_(tau_arr, g, pred, toi, n_gt, wn_gt, ic_arr )\n","\n","    return pd.DataFrame(metrics, columns=[\"cov\", \"pr\", \"rc\", \"wpr\", \"wrc\", \"ru\", \"mi\"])\n","\n","\n","def evaluate_prediction(prediction, gt, ontologies, tau_arr, normalization='cafa', n_cpu=0):\n","    dfs = []\n","    for p in prediction:\n","        ns = p.namespace\n","        ne = np.full(len(tau_arr), gt[ns].matrix.shape[0])\n","\n","        ont = [o for o in ontologies if o.namespace == ns][0]\n","\n","        # cov, pr, rc, wpr, wrc, ru, mi\n","        metrics = compute_metrics(p, gt[ns], ont.toi, tau_arr, ont.ia, n_cpu)\n","\n","        for column in [\"pr\", \"rc\", \"wpr\", \"wrc\", \"ru\", \"mi\"]:\n","            if normalization == 'gt' or (column in [\"rc\", \"wrc\"] and normalization == 'cafa'):\n","                metrics[column] = np.divide(metrics[column], ne, out=np.zeros_like(metrics[column], dtype='float'), where=ne > 0)\n","            else:\n","                metrics[column] = np.divide(metrics[column], metrics[\"cov\"], out=np.zeros_like(metrics[column], dtype='float'), where=metrics[\"cov\"] > 0)\n","\n","        metrics['ns'] = [ns] * len(tau_arr)\n","        metrics['tau'] = tau_arr\n","        metrics['cov'] = np.divide(metrics['cov'], ne, out=np.zeros_like(metrics['cov'], dtype='float'), where=ne > 0)\n","        metrics['f'] = compute_f(metrics['pr'], metrics['rc'])\n","        metrics['wf'] = compute_f(metrics['wpr'], metrics['wrc'])\n","        metrics['s'] = compute_s(metrics['ru'], metrics['mi'])\n","\n","        dfs.append(metrics)\n","\n","    return pd.concat(dfs)\n","\n","# Tau array, used to compute metrics at different score thresholds\n","th_step = 0.01\n","tau_arr = np.arange(0.01, 1, th_step)\n","#Consider terms without parents, e.g. the root(s), in the evaluation\n","no_orphans = False\n","# Parse and set information accretion (optional)\n","ia_dict = ia_parser('/kaggle/input/cafa-5-protein-function-prediction/IA.txt')\n","\n","# Parse the OBO file and creates a different graph for each namespace\n","ontologies = []\n","obo_file = '/kaggle/input/cafa-5-protein-function-prediction/Train/go-basic.obo'\n","for ns, terms_dict in obo_parser(obo_file).items():\n","    ontologies.append(Graph(ns, terms_dict, ia_dict, not no_orphans))\n","    \n","    \n","\n","######################################################################################\n","###############  Wrapper to call CAFA5 metric computation\n","######################################################################################\n","fn = '/kaggle/input/cafa-5-protein-function-prediction/Train/train_terms.tsv'\n","print(fn)\n","trainTerms = pd.read_csv(fn, sep=\"\\t\")\n","print(trainTerms.shape)\n","print('trainTerms memory_usage Mb:', trainTerms.memory_usage().sum()/1e6  )\n","display(trainTerms.head(3))\n","\n","def get_F1_etc_scores_official_CAFA_evaluation( Y_pred, IX, cutoff_threshold_low = 0.01,    make_plots = True ,  verbose = 0 ): \n","    '''\n","    Computation of F1-weighted scores are called here. \n","    Here we prepare Y_pred, Y in format required by functions provided by organizers - see github: https://github.com/BioComputingUP/CAFA-evaluator\n","    Y_pred  -  predictions\n","    IX  - indices selecting part which correspond to Y_pred in full Y \n","    Params:\n","    cutoff_threshold_low - predictions lower (strictly) will be dropped (effectively set to zero)\n","        (!) higher cutoff_threshold_low will improve  both RAM/speed. For most models 0.1 is Okay, and even 0.18. Consider using 0.1-0.15.   \n","    make_plots = True  - create plots of  F1,precision,recall (weighted) depending on threshold  \n","    verbose = 0\n","    \n","    Function uses external variables: \n","    trainTerms - training labels provided by orgs: /kaggle/input/cafa-5-protein-function-prediction/Train/train_terms.tsv\n","    vec_train_protein_ids - ids of the proteins in the current train - should correspond to \"X\" - features \n","    ontologies - data from: /kaggle/input/cafa-5-protein-function-prediction/Train/go-basic.obo\n","    tau_arr - array of thresholds \n","    \n","    Note: pay attention - Y_true is NOT an input argument. (Unusual for metric computation). \n","    We will create Y_true here from \"trainTerms\" cutting only those proteins which correspond to Y_pred indexes \"IX\".\n","    Thus it is highly important pass here the correct \"IX\" i.e. corresponding to Y_pred, otherwise results will not be correct\n","    '''\n","\n","    t00 = time.time()\n","    if verbose >= 100:\n","        print('Scoring starts. n_samples:', len(IX) )\n","\n","    ##########################################################################################\n","    # Prepare \"ground truth\" - \"gt\" terms(labels) in required format  \n","    ##########################################################################################\n","\n","    # First save to file, because function \"gt_parser\" works with files as input \n","    # Only part corresponding to providex indices IX will be generated \n","    t0 = time.time()\n","    trainTerms[ trainTerms.EntryID.isin(vec_train_protein_ids[IX]) ].to_csv('valid.tsv', sep='\\t', index=False) # Wall time: 4.11 s  for 28k samples\n","    if verbose >= 1000:\n","        print('save valid.csv %.1f'%(time.time() - t0 )) \n","\n","    # Prepare \"gt\" labels \n","    t0 = time.time()\n","    gt = gt_parser('valid.tsv', ontologies) # Wall time: 1min 22s  for 28k samples\n","    if verbose >= 100:\n","        print('gt_parser %.1f'%(time.time() - t0 ))\n","\n","    ##########################################################################################\n","    # prepare predicitons as list of triples - (protein, term(label), prediction) \n","    ##########################################################################################\n","\n","    t0 = time.time()\n","    vec_train_protein_ids_loc = vec_train_protein_ids[IX]\n","    preds = []\n","    for i in range(len(vec_train_protein_ids_loc)):\n","        for j in range(len(labels_to_consider)):\n","            if Y_pred[i,j] >= cutoff_threshold_low:\n","                preds.append((vec_train_protein_ids_loc[i], \n","                              labels_to_consider[j],\n","                              Y_pred[i,j]                        ))\n","    if verbose >= 1000:            \n","        print('create preds %.1f'%(time.time() - t0 ))       \n","\n","    ##########################################################################################\n","    # Parse predictions - propagation happens here  \n","    ##########################################################################################\n","    t0 = time.time()\n","    preds = pred_parser(preds, ontologies, gt, prop_mode='fill', max_terms=500) # \n","    if verbose >= 1000:            \n","        print('pred_parser %.1f'%(time.time() - t0 ), 'len(preds)', len(preds) )            \n","\n","    gc.collect()\n","\n","    ##########################################################################################\n","    # Main scores calculations happends here: \n","    ##########################################################################################\n","    # %%time\n","    t0 = time.time()\n","    df_metrics = evaluate_prediction(preds, gt, ontologies, tau_arr, n_cpu=1) # Wall time: 37.7 s for 28k samples\n","    if verbose >= 1000:            \n","        print('evaluate_prediction %.1f'%(time.time() - t0 ), 'got df_metrics with shape:', df_metrics.shape )            \n","    if verbose >= 10000:            \n","        display( df_metrics.head(2) )\n","\n","        \n","    ##########################################################################################\n","    # Comptutations finished. Below are optional plots, output preparartions etc.  \n","    ##########################################################################################\n","    if verbose >= 100:\n","        _t = df_metrics.groupby('ns').agg({'wf':'max'})\n","        display( _t )\n","        print( _t.mean() ) \n","\n","    if verbose >= 100:\n","        print('F1-scoring finished. %.1f secs passed'%(time.time() - t00 ))\n","\n","    # %%time\n","    if make_plots:\n","        try:\n","            list_uv = list(df_metrics['ns'].unique() )\n","            #print(list_uv)\n","            fig = plt.figure(figsize = (20,4))\n","            i0 = 0;\n","            for  col in  ['wf', 'wpr', 'wrc' ] :\n","                i0+=1\n","    #             print(i0,col)\n","                fig.add_subplot(1,3,i0)\n","\n","                for uv in list_uv:\n","                    mask = df_metrics['ns'] == uv\n","                    v = df_metrics[mask][col]\n","                    plt.plot(v.values, label = uv)\n","                plt.title(col, fontsize  = 20)\n","                plt.legend()\n","                plt.grid()\n","            plt.show()        \n","        except:\n","            print('Exception in plot')\n","    \n","    ########################################################################################\n","    # Prepare output of scores : \n","    ########################################################################################\n","    _t = {'cellular_component':'CCO', 'biological_process':'BPO','molecular_function':'MFO'}\n","    dict_scores_etc = {}\n","    df_s = df_metrics.groupby('ns').agg({'wf':'max'})\n","    dict_scores_etc['F1w'] = np.round( df_s.mean().iloc[0], 6) \n","    for k in _t:\n","        k2 = _t[k]\n","        # print(k,dict_scores_etc )\n","        if k in  df_s.index:\n","            dict_scores_etc['F1 '+ k2 ] = np.round( df_s.loc[k].iat[0], 6) \n","        else:\n","            dict_scores_etc['F1 '+ k2 ] = 0\n","\n","    ########################################################################################\n","    # Prepare output of thresholds : \n","    ########################################################################################\n","    for k in _t:\n","        k2 = _t[k]\n","        m = df_metrics['ns'] == k\n","        if m.sum()>0:\n","            IX = df_metrics[m]['wf'].argmax()\n","            thres_optimal = df_metrics[m]['tau'].iat[IX]\n","            dict_scores_etc['thres '+ k2 ] = thres_optimal\n","        else:\n","            dict_scores_etc['thres '+ k2 ] = 0\n","            \n","    dict_scores_etc['F-Scores Time'] = np.round( time.time() - t00   ,1)         \n","    if verbose >= 100:\n","        print('Scores: ', dict_scores_etc)        \n","\n","    if os.path.isfile('valid.tsv') :\n","        os.remove('valid.tsv')\n","        \n","    return  dict_scores_etc   "]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T00:21:55.226239Z","iopub.status.busy":"2024-05-23T00:21:55.225827Z","iopub.status.idle":"2024-05-23T00:21:55.240785Z","shell.execute_reply":"2024-05-23T00:21:55.239334Z","shell.execute_reply.started":"2024-05-23T00:21:55.226206Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 7 µs, sys: 0 ns, total: 7 µs\n","Wall time: 10.7 µs\n"]}],"source":["%%time\n","######################################################################################\n","###############  Auxilliary functions compute/save scores\n","######################################################################################\n","\n","def update_modeling_stat(df_stat, Y_pred, Y, flag_compute_cafa_f1=False, str_model_id='', dict_optional_info={}, verbose=0):\n","    \"\"\"\n","    计算/存储/保存模型的分数/指标/统计信息\n","    \"\"\"\n","    if verbose >= 100:\n","        print('Scoring starts')          \n","\n","    list_folds_ix = np.sort(list(set(folds)))\n","    for ix_fold in list_folds_ix[:n_folds_to_process]:\n","        t0 = time.time()\n","        IX_df_stat = len(df_stat) + 1\n","        mask_fold = folds == ix_fold\n","        IX_loc = np.where(mask_fold > 0)[0]\n","        \n","        # 记录模型ID和Fold编号\n","        df_stat.loc[IX_df_stat, 'Model'] = str_model_id\n","        df_stat.loc[IX_df_stat, 'Fold'] = ix_fold\n","\n","        # 计算AUC得分\n","        auc_score = roc_auc_score(Y[IX_loc, :].ravel(), Y_pred[IX_loc, :].ravel())\n","        df_stat.loc[IX_df_stat, 'AUC'] = np.round(auc_score, 5)\n","\n","        ####################################################################\n","        # 调用CAFA-F1计算 - 慢且消耗RAM\n","        ####################################################################\n","        if flag_compute_cafa_f1:\n","            dict_scores_etc = get_F1_etc_scores_official_CAFA_evaluation(\n","                Y_pred[IX_loc, :], IX_loc, cutoff_threshold_low=cutoff_threshold_low,\n","                make_plots=True, verbose=10000\n","            )\n","            for k in dict_scores_etc:\n","                df_stat.loc[IX_df_stat, k] = dict_scores_etc[k]\n","\n","            for t in [0.2, 0.3, 0.4, 0.5]:\n","                _c = (Y_pred >= t).ravel().sum()\n","                df_stat.loc[IX_df_stat, 'GE%.1f per prot' % t] = np.round(_c / Y.shape[0])\n","\n","            log_available_ram(f'CAFA-F1 scoring fold {ix_fold} finished. Model {str_model_id}')\n","\n","        # 记录样本和目标数量\n","        df_stat.loc[IX_df_stat, 'n_targets'] = Y.shape[1]\n","        df_stat.loc[IX_df_stat, 'n_samples Val'] = Y.shape[0]\n","        df_stat.loc[IX_df_stat, 'Time scoring'] = np.round(time.time() - t0, 1)\n","\n","        # 记录可选信息\n","        for k, val in dict_optional_info.items():\n","            df_stat.loc[IX_df_stat, k] = val\n","\n","        try:\n","            df_stat.loc[IX_df_stat, 'Features'] = str(list_features_id)\n","            df_stat.loc[IX_df_stat, 'Model Features'] = str_model_id.split(' ')[-1] + ' ' + str(list_features_id)\n","        except:\n","            pass\n","\n","        torch.cuda.empty_cache()\n","        gc.collect()\n","        df_stat.to_csv('df_stat.csv')\n","\n","    if verbose > 0:\n","        display(df_stat.tail(n_folds_to_process))\n","        print('Scoring finished. Seconds passed:  %.1f' % (time.time() - t0))\n","\n","    return df_stat"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T00:24:26.446815Z","iopub.status.busy":"2024-05-23T00:24:26.444391Z","iopub.status.idle":"2024-05-23T00:24:52.681786Z","shell.execute_reply":"2024-05-23T00:24:52.680600Z","shell.execute_reply.started":"2024-05-23T00:24:26.446703Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Before modeling\n","Available RAM: 27.69 G  Current datetime: 2024-05-23 00:24:26.471793\n","Y_submit mbytes: 135.29300689697266\n","Y_pred_oof_blend mbytes: 135.65635681152344\n","Right before modeling\n","Available RAM: 27.69 G  Current datetime: 2024-05-23 00:24:26.476669\n","\n","Start training models 2024-05-23 00:24:26.492524\n","\n","model_fit\n","model_fit_pytorch_Sophia_Andrey1\n"," Start model training. X_train.shape, Y_train.shape torch.Size([34673, 2304]) torch.Size([34673, 500])\n","After X_train initialization. Right before model train \n","Available RAM: 26.99 G  Current datetime: 2024-05-23 00:24:27.145817\n","epoch =======  0\n","epoch =======  1\n","After Model fit. ix_fold 0, i_selfblend, 0, 0 pMLP_Andrey 0\n","Available RAM: 27.74 G  Current datetime: 2024-05-23 00:24:51.483935\n","model_predict_pytorch====== Model5(\n","  (activation): PReLU(num_parameters=1)\n","  (bn1): BatchNorm1d(2304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (fc1): Linear(in_features=2304, out_features=800, bias=True)\n","  (ln1): LayerNorm((800,), eps=1e-05, elementwise_affine=True)\n","  (bn2): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (fc2): Linear(in_features=800, out_features=600, bias=True)\n","  (ln2): LayerNorm((600,), eps=1e-05, elementwise_affine=True)\n","  (bn3): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (fc3): Linear(in_features=600, out_features=400, bias=True)\n","  (ln3): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n","  (bn4): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (fc4): Linear(in_features=1200, out_features=500, bias=True)\n","  (ln4): LayerNorm((500,), eps=1e-05, elementwise_affine=True)\n","  (sigm): Sigmoid()\n",")\n","model_predict_pytorch====== None\n"]},{"ename":"TypeError","evalue":"must be real number, not NoneType","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","File \u001b[0;32m<timed exec>:91\u001b[0m\n","File \u001b[0;32m<timed exec>:259\u001b[0m, in \u001b[0;36mmodel_predict\u001b[0;34m(model, XX, model_config, str_model_id, verbose)\u001b[0m\n","File \u001b[0;32m<timed exec>:272\u001b[0m, in \u001b[0;36mmodel_predict_pytorch\u001b[0;34m(model, XX, model_config, str_model_id, verbose)\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: must be real number, not NoneType"]}],"source":["%%time\n","import gc \n","import time \n","import datetime\n","log_available_ram('Before modeling')\n","\n","######################### Params ##################################################3\n","\n","if 0: \n","    mode_submit = True \n","verbose = 0\n","\n","\n","######################### Output ##################################################\n","df_stat = pd.DataFrame()\n","\n","if (mode_submit is not None) and ( mode_submit != False ):\n","    Y_submit = np.zeros( (141865, Y.shape[1] )  , dtype = np.float16 )  # Predictions for submission will be stored here \n","    # Results from all models and all folds will be blended \n","    print('Y_submit mbytes:', Y_submit.nbytes/1024/1024)\n","cnt_blend_submit = 0 ;  \n","\n","if  flag_compute_oof_predictions:\n","    Y_pred_oof_blend  = np.zeros( ( Y.shape )  , dtype = np.float16 )\n","    print('Y_pred_oof_blend mbytes:', Y_pred_oof_blend.nbytes/1024/1024)\n","cnt_blend_oof = -1;\n","\n","\n","########################## Preparations ###########################################\n","log_available_ram('Right before modeling')\n","\n","if flag_compute_stat_for_each_model:  # Predictions OOF for each particular model - will be rewritten for each modelling \n","    Y_pred_oof = np.zeros( ( Y.shape )  , dtype = np.float16 )\n","    print('Y_pred_oof mbytes:', Y_pred_oof.nbytes/1024/1024)\n","\n","i_model = -1 # \n","i_config = -1 # conter for configurations \n","t0modeling = time.time()\n","list_folds_ix =  np.sort(list ( set(folds))  )\n","print(); print('Start training models',datetime.datetime.now()) ; print()\n","########################## Main modelling  ###########################################\n","for main_config_model_feature_etc  in list_main_config_model_feature_etc:\n","    i_config += 1 \n","    model_config = main_config_model_feature_etc['model']\n","    if ('Keras' in model_config.keys() ) and ( model_config['Keras'] ): continue   # Keras models will be processed in the next cell - RAM leak problem\n","    if 'list_features_id' in main_config_model_feature_etc.keys():\n","        print()\n","        X,vec_train_protein_ids,X_submit,  submit_protein_ids = get_features(main_config_model_feature_etc['list_features_id'], verbose = 100)\n","        gc.collect()\n","        log_available_ram(f\"New features loaded:  {str(main_config_model_feature_etc['list_features_id'])}\" )  \n","        print()\n","        \n","    mode_downsample_train = model_config.get('mode_downsample_train', mode_downsample_train_default)\n","    \n","    n_selfblend = model_config.get( 'n_selfblend' , 1)\n","    if verbose >= 100:\n","        print(); print('Starting model_config:', model_config, f'time from start: {(time.time() - t0modeling ):.1f}' )\n","    for i_selfblend in range( n_selfblend ): # train-predict same model several times and blend predictions - especially useful for NN, but do not fix random seed (!)\n","        i_model += 1 # Models count\n","        t0one_model_all_folds = time.time()\n","        for ix_fold  in  list_folds_ix[:n_folds_to_process]:\n","        \n","            model, str_model_id = get_model(model_config)\n","            str_model_id_pure_save =  str_model_id\n","            str_model_id = str( i_model) + ' ' + str_model_id\n","            if n_selfblend > 1:  str_model_id += ' ' + str( i_selfblend )\n","\n","            ##################### Prepare train data ###################################################\n","            mask_fold = folds == ix_fold\n","            IX_train = np.where(mask_fold ==  0)[0]; \n","            # IX_train = [ix for ix in IX_train if ix in  set_allowed_train_indexes]\n","            IX_train = get_downsampled_IX_train(IX_train, mode_downsample_train )\n","            X_train = X[IX_train,:]; Y_train = Y[IX_train,:]\n","\n","            if verbose >= 10:\n","                print(f'fold {ix_fold}, model: {str_model_id},  X_train.shape: {X_train.shape}, Y_train.shape: {Y_train.shape}, time: { (time.time() - t0modeling):12.1f} ')\n","                print('X_train Mbytes:', X_train.nbytes/1024/1024, 'Y_train Mbytes:', Y_train.nbytes/1024/1024,  )\n","            ##################### Call train model ###################################################\n","            t0 = time.time()\n","            model_fit(model , X_train, Y_train, model_config , str_model_id, verbose = 0 )   \n","            time_fit = time.time() - t0\n","            if verbose >= 1000:\n","                print(f'time_fit {time_fit:.1f}' ) \n","            del X_train, Y_train\n","            torch.cuda.empty_cache()\n","            gc.collect()\n","            log_available_ram(f'After Model fit. ix_fold {ix_fold}, i_selfblend, {i_selfblend}, {str_model_id}')\n","            \n","            ##################### Compute predictions for submission and blend with the previous one ###################################################\n","            if  mode_submit : \n","                t0 = time.time()\n","                Y_submit = (Y_submit * cnt_blend_submit  + model_predict(model , X_submit,  model_config , str_model_id , verbose = 0 ) )/ (cnt_blend_submit + 1);  # Average predictions from different folds/models\n","                cnt_blend_submit += 1 \n","                time_pred_submit = time.time() - t0\n","                torch.cuda.empty_cache()\n","                gc.collect()\n","\n","            #flag_compute_oof_predictions = True                 \n","            if  flag_compute_oof_predictions:\n","                t0 = time.time()\n","                IX_val = np.where(mask_fold > 0 )[0]; \n","                X_val = X[IX_val,:];#  Y_val = Y[IX_val,:]\n","                Y_pred_val = model_predict(model , X_val,  model_config , str_model_id , verbose = 0 )\n","                time_pred_val = time.time() - t0\n","                if verbose >= 10000:\n","                    print('Y_pred_val.shape', Y_pred_val.shape, f'time_pred_val {time_pred_val:.1f}')\n","                    \n","                if ix_fold == 0: cnt_blend_oof += 1 \n","                Y_pred_oof_blend[IX_val,:] = (Y_pred_oof_blend[IX_val,:] * cnt_blend_oof  + Y_pred_val )/ (cnt_blend_oof + 1); \n","            \n","                if  flag_compute_stat_for_each_model:\n","                    Y_pred_oof[IX_val,:] = (Y_pred_val ) \n","                    \n","                del X_val, Y_pred_val                    \n","                torch.cuda.empty_cache()\n","                gc.collect()\n","                \n","            del model \n","            torch.cuda.empty_cache()\n","            gc.collect()\n","            log_available_ram(f'At fold end. Model {str_model_id}, i_selfblend {i_selfblend}, fold {ix_fold} ')\n","\n","        time_one_model = np.round( time.time() - t0one_model_all_folds )\n","        if flag_compute_stat_for_each_model and flag_compute_oof_predictions: \n","            update_modeling_stat(df_stat, Y_pred_oof,  Y, flag_compute_cafa_f1 = flag_compute_cafa_f1_for_each_model , \n","                                 str_model_id = str_model_id, dict_optional_info = {'Time': time_one_model , 'i_selfblend':i_selfblend,\n","                                'ModelID Pure':str_model_id_pure_save, 'i_config':i_config}, verbose = 0)\n","            torch.cuda.empty_cache()\n","            gc.collect()\n","            log_available_ram(f'After OOF-Stat Calculation. Model {str_model_id}, i_selfblend {i_selfblend}' )\n","            \n","        if flag_compute_each_blend_stat and flag_compute_oof_predictions:\n","            update_modeling_stat(df_stat, Y_pred_oof_blend,  Y, flag_compute_cafa_f1 = flag_compute_cafa_f1_for_each_blend , \n","                                 str_model_id = str(cnt_blend_oof )+ 'Blend'+ ' ' +str_model_id, dict_optional_info = {'Time': time_one_model, \n","                                        'Blend': cnt_blend_oof , 'i_selfblend':i_selfblend}, verbose = 0)\n","            torch.cuda.empty_cache()\n","            gc.collect()\n","            log_available_ram(f'After Blend-Stat Calculation. Model {str_model_id}, i_selfblend {i_selfblend}' )\n","                \n","if flag_save_numpy_Y_pred_oof_blend and flag_compute_oof_predictions:\n","    t0 = time.time()\n","    fn = 'Y_pred_oof_blend.npy'\n","    np.save(fn,Y_pred_oof_blend)\n","    print(f'File {fn} saved. Y_pred_oof_blend.shape: {Y_pred_oof_blend.shape}. Time: {(time.time()-t0):.1f}')\n","    t0 = time.time()\n","    fn = 'Y_labels.npy'\n","    np.save(fn,Y_labels)\n","    print(f'File {fn} saved. Time: {(time.time()-t0):.1f}')\n","\n","if flag_save_numpy_Y_submit and mode_submit:\n","    t0 = time.time()\n","    fn = 'Y_submit.npy'\n","    np.save(fn,Y_submit)\n","    print(f'File {fn} saved. Y_submit.shape: {Y_submit.shape}. Time: {(time.time()-t0):.1f}')\n","    t0 = time.time()\n","    fn = 'Y_labels.npy'\n","    np.save(fn,Y_labels)\n","    print(f'File {fn} saved. Time: {(time.time()-t0):.1f}')\n","\n","\n","if flag_compute_final_model_stat and  flag_compute_oof_predictions:  \n","    update_modeling_stat(df_stat, Y_pred_oof_blend,  Y, flag_compute_cafa_f1 = True , str_model_id= 'Final1 Blend', dict_optional_info = { }, verbose = 0)\n","    gc.collect()\n","    log_available_ram('After Final Stat Calculation')\n","            \n","display(df_stat)   \n","\n","print('%.1f seconds passed total '%(time.time()-t0start) )\n","log_available_ram('After Modelling 1 Finished')"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-08-11T08:02:45.837799Z","iopub.status.busy":"2023-08-11T08:02:45.837105Z","iopub.status.idle":"2023-08-11T08:02:45.846045Z","shell.execute_reply":"2023-08-11T08:02:45.845078Z","shell.execute_reply.started":"2023-08-11T08:02:45.837752Z"},"trusted":true},"outputs":[],"source":["%%time\n","######################################################################################\n","###############  Modeling specific to Keras models\n","######################################################################################\n","# Keras models leak RAM in the code above - so here we try to create a specific  code \n","# for modelling with Keras \n","# The cell below is almost same as the one above, but we simplify things specifically to Keras models setup\n","# That is not intended technical modifaction - if we will find the ways to avoid memory leak in Keras models will return to uniform code as in the previous cell. \n","\n","######################### Params ##################################################3\n","\n","verbose = 1000\n","\n","######################### Output ##################################################\n","\n","########################## Preparations ###########################################\n","log_available_ram('Right before Keras modeling')\n","\n","\n","t0modeling = time.time()\n","list_folds_ix =  np.sort(list ( set(folds))  )\n","print(); print('Start training Keras models',datetime.datetime.now()) ; print()\n","########################## Main modelling  ###########################################\n","for main_config_model_feature_etc  in list_main_config_model_feature_etc:\n","    model_config = main_config_model_feature_etc['model']\n","    if 'Keras' not in model_config.keys(): continue  \n","    if  model_config['Keras'] == False : continue  \n","    if 'list_features_id' in main_config_model_feature_etc.keys():\n","        print()\n","        X,vec_train_protein_ids,X_submit,  submit_protein_ids = get_features(main_config_model_feature_etc['list_features_id'], verbose = 100)\n","        gc.collect()\n","        log_available_ram(f\"New features loaded:  {str(main_config_model_feature_etc['list_features_id'])}\" )  \n","        print()\n","    \n","    mode_downsample_train = model_config.get('mode_downsample_train', mode_downsample_train_default)\n","        \n","    n_selfblend = model_config.get( 'n_selfblend' , 1)\n","    if verbose >= 100:\n","        print(); print('Starting model_config:', model_config, f'time from start: {(time.time() - t0modeling ):.1f}' )\n","    for i_selfblend in range( n_selfblend ): # train-predict same model several times and blend predictions - especially useful for NN, but do not fix random seed (!)\n","        i_model += 1 # Models count\n","        t0one_model_all_folds = time.time()\n","        for ix_fold  in  list_folds_ix[:n_folds_to_process]:\n","            if verbose >= 10: \n","                print('---------------------------------------------- Fold', ix_fold, '----------------------------------------------')\n","                \n","            model, str_model_id = get_model(model_config)\n","            str_model_id_pure_save = str_model_id\n","            log_available_ram(f'After Keras model init. ix_fold {ix_fold}, n_selfblend, {n_selfblend}, {str_model_id}')\n","            str_model_id = str( i_model) + ' ' + str_model_id\n","            if n_selfblend > 1:  str_model_id += ' ' + str( i_selfblend )\n","                \n","            ##################### Prepare train data ###################################################\n","            mask_fold = folds == ix_fold\n","            IX_train = np.where(mask_fold ==  0)[0]; \n","            # IX_train = [ix for ix in IX_train if ix in  set_allowed_train_indexes]\n","            IX_train = get_downsampled_IX_train(IX_train, mode_downsample_train )\n","\n","            #X_train = X[IX_train,:]; Y_train = Y[IX_train,:]\n","            log_available_ram(f'After setting X_train. ix_fold {ix_fold}, n_selfblend, {n_selfblend}, {str_model_id}')\n","\n","            if verbose >= 10:\n","                print(f'fold {ix_fold}, model: {str_model_id},  X_train.shape: {X[IX_train,:].shape}, Y_train.shape: {Y[IX_train,:].shape}, time: { (time.time() - t0modeling):12.1f} ')\n","\n","            ##################### Call train model ###################################################\n","            t0 = time.time()\n","            #model_fit(model , X_train, Y_train, model_config , str_model_id, verbose = 0 )   \n","            epochs = model_config.get( 'epochs' , 15)\n","            batch_size = model_config.get( 'batch_size' , 128 )\n","            model.fit( X[IX_train,:], Y[IX_train,:] ,epochs = epochs,   batch_size = batch_size, verbose = 0  )\n","            time_fit = time.time() - t0\n","            if verbose >= 1000:\n","                print(f'time_fit {time_fit:.1f}' ) \n","            #del X_train, Y_train\n","            torch.cuda.empty_cache()\n","            gc.collect()\n","            log_available_ram(f'After Model fit. ix_fold {ix_fold}, i_selfblend, {i_selfblend}, {str_model_id}')\n","            \n","            ##################### Compute predictions for submission and blend with the previous one ###################################################\n","            if  mode_submit : \n","                t0 = time.time()\n","                Y_submit = (Y_submit * cnt_blend_submit  + model_predict(model , X_submit,  model_config , str_model_id , verbose = 0 ) )/ (cnt_blend_submit + 1);  # Average predictions from different folds/models\n","                cnt_blend_submit += 1 \n","                time_pred_submit = time.time() - t0\n","                torch.cuda.empty_cache()\n","                gc.collect()\n","                log_available_ram(f'After Predict on submit. ix_fold {ix_fold}, i_selfblend, {i_selfblend}, {str_model_id}')\n","                \n","            #flag_compute_oof_predictions = True                 \n","            if  flag_compute_oof_predictions:\n","                t0 = time.time()\n","                IX_val = np.where(mask_fold > 0 )[0]; \n","                #X_val = X[IX_val,:];#  Y_val = Y[IX_val,:]\n","                Y_pred_val = model_predict(model , X[IX_val,:],  model_config , str_model_id , verbose = 0 )\n","                time_pred_val = time.time() - t0\n","                if verbose >= 10000:\n","                    print('Y_pred_val.shape', Y_pred_val.shape, f'time_pred_val {time_pred_val:.1f}')\n","                if ix_fold == 0: cnt_blend_oof += 1 \n","                Y_pred_oof_blend[IX_val,:] = (Y_pred_oof_blend[IX_val,:] * cnt_blend_oof  + Y_pred_val )/ (cnt_blend_oof + 1); \n","            \n","                if  flag_compute_stat_for_each_model:\n","                    Y_pred_oof[IX_val,:] = (Y_pred_val ) \n","                    \n","                del  Y_pred_val                    \n","                torch.cuda.empty_cache()\n","                gc.collect()\n","                log_available_ram(f'After Predict on OOF. ix_fold {ix_fold}, i_selfblend, {i_selfblend}, {str_model_id}')\n","            \n","            keras.backend.clear_session()\n","            del model\n","            gc.collect()\n","            keras.backend.clear_session()\n","            torch.cuda.empty_cache()\n","            gc.collect()\n","            log_available_ram(f'At fold end. Model {str_model_id}, i_selfblend {i_selfblend}, fold {ix_fold} ')\n","\n","        time_one_model = np.round( time.time() - t0one_model_all_folds )\n","        if flag_compute_stat_for_each_model and flag_compute_oof_predictions: \n","            update_modeling_stat(df_stat, Y_pred_oof,  Y, flag_compute_cafa_f1 = flag_compute_cafa_f1_for_each_model , \n","                 str_model_id = str_model_id, dict_optional_info = {'Time': time_one_model, \n","                'i_selfblend':i_selfblend, 'ModelID Pure':str_model_id_pure_save, 'i_config':i_config }, verbose = 0)\n","            torch.cuda.empty_cache()\n","            gc.collect()\n","            log_available_ram(f'After OOF-Stat Calculation. Model {str_model_id}, i_selfblend {i_selfblend}' )\n","            \n","        if flag_compute_each_blend_stat and flag_compute_oof_predictions:\n","            update_modeling_stat(df_stat, Y_pred_oof_blend,  Y, flag_compute_cafa_f1 = flag_compute_cafa_f1_for_each_blend , \n","                                 str_model_id = str(cnt_blend_oof )+ 'Blend'+ ' ' +str_model_id, dict_optional_info = {'Time': time_one_model, \n","                                'Blend': cnt_blend_oof, 'i_selfblend':i_selfblend}, verbose = 0)\n","            torch.cuda.empty_cache()\n","            gc.collect()\n","            log_available_ram(f'After Blend-Stat Calculation. Model {str_model_id}, i_selfblend {i_selfblend}' )\n","                \n","if flag_save_numpy_Y_pred_oof_blend and flag_compute_oof_predictions:\n","    t0 = time.time()\n","    fn = 'Y_pred_oof_blend.npy'\n","    np.save(fn,Y_pred_oof_blend)\n","    print(f'File {fn} saved. Y_pred_oof_blend.shape: {Y_pred_oof_blend.shape}. Time: {(time.time()-t0):.1f}')\n","    t0 = time.time()\n","    fn = 'Y_labels.npy'\n","    np.save(fn,Y_labels)\n","    print(f'File {fn} saved. Time: {(time.time()-t0):.1f}')\n","\n","if flag_save_numpy_Y_submit and mode_submit:\n","    t0 = time.time()\n","    fn = 'Y_submit.npy'\n","    np.save(fn,Y_submit)\n","    print(f'File {fn} saved. Y_submit.shape: {Y_submit.shape}. Time: {(time.time()-t0):.1f}')\n","    t0 = time.time()\n","    fn = 'Y_labels.npy'\n","    np.save(fn,Y_labels)\n","    print(f'File {fn} saved. Time: {(time.time()-t0):.1f}')\n","    log_available_ram(f'After Save Y_submit' )\n","\n","\n","if flag_compute_final_model_stat and  flag_compute_oof_predictions:  \n","    #time_one_model = np.round( time.time() - t0one_model_all_folds )\n","    update_modeling_stat(df_stat, Y_pred_oof_blend,  Y, flag_compute_cafa_f1 = True , str_model_id= 'FinalKeras Blend', dict_optional_info = { }, verbose = 0)\n","    gc.collect()\n","    log_available_ram('After Final Stat Calculation')\n","            \n","display(df_stat)            \n","\n","if flag_compute_stat_for_each_model: \n","    del Y_pred_oof\n","    \n","torch.cuda.empty_cache()    \n","gc.collect()\n","\n","log_available_ram('After Modelling Keras Finished')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-11T08:02:45.849154Z","iopub.status.busy":"2023-08-11T08:02:45.848674Z","iopub.status.idle":"2023-08-11T08:02:45.866344Z","shell.execute_reply":"2023-08-11T08:02:45.865305Z","shell.execute_reply.started":"2023-08-11T08:02:45.849119Z"},"trusted":true},"outputs":[],"source":["%%time\n","######################################################################################\n","###############  Display/Plot Modeling Results\n","######################################################################################\n","\n","if 'AUC' in df_stat.columns and 'F1w' in df_stat.columns:\n","    print(\"\\nPearson相关性:\")\n","    display(df_stat[['AUC', 'F1w']].corr())\n","\n","    print(\"\\nSpearman相关性:\")\n","    display(df_stat[['AUC', 'F1w']].corr(method='spearman'))\n","\n","print(df_stat.shape)\n","display(df_stat)\n","\n","# 按fold平均的图表\n","print('Fold averaged')\n","df_stat = df_stat.copy()\n","df_stat['RankTmp'] = range(len(df_stat))\n","d = df_stat.groupby('Model').mean().sort_values('RankTmp')  # 确保顺序与初始df_stat相同\n","\n","for col2 in ['F1w', 'AUC']:\n","    if col2 not in df_stat.columns:\n","        continue\n","    print(col2, 'top data:')\n","    display(d.sort_values(col2, ascending=False).head(10))\n","    \n","    plt.figure(figsize=(20, 5))\n","    plt.suptitle(col2 + ' Fold averaged', fontsize=20)\n","\n","    plt.subplot(1, 2, 1)\n","    m = ['Blend' not in t for t in d.index]\n","    plt.plot(d[col2][m], '*-')\n","    plt.title('Models (no blend)', fontsize=20)\n","    plt.grid()\n","    plt.xticks(rotation=90)\n","\n","    plt.subplot(1, 2, 2)\n","    m = ['Blend' in t for t in d.index]\n","    plt.plot(d[col2][m], '*-')\n","    plt.title('Blend', fontsize=20)\n","    plt.grid()\n","    plt.xticks(rotation=90)\n","    \n","    plt.show()\n","    \n","    d.sort_values(col2, ascending=False).to_csv(col2 + '_sorted_fold_averaged.csv')\n","\n","print('\\n输出前后排序数据')\n","for col2 in ['F1w', 'AUC']:\n","    if col2 not in df_stat.columns:\n","        continue\n","    print(col2, 'top 50 data:')\n","    display(d.sort_values(col2, ascending=False).head(50))\n","    print(col2, 'tail 50 data:')\n","    display(d.sort_values(col2, ascending=False).tail(50))\n","    print(\"\\n\\n\\n\\n\\n\")\n","\n","# 按模型类型分组并平均\n","print('按模型类型分组')\n","print('注意可能会将不同输入特征的模型分组 - 这可能是不希望的 - 如果是这样，请查看按i_config分组 - 下面')\n","print('如果所有模型都在相同的数据上 - 完全没有问题')\n","df_stat = df_stat.copy()\n","df_stat['RankTmp'] = range(len(df_stat))\n","\n","if 'ModelID Pure' in df_stat.columns:\n","    d = df_stat.groupby('ModelID Pure').mean().sort_values('RankTmp')  # 确保顺序与初始df_stat相同\n","    for col2 in ['F1w', 'AUC']:\n","        if col2 not in df_stat.columns:\n","            continue\n","        print(col2, 'top data:')\n","        display(d.sort_values(col2, ascending=False).head(10))\n","        \n","        plt.figure(figsize=(20, 5))\n","        plt.suptitle(col2 + ' Fold averaged', fontsize=20)\n","\n","        plt.subplot(1, 2, 1)\n","        m = ['Blend' not in t for t in d.index]\n","        plt.plot(d[col2][m], '*-')\n","        plt.title('Models (no blend)', fontsize=20)\n","        plt.grid()\n","        plt.xticks(rotation=90)\n","        plt.show()\n","\n","        d.sort_values(col2, ascending=False).to_csv(col2 + '_sorted_groupped_by_model_type.csv')\n","\n","    print('\\n输出前后排序数据')\n","    for col2 in ['F1w', 'AUC']:\n","        if col2 not in df_stat.columns:\n","            continue\n","        print(col2, 'top 50 data:')\n","        display(d.sort_values(col2, ascending=False).head(50))\n","        print(col2, 'tail 50 data:')\n","        display(d.sort_values(col2, ascending=False).tail(50))\n","        print(\"\\n\\n\\n\\n\\n\")\n","\n","# 按i_config分组（所有自我混合将被平均），保留不同特征\n","print('按i_config分组')\n","df_stat = df_stat.copy()\n","df_stat['RankTmp'] = range(len(df_stat))\n","\n","if 'i_config' in df_stat.columns:\n","    d = df_stat.groupby('i_config').mean().sort_values('RankTmp')  # 确保顺序与初始df_stat相同\n","    for col2 in ['F1w', 'AUC']:\n","        if col2 not in df_stat.columns:\n","            continue\n","        print(col2, 'top data:')\n","        display(d.sort_values(col2, ascending=False).head(10))\n","        \n","        plt.figure(figsize=(20, 5))\n","        plt.suptitle(col2 + ' Fold averaged', fontsize=20)\n","\n","        plt.subplot(1, 2, 1)\n","        m = np.ones(len(d)).astype(bool)\n","        plt.plot(d[col2][m], '*-')\n","        plt.title('Models (no blend)', fontsize=20)\n","        plt.grid()\n","        plt.xticks(rotation=90)\n","        plt.show()\n","\n","        d.sort_values(col2, ascending=False).to_csv(col2 + '_sorted_groupped_by_i_config.csv')\n","\n","    print('\\n输出前后排序数据')\n","    for col2 in ['F1w', 'AUC']:\n","        if col2 not in df_stat.columns:\n","            continue\n","        print(col2, 'top 50 data:')\n","        display(d.sort_values(col2, ascending=False).head(50))\n","        print(col2, 'tail 50 data:')\n","        display(d.sort_values(col2, ascending=False).tail(50))\n","        print(\"\\n\\n\\n\\n\\n\")\n","\n","# 不按fold平均的图表\n","print('不按fold平均的图表')\n","col = 'Model'\n","for col2 in ['F1w', 'AUC']:\n","    if col not in df_stat.columns or col2 not in df_stat.columns:\n","        continue\n","    print(col2, 'top data:')\n","    display(df_stat.sort_values(col2, ascending=False).head(5))\n","    \n","    plt.figure(figsize=(20, 5))\n","    plt.suptitle(col2, fontsize=20)\n","    \n","    plt.subplot(1, 2, 1)\n","    m = ['Blend' not in t for t in df_stat[col]]\n","    plt.plot(df_stat[col2][m], '*-')\n","    plt.title('Models (no blend)', fontsize=20)\n","    plt.grid()\n","    plt.xticks(rotation=90)\n","\n","    plt.subplot(1, 2, 2)\n","    m = ['Blend' in t for t in df_stat[col]]\n","    plt.plot(df_stat[col2][m], '*-')\n","    plt.title('Blend', fontsize=20)\n","    plt.grid()\n","    plt.xticks(rotation=90)\n","    \n","    plt.show()\n","\n","if 'AUC' in df_stat.columns:\n","    plt.plot(df_stat['AUC'].values, '*-')\n","    plt.show()\n","\n","# 按fold平均的图表\n","col = 'AUC'\n","col2 = 'Fold'\n","if col in df_stat.columns and col2 in df_stat.columns:\n","    d2 = df_stat.groupby(col2).mean()\n","    display(d2)\n","    d2.to_csv('df_stat_folds_mean.csv')\n","    plt.plot(d2[col].values, '*-')\n","    plt.xlabel(col2, fontsize=20)\n","    plt.title(col, fontsize=20)\n","    plt.show()\n","\n","col2 = 'Model'\n","if col in df_stat.columns and col2 in df_stat.columns:\n","    d3 = df_stat.groupby(col2).mean()\n","    display(d3)\n","    d3.to_csv('df_stat_models_mean.csv')\n","    plt.plot(d3[col].values, '*-')\n","    plt.title(col, fontsize=20)\n","    plt.xlabel(col2, fontsize=20)\n","    plt.show()\n","    display()\n","\n","m = df_stat['Model'] == 'Final'\n","if m.sum() == 0:\n","    m = (df_stat['Model'] == 'FinalKeras Blend')\n","if m.sum() == 0:\n","    m = (df_stat['Model'] == 'Final1 Blend')\n","\n","display(df_stat[m])\n","display(df_stat[m].mean())\n","if 'F1w' in df_stat.columns:\n","    print(df_stat[m].mean().loc['F1w'])\n","df_stat[m].mean().to_csv('final_means.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-11T08:01:52.995749Z","iopub.status.busy":"2023-08-11T08:01:52.995363Z","iopub.status.idle":"2023-08-11T08:02:00.173533Z","shell.execute_reply":"2023-08-11T08:02:00.172249Z","shell.execute_reply.started":"2023-08-11T08:01:52.995718Z"},"trusted":true},"outputs":[],"source":["%%time\n","######################################################################################\n","###############  Prepare submission tsv file\n","######################################################################################\n","\n","# 清理内存\n","if 0:\n","    del X_train, Y_train, X_val, Y_val, train_dataset, train_dataloader, X, Y\n","gc.collect()\n","torch.cuda.empty_cache()\n","log_available_ram()\n","mode_submit_prepare = 'slow_less_RAM_consuming'\n","t0 = time.time()\n","print(mode_submit_prepare, mode_submit)\n","\n","if mode_submit and flag_save_final_submit_file:\n","    print(Y_submit.shape)\n","    \n","    if mode_submit_prepare == 'slow_less_RAM_consuming':\n","        file_path = \"submission.tsv\"\n","        cc = 0\n","        cc2 = 0\n","        with open(file_path, 'w') as file:\n","            for i in range(Y_submit.shape[0]):\n","                for j in range(Y_submit.shape[1]):\n","                    val = Y_submit[i, j]\n","                    if val >= cutoff_threshold_low:\n","                        str_go_term = str(Y_labels[j])\n","                        str_protein_id = str(submit_protein_ids[i])\n","                        str_save = f'{str_protein_id}\\t{str_go_term}\\t{val:.3f}\\n'\n","                        file.write(str_save)\n","                        cc2 += 1\n","                        if cc2 <= 10:\n","                            if cc2 == 1:\n","                                print('First 10 examples of the saved data:')\n","                            print(str_save)\n","                    cc += 1\n","                    if cc % 30_000_000 == 0:\n","                        sz = Y_submit.shape[0] * Y_submit.shape[1]\n","                        print(cc, 'out of', sz, f'percent {cc/sz*100:.2f}', 'saved:', cc2, f'time {time.time() - t0:.1f}')\n","        print(cc2, 'results saved to submission file', f'time {time.time() - t0:.1f}')\n","\n","    else:\n","        df_submission = pd.DataFrame(columns=['Protein Id', 'GO Term Id', 'Prediction'])\n","\n","        n_targets_predicted = Y_submit.shape[1]\n","        n_samples_predicted = Y_submit.shape[0]\n","        print('n_samples_predicted, n_targets_predicted', n_samples_predicted, n_targets_predicted)\n","\n","        # 创建提交数据框\n","        protein_list = [k for k in submit_protein_ids for _ in range(n_targets_predicted)]\n","        df_submission['Protein Id'] = protein_list\n","        df_submission['GO Term Id'] = list(Y_labels) * n_samples_predicted\n","        df_submission['Prediction'] = Y_submit.ravel()\n","\n","        df_submission = df_submission.round(3)\n","        df_submission = df_submission[df_submission['Prediction'] >= cutoff_threshold_low]\n","\n","        memory_usage_per_column = df_submission.memory_usage(deep=True)\n","        total_memory_usage = memory_usage_per_column.sum()\n","        print(\"\\nTotal memory usage:\", total_memory_usage / 1e6, \"Megabytes\")\n","\n","        print(df_submission.shape)\n","        display(df_submission)\n","\n","        if 0:\n","            del preds\n","\n","        gc.collect()\n","\n","        df_submission.to_csv(\"submission.tsv\", header=False, index=False, sep='\\t')\n","\n","    log_available_ram('After saving submission')\n","\n","    \n","## Plot histograms on submission predictions\n","try:\n","    # 打印数据框的形状\n","    print(df_submission.shape)\n","\n","    # 绘制预测值的直方图\n","    plt.figure(figsize=(15, 4))\n","    plt.hist(df_submission['Prediction'].values, bins=1000)\n","    plt.show()\n","\n","    # 再次打印数据框的形状并显示描述统计信息\n","    print(df_submission.shape)\n","    display(df_submission.describe())\n","\n","    # 打印不同阈值下的预测数量和占比\n","    thresholds = [0.1, 0.2, 0.25, 0.28, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n","    for t in thresholds:\n","        m = df_submission['Prediction'] > t\n","        print(f\"Threshold: {t}, Count: {m.sum()}, Proportion: {m.sum() / (n_samples_predicted * n_targets_predicted):.4f}\")\n","\n","    print()\n","\n","    # 尝试打印训练数据中的1的数量和占比\n","    try:\n","        print(f\"Y Sum: {Y.sum()}, Proportion: {Y.sum() / (Y.shape[0] * Y.shape[1]):.4f}\")\n","    except:\n","        pass\n","\n","    print('Here is a fast rationale why we should think of threshold for F1 is around 0.28 - number of 1 in that case corresponds to train data')\n","except Exception as e:\n","    print(f\"An error occurred: {e}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print('%.1f seconds passed total '%(time.time()-t0start) )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":5521661,"sourceId":41875,"sourceType":"competition"},{"datasetId":2673536,"sourceId":4584547,"sourceType":"datasetVersion"},{"datasetId":3167603,"sourceId":5499219,"sourceType":"datasetVersion"},{"datasetId":3225525,"sourceId":5607816,"sourceType":"datasetVersion"},{"datasetId":3207078,"sourceId":5806679,"sourceType":"datasetVersion"},{"datasetId":3207084,"sourceId":5806682,"sourceType":"datasetVersion"},{"datasetId":3207096,"sourceId":5806692,"sourceType":"datasetVersion"},{"datasetId":3211581,"sourceId":5807641,"sourceType":"datasetVersion"},{"datasetId":3207113,"sourceId":5807722,"sourceType":"datasetVersion"},{"datasetId":3189532,"sourceId":6278150,"sourceType":"datasetVersion"},{"datasetId":3614490,"sourceId":6285621,"sourceType":"datasetVersion"}],"dockerImageVersionId":30476,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
