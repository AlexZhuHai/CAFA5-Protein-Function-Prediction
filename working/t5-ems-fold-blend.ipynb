{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T06:13:59.636971Z","iopub.status.busy":"2024-05-23T06:13:59.636521Z","iopub.status.idle":"2024-05-23T06:13:59.667723Z","shell.execute_reply":"2024-05-23T06:13:59.666358Z","shell.execute_reply.started":"2024-05-23T06:13:59.636938Z"},"trusted":true},"outputs":[],"source":["n_samples_to_consider =   142246 \n","n_labels_to_consider = 8000 # Up to 31466 but more than 10000 may OUT OF MEMORY \n","n_folds_to_process = 15 \n","######################################### Set the models, their params, etc  ########################################################\n","list_main_config_model_feature_etc = [] \n","cfg2 = {'model': {'id':'pMLP_Andrey' , 'n_selfblend':12, 'epochs':80, 'custom_model_fit_function_name':'model_fit_pytorch_Sophia_Andrey1' } } \n","\n","list_main_config_model_feature_etc.append( cfg2 )\n","\n","#########################################  Feature sets  ########################################################\n","list_features_id =  ['t5', 'esm2S1280' ] #  ['protbert']#  ] \n","\n","#########################################  Compute/not/when the scores  ########################################################\n","flag_compute_oof_predictions = True\n","\n","# Compute statistics on OOF for each model \n","flag_compute_stat_for_each_model = ( False ) and (flag_compute_oof_predictions)\n","flag_compute_cafa_f1_for_each_model = ( True ) and ( flag_compute_stat_for_each_model )  and ( flag_compute_oof_predictions )\n","\n","# Compute statistics on blend of OOF each blend  \n","flag_compute_each_blend_stat = ( False) and (flag_compute_oof_predictions)\n","flag_compute_cafa_f1_for_each_blend = ( True ) and (flag_compute_each_blend_stat) and (flag_compute_oof_predictions)\n","\n","# Compute stat on final blend   \n","flag_compute_final_model_stat = ( True ) and (flag_compute_oof_predictions)\n","\n","#########################################  Furthter params   ########################################################\n","\n","cutoff_threshold_low = 0.1 # prediction < cutoff_threshold_low will be set to zero (no need to save to submission file)\n","\n","mode_submit = True # True # Compute prediction for submission part and prepare submission file in required CAFA5 format. Set to False if you only interested in local score\n","\n","###  Save/not/what predictions   #####\n","flag_save_final_submit_file = (True ) and ( mode_submit ) # Prepare and save final submission txt file. \n","flag_save_numpy_Y_pred_oof_blend = (True)  and (flag_compute_oof_predictions)  # Save  Y_pred_oof_blend in numpy format \"npy\" for possible further blend outside current notebook \n","flag_save_numpy_Y_submit = (True) and (mode_submit) # Save Y_submit matrix in numpy  format \"npy\" for possible further blend outside current notebook \n","\n","mode_downsample_train_default = '43k'\n","RANDOM_SEED = None \n","logs_file_path = 'logs.txt'"]},{"cell_type":"markdown","metadata":{},"source":["# Preparations "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-23T06:13:59.691714Z","iopub.status.busy":"2024-05-23T06:13:59.691048Z","iopub.status.idle":"2024-05-23T06:13:59.705397Z","shell.execute_reply":"2024-05-23T06:13:59.704299Z","shell.execute_reply.started":"2024-05-23T06:13:59.691668Z"},"trusted":true},"outputs":[],"source":["%%capture \n","%%time\n","import time\n","t0start = time.time()\n","import numpy as np \n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import os\n","import psutil\n","import datetime\n","\n","def get_available_ram():\n","    virtual_memory = psutil.virtual_memory()\n","    available_ram = virtual_memory.available\n","    return available_ram\n","\n","\n","def log_available_ram( str_for_logging_optional = None):\n","    try:\n","        virtual_memory = psutil.virtual_memory()\n","        available_ram_bytes = virtual_memory.available\n","        available_ram_megabytes = available_ram_bytes / (1024 ** 2)\n","        available_ram_gigabytes = available_ram_bytes / (1024 ** 3)\n","\n","        if str_for_logging_optional is not None:\n","            print(str_for_logging_optional)\n","        current_datetime = datetime.datetime.now()\n","        str1 = f\"Available RAM: {available_ram_gigabytes:.2f} G  Current datetime: {current_datetime}\"\n","        print( str1 ) \n","        \n","        with open(logs_file_path, 'a') as file:\n","            if str_for_logging_optional is not None:\n","                file.write(str_for_logging_optional + '\\n')\n","            file.write(str1 + '\\n')\n","        # print(\"Data appended successfully.\")\n","    except Exception as e:\n","        print(f\"Error while appending data: {e}\")        \n","\n","        \n","log_available_ram('On start')\n","\n","\n","!pip install torchmetrics\n","from torchmetrics import AUROC as torch_AUCROC\n","from torchmetrics import F1Score as torch_F1Score\n","\n","!pip install torchsummary\n","from torchsummary import summary as torchsummary\n","import os\n","import gc\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from tqdm.notebook import tqdm\n","tqdm.pandas()\n","import torch\n","import warnings\n","warnings.filterwarnings('ignore')\n","from sklearn.model_selection import train_test_split\n","import torch.nn.functional as F\n","import torch.nn as nn\n","from torch.utils.data import Dataset, TensorDataset , DataLoader\n","\n","\n","import random \n","def seed_all(RANDOM_SEED):\n","    if RANDOM_SEED is not None: \n","        try:\n","            SEED = RANDOM_SEED\n","            random.seed(SEED)\n","            np.random.seed(SEED)\n","            torch.manual_seed(SEED)\n","            torch.cuda.manual_seed_all(SEED)\n","            torch.backends.cudnn.deterministic = True\n","            torch.backends.cudnn.benchmark = False\n","\n","        except Exception as e:\n","            print(f\"Exception: {e}\")\n","            \n","seed_all(RANDOM_SEED)      \n","\n","\n","# Optimizer \"Sophia\" sometimes better than Adam\n","!git clone https://github.com/kyegomez/Sophia.git\n","! python Sophia/setup.py install\n","!rm Sophia/Sophia/__init__.py\n","from Sophia.Sophia.Sophia import SophiaG\n","\n","# Choose device - GPU or CPU and assign model to it\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Load data"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%time \n","\n","# 根据特征ID获取特征路径的函数\n","def get_paths_to_features(features_id):\n","    # 基础目录映射\n","    base_dir_map = {\n","        'esm2S2560': '../input/4637427/',\n","        't5': '../input/cafa5-features-etc/',\n","        'protbert': '../input/protbert-embeddings-for-cafa5/',\n","        'esm2S1280': '../input/23468234/',\n","        'esm2S320': '../input/315701375/',\n","        'esm2S640': '../input/8947923/',\n","        'esm2S480': '../input/3023750/'\n","    }\n","\n","    # 文件名映射\n","    file_name_map = {\n","        'esm2S2560': ('train_embeds_esm2_t36_3B_UR50D.npy', 'train_ids_esm2_t36_3B_UR50D.npy', 'test_embeds_esm2_t36_3B_UR50D.npy', 'test_ids_esm2_t36_3B_UR50D.npy'),\n","        't5': ('T5_train_embeds_float32.npy', 'train_ids.npy', 'T5_test_embeds_float32.npy', 'test_ids.npy'),\n","        'protbert': ('train_embeddings.npy', 'train_ids.npy', 'test_embeddings.npy', 'test_ids.npy'),\n","        'esm2S1280': ('train_embeds_esm2_t33_650M_UR50D.npy', 'train_ids_esm2_t33_650M_UR50D.npy', 'test_embeds_esm2_t33_650M_UR50D.npy', 'test_ids_esm2_t33_650M_UR50D.npy'),\n","        'esm2S320': ('train_embeds_esm2_t6_8M_UR50D.npy', 'train_ids_esm2_t6_8M_UR50D.npy', 'test_embeds_esm2_t6_8M_UR50D.npy', 'test_ids_esm2_t6_8M_UR50D.npy'),\n","        'esm2S640': ('train_embeds_esm2_t30_150M_UR50D.npy', 'train_ids_esm2_t30_150M_UR50D.npy', 'test_embeds_esm2_t30_150M_UR50D.npy', 'test_ids_esm2_t30_150M_UR50D.npy'),\n","        'esm2S480': ('train_embeds_esm2_t12_35M_UR50D.npy', 'train_ids_esm2_t12_35M_UR50D.npy', 'test_embeds_esm2_t12_35M_UR50D.npy', 'test_ids_esm2_t12_35M_UR50D.npy')\n","    }\n","\n","    dn = base_dir_map.get(features_id)\n","    fn_X, fn_protein_ids, fn_X_submit, fn_submit_protein_ids = file_name_map.get(features_id)\n","    \n","    return os.path.join(dn, fn_X), os.path.join(dn, fn_protein_ids), os.path.join(dn, fn_X_submit), os.path.join(dn, fn_submit_protein_ids)\n","\n","################ load  features #########################\n","\n","print(); print('!!! Pay attention proteins ids should be the same as in all the files !!!!!!!!!!!!!!!!!! '); print();\n","\n","def get_features(list_features_id, verbose = 0):\n","    \n","    if verbose >= 100: print(list_features_id ); \n","        \n","    X_submit,  submit_protein_ids = None, None\n","    for i0,features_id in enumerate(list_features_id):\n","        fn_X, fn_protein_ids, fn_X_submit, fn_submit_protein_ids  = get_paths_to_features(features_id)\n","        fn = fn_X \n","        if verbose >= 100:  print(fn)\n","        if i0 == 0:\n","            X = np.load(fn).astype(np.float32)[:n_samples_to_consider, :]\n","        else:\n","            X = np.concatenate( [X, np.load(fn).astype(np.float32)[:n_samples_to_consider, :] ] , axis = 1 )\n","        if verbose >= 100: print(X.shape)\n","        if verbose >= 100: print(X[:2,:3])\n","        protein_ids  = np.load(fn_protein_ids)[:n_samples_to_consider ]\n","        vec_train_protein_ids = protein_ids\n","        if verbose >= 100: print('protein_ids.shape:', protein_ids.shape)\n","        if verbose >= 100: print('protein_ids[:15]:', protein_ids[:15])\n","\n","            \n","        ################ load  features for submit #########################\n","        if mode_submit:\n","\n","            fn = fn_X_submit\n","            if verbose >= 100: print(fn)\n","            if i0 == 0:\n","                X_submit = np.load(fn).astype(np.float32)\n","            else:\n","                X_submit = np.concatenate( [X_submit, np.load(fn).astype(np.float32) ] , axis = 1 )\n","            if verbose >= 100: print(X_submit.shape)\n","            if verbose >= 100: print(X_submit[:2,:3])\n","\n","            fn = fn_submit_protein_ids\n","            submit_protein_ids = np.load(fn)\n","            if verbose >= 100: print(submit_protein_ids.shape, submit_protein_ids[:10])\n","    return X,vec_train_protein_ids,X_submit,  submit_protein_ids \n","\n","X,vec_train_protein_ids,X_submit,  submit_protein_ids = get_features(list_features_id, verbose = 100)\n","print('X_submit ==== ',X_submit)\n","\n","############################ load targets and their ids  ######################################\n","\n","# Load targets Y\n","from scipy import sparse\n","fn = '/kaggle/input/cafa5-features-etc/Y_31466_sparse_float32.npz'\n","Y = sparse.load_npz(fn )\n","print('Y', Y.shape, 'loaded')\n","Y = Y[:n_samples_to_consider,:n_labels_to_consider].toarray()\n","print('Y', Y.shape, 'truncated')\n","n_labels_to_consider = Y.shape[1] # in case n_labels_to_consider is greater that Y.shape we decrease it\n","\n","fn = '/kaggle/input/cafa5-features-etc/Y_31466_labels.npy'\n","Y_labels = np.load(fn, allow_pickle=True )[:n_labels_to_consider]\n","labels_to_consider = Y_labels\n","print(Y_labels.shape)\n","print(Y_labels[:20])\n","\n","# %%time\n","if 1:\n","    v = Y.sum(axis = 0)\n","    plt.figure(figsize = (20,6))\n","    plt.plot(v[:500], '*-')\n","    plt.grid()\n","    plt.title(' Number of 1 in targets',fontsize = 20 )\n","    plt.xlabel('target index', fontsize = 20 )\n","    plt.show()\n","\n","\n","import gc\n","gc.collect()\n","\n","print('X mbytes:', X.nbytes/1024/1024)\n","print('Y mbytes:', Y.nbytes/1024/1024)\n","try :\n","    print('X_submit mbytes:', X_submit.nbytes/1024/1024)\n","except:\n","    pass\n","log_available_ram('After data load')\n","\n","## Prepare possible downsampling of the train \n","dict_set_allowed_train_indexes = {}\n","\n","fn = '/kaggle/input/cafa5-features-etc/train_ids_cut43k.npy'\n","allowed_train_ids = np.load(fn)\n","print(allowed_train_ids.shape, allowed_train_ids[:10])\n","vec_allowed_train_indexes_43k =  [ ix for ix in range(len(vec_train_protein_ids)) if  vec_train_protein_ids[ix] in ( allowed_train_ids ) ] \n","set_allowed_train_indexes_43k = set( vec_allowed_train_indexes_43k  )\n","dict_set_allowed_train_indexes['43k'] = set_allowed_train_indexes_43k\n","print(len(dict_set_allowed_train_indexes['43k']), list(dict_set_allowed_train_indexes['43k'])[:10] )\n","\n","def get_downsampled_IX_train(IX_train, mode_downsample_train ):\n","    if mode_downsample_train in dict_set_allowed_train_indexes.keys():\n","        set_allowed_train_indexes = dict_set_allowed_train_indexes[ mode_downsample_train ]\n","        IX_train = [t for t in IX_train if t in set_allowed_train_indexes ]\n","    elif 'random_subsample_percent' in str(mode_downsample_train):\n","        random_subsample_percent = float( str(mode_downsample_train).split('_')[-1] ) # 'random_subsample_percent_90'\n","        IX_train = np.random.permutation(IX_train)[ :int(len(IX_train)*random_subsample_percent / 100  )]\n","    \n","    return IX_train\n","\n","# Load partition to folds for cross-validation\n","fn = '/kaggle/input/cafa5-features-etc/random_folds/folds_gkf.npy'\n","folds = np.load(fn)[:n_samples_to_consider]\n","print(folds.shape, len(set(folds)))\n","for k in set(folds):\n","    m = folds == k\n","    print(k, m.sum() )\n","folds"]},{"cell_type":"markdown","metadata":{},"source":["# Define neural networks"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class Model5(nn.Module):\n","    def __init__(self,input_features,output_features):\n","        super().__init__()\n","        \n","        self.activation = nn.PReLU()\n","        \n","        self.bn1 = nn.BatchNorm1d(input_features)\n","        self.fc1 = nn.Linear(input_features, 800)\n","        self.ln1 = nn.LayerNorm(800, elementwise_affine=True)\n","        \n","        self.bn2 = nn.BatchNorm1d(800)\n","        self.fc2 = nn.Linear(800, 600)\n","        self.ln2 = nn.LayerNorm(600, elementwise_affine=True)\n","        \n","        self.bn3 = nn.BatchNorm1d(600)\n","        self.fc3 = nn.Linear(600, 400)\n","        self.ln3 = nn.LayerNorm(400, elementwise_affine=True)\n","        \n","        self.bn4 = nn.BatchNorm1d(1200)\n","        self.fc4 = nn.Linear(1200, output_features)\n","        self.ln4 = nn.LayerNorm(output_features, elementwise_affine=True)\n","        \n","        self.sigm = nn.Sigmoid()\n","    def forward(self,inputs):\n","\n","        fc1_out = self.bn1(inputs)\n","        fc1_out = self.ln1(self.fc1(inputs))\n","        fc1_out = self.activation(fc1_out)\n","        \n","        x = self.bn2(fc1_out)\n","        \n","        x = self.ln2(self.fc2(x))\n","        x = self.activation(x)\n","        \n","        x = self.bn3(x)\n","        \n","        x = self.ln3(self.fc3(x))\n","        x = self.activation(x)\n","        \n","        x = torch.cat([x, fc1_out], axis = -1)\n","        \n","        x = self.bn4(x)\n","        \n","        x = self.ln4(self.fc4(x))\n","        out = self.sigm(x)\n","        return out\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%time\n","import gc\n","import time \n","import numpy as np\n","try: \n","    from sklearn.metrics import roc_auc_score\n","    from sklearn.metrics import f1_score\n","    from sklearn.linear_model import Ridge\n","    from sklearn.neural_network import MLPRegressor\n","    import lightgbm as lgbm\n","    from sklearn.multioutput import MultiOutputRegressor\n","    from sklearn.multioutput import MultiOutputClassifier  \n","    \n","    from catboost import CatBoostRegressor\n","    from catboost import CatBoostClassifier    \n","except Exception as e:\n","    print(f'Exception importing models {e} ')\n","    pass\n","\n","\n","import keras\n","from keras.models import Sequential, Model\n","from keras.layers import Dense, Input,  Concatenate, Dropout, BatchNormalization, Activation\n","from keras.layers import LayerNormalization"]},{"cell_type":"markdown","metadata":{},"source":["# Get model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def get_model(model_config):\n","    \n","    str_model_id = str(model_config['id'])\n","    if model_config['id'] == 'pMLP_Andrey':\n","        print('get_model pMLP_Andrey')\n","        model = Model5(X.shape[1],Y.shape[1])\n","        model.to(device)\n","    elif model_config['id'] == 'Ridge':\n","        print('get_model Ridge')\n","        alpha = model_config.get('alpha',10)\n","        str_model_id = 'Ridge'+str(alpha)\n","        model = Ridge(alpha=alpha)\n","        \n","    elif model_config['id'] == 'skMLP':\n","        print('get_model skMLP')\n","        str_model_id = 'skMLP'\n","        max_iter = model_config.get('max_iter',500); str_model_id += '_MI'+str(max_iter) \n","        random_state = model_config.get('random_state',np.random.randint(0,100)); str_model_id += '_RS'+str(random_state)\n","        hidden_layer_sizes = model_config.get('Layers', (100,) ); str_model_id += '_HL'+str(hidden_layer_sizes)\n","        alpha = model_config.get('alpha', 1e-4 ); str_model_id += '_alpha'+str(alpha)\n","        learning_rate_init = model_config.get('LR', 0.001 ); str_model_id += '_LR'+str(learning_rate_init)\n","        model = MLPRegressor( max_iter=max_iter , random_state=random_state, hidden_layer_sizes = hidden_layer_sizes, alpha= alpha, learning_rate_init = learning_rate_init  )\n","        \n","    elif model_config['id'] == 'KMLP_simple':\n","        print('get_model KMLP_simple')\n","        model = Sequential(); str_model_id = 'KMLP_simple'\n","        \n","        nfeats = X.shape[1]#  model_config['input_dim']\n","        nlabels = Y.shape[1]#  model_config['output_dim']\n","        \n","        layer_dim = 500\n","        model.add(Dense(layer_dim, activation='relu', input_dim=nfeats   ) )  ; str_model_id += '_L1_'+str(layer_dim) \n","        model.add(BatchNormalization()) ; str_model_id += '_BN'\n","        droupout_rate = 0.1\n","        model.add(Dropout(droupout_rate)) ; str_model_id += '_DR'+str( np.round(droupout_rate ,2) ) \n","        \n","        layer_dim = 800\n","        model.add(Dense(layer_dim, activation='relu'   ) )  ; str_model_id += '_L2_'+str(layer_dim) \n","        model.add(BatchNormalization()) ; str_model_id += '_BN'\n","        droupout_rate = 0.1\n","        model.add(Dropout(droupout_rate)) ; str_model_id += '_DR'+str( np.round(droupout_rate ,2) ) \n","        \n","        ######### Last layer ########################################################################################\n","        model.add(Dense(nlabels, activation='sigmoid'   ))\n","        \n","        model.compile(loss='binary_crossentropy', optimizer='adam',  metrics=[keras.metrics.AUC()] )\n","        \n","        \n","    elif model_config['id'] == 'KMLP':   \n","        print('get_model KMLP')\n","        model = Sequential(); str_model_id = 'KMLP'\n","        \n","        layers_sizes = model_config['Layers']\n","        list_droupouts = model_config.get('Dropouts' ,  []  )\n","        list_batchnormalization = model_config.get( 'BatchNormalizations', [] )\n","        \n","        nfeats = X.shape[1]#  model_config['input_dim']\n","        nlabels = Y.shape[1]#  model_config['output_dim']\n","        \n","        ######### First Layer #################################################################################\n","        i_layer = 0 \n","        layer_dim = layers_sizes[i_layer]\n","        model.add(Dense(layer_dim, activation='relu', input_dim=nfeats   ) )  ; str_model_id += '_L1_'+str(layer_dim) \n","        if len( list_batchnormalization ) > i_layer:\n","            if list_batchnormalization[i_layer]:\n","                model.add(BatchNormalization())\n","                str_model_id += '_BN'\n","        if (len( list_droupouts ) > i_layer) and ( list_droupouts[i_layer] is not None ):\n","            droupout_rate  = list_droupouts[i_layer]\n","            model.add(Dropout(droupout_rate))\n","            str_model_id += '_DR'+str( np.round(droupout_rate ,2) ) \n","            \n","        ######### Middle layers ##########################################################################################\n","        for i_layer in range(1, len( layers_sizes ) ) : \n","            layer_dim = layers_sizes[i_layer]\n","            if layer_dim is None: break  \n","            model.add(Dense(layer_dim, activation='relu' ));\n","            str_model_id += '_L'+str(i_layer+1)+'_'+str(layer_dim)    \n","            if len( list_batchnormalization ) > i_layer:\n","                if list_batchnormalization[i_layer]:\n","                    model.add(BatchNormalization())\n","                    str_model_id += '_BN'\n","            if (len( list_droupouts ) > i_layer) and ( list_droupouts[i_layer] is not None ):\n","                droupout_rate  = list_droupouts[i_layer]\n","                model.add(Dropout(droupout_rate))\n","                str_model_id += '_DR'+str( np.round(droupout_rate ,2) )         \n","        \n","        ######### Last layer ########################################################################################\n","        model.add(Dense(nlabels, activation='sigmoid'   ))\n","        model.compile(loss='binary_crossentropy',\n","                        optimizer='adam',\n","                        metrics=[keras.metrics.AUC()])           \n","        \n","    elif model_config['id'] == 'gpuLogReg':\n","        print('get_model gpuLogReg')\n","        import torch\n","        if torch.cuda.is_available():\n","            from cuml.linear_model import LogisticRegression as CULogisticRegression        \n","            model = MultiOutputClassifier(estimator= CULogisticRegression( ) )# **params ) )\n","            str_model_id = 'gpuLogReg'\n","            \n","    elif model_config['id'] == 'gpuCatBClasdefault':\n","        print('get_model gpuCatBClasdefault')\n","        model = MultiOutputClassifier(estimator= CatBoostClassifier(task_type = 'GPU', verbose = 0 ))# **params ) )\n","        str_model_id = model_cfg[0] \n","\n","    elif model_config['id'] == 'gpuCatBdefault': \n","        print('get_model gpuCatBdefault')\n","        model = MultiOutputRegressor(estimator= CatBoostRegressor(task_type = 'GPU', verbose = 0 ))# **params ) )\n","        str_model_id = model_cfg[0] \n","\n","    elif model_config['id'] == 'CatBdefault': \n","        print('get_model CatBdefault')\n","        model = MultiOutputRegressor(estimator= CatBoostRegressor(verbose = 0 ))# **params ) )\n","        str_model_id = model_cfg[0] \n","    \n","    elif model_config['id'] == 'LGBdefault': \n","        print('get_model LGBdefault')\n","        model = MultiOutputRegressor(estimator=lgbm.LGBMRegressor())# **params ) )\n","        str_model_id = 'LGBdefault'\n","            \n","    namepostfix = model_config.get('namepostfix',\"\")\n","    str_model_id += namepostfix\n","    \n","    return model, str_model_id\n","\n","# model_config_tmp = {'id':'Ridge'}\n","# model_config_tmp = {'id':'Ridge', 'alpha':10,'namepostfix':'_test' }\n","\n","# model, str_model_id = get_model(model_config_tmp)\n","# print(model, str_model_id )"]},{"cell_type":"markdown","metadata":{},"source":["# Fit model function "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%time\n","def model_fit(model , X_train, Y_train, model_config , str_model_id = '', verbose = 1000 ):\n","    print('model_fit')\n","    if verbose >= 100:\n","        print('model_fit', str_model_id,  model_config )\n","\n","    if 'custom_model_fit_function_name' in model_config.keys():\n","        str_func_name = model_config['custom_model_fit_function_name']\n","        if verbose >= 1000: print('str_func_name', str_func_name)\n","        globals()[str_func_name](model , X_train, Y_train, model_config , str_model_id = '', verbose = 1000 ) # Call a function with name \"str_func_name\"\n","\n","    elif  'pMLP' in str(model_config['id']):\n","        model_fit_pytorch( model , X_train, Y_train, model_config , str_model_id, verbose )\n","        \n","    else:\n","        \n","        dict_prm_for_fit = {t : model_config[t] for t in ['epochs' ,   'batch_size' , 'verbose']   if t in model_config.keys()  }\n","        if len( dict_prm_for_fit ) == 0:\n","            model.fit( X_train, Y_train  )\n","        else:\n","            model.fit( X_train, Y_train , **dict_prm_for_fit  )\n","            \n","def model_fit_pytorch( model , X_train, Y_train, model_config , str_model_id = '', verbose = 1000 ):\n","    print('model_fit_pytorch ======= ',model)\n","    criterion = model_config.get( 'criterion', nn.BCELoss() )\n","    max_epoch = model_config.get('epochs', 10 )\n","    BATCH_SIZE = model_config.get('batch_size',  128 )\n","    LEARNING_RATE = model_config.get( 'LR' , 0.001 )\n","    \n","    optimizer = model_config.get( 'optimizer' ,  torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)  )\n","    lr_sched = model_config.get( 'lr_scheduler' , None ) \n","\n","    X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n","    Y_train = torch.tensor(Y_train, dtype=torch.float32).to(device)\n","    train_dataset = TensorDataset(X_train, Y_train)\n","    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True) \n","\n","    if verbose >= 100:\n","        print(str_model_id, 'Start model training. X_train.shape, Y_train.shape', X_train.shape, Y_train.shape )\n","    log_available_ram(f'After X_train initialization. Right before model train {str_model_id}')        \n","    \n","    ##################### Model Training ###################################################\n","    for epoch in range(max_epoch):\n","        t0_epoch = time.time()\n","        model.train() \n","        for i_batch, (x_batch, y_batch) in enumerate(train_dataloader): \n","            optimizer.zero_grad() \n","            preds = model(x_batch)\n","            loss = criterion(preds, y_batch) \n","            loss.backward() \n","            optimizer.step() \n","\n","        if lr_sched is not None:\n","            lr_sched.step() \n","\n","        if (verbose >= 10 ) and (i_batch % 100 == 0)  :\n","            print(str_model_id,  f'Epoch: {epoch}, batch: {i_batch},  train loss on batch: {loss.item():12.5f} , time: {time.time() - t0:.1f} ' )         \n","            \n","            \n","def model_fit_pytorch_Sophia_Andrey1(  model , X_train, Y_train, model_config , str_model_id = '', verbose = 1000 ):\n","    print('model_fit_pytorch_Sophia_Andrey1')\n","    criterion = model_config.get( 'criterion', nn.BCELoss() )\n","    max_epoch = model_config.get('epochs', 50 ) \n","    BATCH_SIZE = model_config.get('batch_size',  128 )\n","    LEARNING_RATE = model_config.get( 'LR' , 0.001 )\n","    criterion = model_config.get( 'criterion', nn.BCELoss() )\n","\n","    lr = LEARNING_RATE \n","    optimizer = SophiaG(model.parameters(),lr , betas=(0.965, 0.99), rho = 0.01, weight_decay=1e-1)\n","\n","    \n","    X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n","    Y_train = torch.tensor(Y_train, dtype=torch.float32).to(device)\n","    train_dataset = TensorDataset(X_train, Y_train)\n","    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True) \n","\n","    if verbose >= 100:\n","        print(str_model_id, 'Start model training. X_train.shape, Y_train.shape', X_train.shape, Y_train.shape )\n","    log_available_ram(f'After X_train initialization. Right before model train {str_model_id}')        \n","        \n","    ##################### Model Training ###################################################\n","    for epoch in range(max_epoch):\n","        print('epoch ======= ',epoch)\n","        t0_epoch = time.time()\n","        model.train() \n","        for i_batch, (x_batch, y_batch) in enumerate(train_dataloader): \n","            optimizer.zero_grad()   \n","            preds = model(x_batch)\n","            loss = criterion(preds, y_batch)\n","            loss.backward() \n","            optimizer.step() \n","\n","\n","        if (verbose >= 10 ) and (i_batch % 100 == 0)  :\n","            print(str_model_id,  f'Epoch: {epoch}, batch: {i_batch},  train loss on batch: {loss.item():12.5f} , time: {time.time() - t0:.1f} ' )         "]},{"cell_type":"markdown","metadata":{},"source":["# Models predict function"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def model_predict(model , XX,  model_config , str_model_id = '', verbose = 1000 ):\n","    \n","    if verbose >= 100:\n","        print('model_predict',  str_model_id,  model_config )\n","\n","    if  'pMLP' in str(model_config['id']):\n","        Y_pred = model_predict_pytorch( model ,XX,  model_config , str_model_id, verbose )\n","    else:\n","        Y_pred = model.predict( XX ) \n","    \n","    return Y_pred\n","\n","\n","def model_predict_pytorch( model ,XX,  model_config , str_model_id = '', verbose = 1000 ):\n","    print('model_predict_pytorch model ====== ',model_predict_pytorch)\n","    print('model_predict_pytorch xx ====== ',XX)\n","    t0_submit = time.time()\n","    XX = torch.tensor(XX, dtype=torch.float32).to(device)\n","    \n","    model.eval()\n","    with torch.no_grad():\n","        Y_pred = model(XX).cpu().numpy()\n","        \n","    if verbose >= 100: print(str_model_id,  f'Y_pred.shape: {Y_pred.shape}, type(Y_pred): {type(Y_pred)}, predict on submit time: {time.time() - t0_submit :.1f} ' )  \n","\n","    return Y_pred"]},{"cell_type":"markdown","metadata":{},"source":["# CAFA5 metric "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%time\n","\n","# Evaluation for CAFA \n","# https://github.com/BioComputingUP/CAFA-evaluator\n","flag_correct_metric_computation_bug_found_by_Anton = True\n","# https://www.kaggle.com/competitions/cafa-5-protein-function-prediction/discussion/420241 - Anton Vakhrushev - correcting error in the initial code of the metric computation - please upvote \n","import numpy as np\n","import pandas as pd\n","import multiprocessing as mp\n","import copy\n","import logging\n","\n","class Graph:\n","    \"\"\"\n","    Ontology class. One ontology == one namespace\n","    DAG is the adjacence matrix (sparse) which represent a Directed Acyclic Graph where\n","    DAG(i,j) == 1 means that the go term i is_a (or is part_of) j\n","    Parents that are in a different namespace are discarded\n","    \"\"\"\n","    def __init__(self, namespace, terms_dict, ia_dict=None, orphans=False):\n","        \"\"\"\n","        terms_dict = {term: {name: , namespace: , def: , alt_id: , rel:}}\n","        \"\"\"\n","        self.namespace = namespace\n","        self.dag = []  # [[], ...] terms (rows, axis 0) x parents (columns, axis 1)\n","        self.terms_dict = {}  # {term: {index: , name: , namespace: , def: }  used to assign term indexes in the gt\n","        self.terms_list = []  # [{id: term, name:, namespace: , def:, adg: [], children: []}, ...]\n","        self.idxs = None  # Number of terms\n","        self.order = None\n","        self.toi = None\n","        self.ia = None\n","\n","        rel_list = []\n","        for self.idxs, (term_id, term) in enumerate(terms_dict.items()):\n","            rel_list.extend([[term_id, rel, term['namespace']] for rel in term['rel']])\n","            self.terms_list.append({'id': term_id, 'name': term['name'], 'namespace': namespace, 'def': term['def'],\n","                                 'adj': [], 'children': []})\n","            self.terms_dict[term_id] = {'index': self.idxs, 'name': term['name'], 'namespace': namespace, 'def': term['def']}\n","            for a_id in term['alt_id']:\n","                self.terms_dict[a_id] = copy.copy(self.terms_dict[term_id])\n","        self.idxs += 1\n","\n","        self.dag = np.zeros((self.idxs, self.idxs), dtype='bool')\n","\n","        # id1 term (row, axis 0), id2 parent (column, axis 1)\n","        for id1, id2, ns in rel_list:\n","            if self.terms_dict.get(id2):\n","                i = self.terms_dict[id1]['index']\n","                j = self.terms_dict[id2]['index']\n","                self.dag[i, j] = 1\n","                self.terms_list[i]['adj'].append(j)\n","                self.terms_list[j]['children'].append(i)\n","                logging.debug(\"i,j {},{} {},{}\".format(i, j, id1, id2))\n","            else:\n","                logging.debug('Skipping branch to external namespace: {}'.format(id2))\n","        logging.debug(\"dag {}\".format(self.dag))\n","        # Topological sorting\n","        self.top_sort()\n","        logging.debug(\"order sorted {}\".format(self.order))\n","\n","        if orphans:\n","            self.toi = np.arange(self.dag.shape[0])  # All terms, also those without parents\n","        else:\n","            self.toi = np.nonzero(self.dag.sum(axis=1) > 0)[0]  # Only terms with parents\n","        logging.debug(\"toi {}\".format(self.toi))\n","\n","        if ia_dict is not None:\n","            self.set_ia(ia_dict)\n","\n","        return\n","\n","    def top_sort(self):\n","        \"\"\"\n","        Takes a sparse matrix representing a DAG and returns an array with nodes indexes in topological order\n","        https://en.wikipedia.org/wiki/Topological_sorting\n","        \"\"\"\n","        indexes = []\n","        visited = 0\n","        (rows, cols) = self.dag.shape\n","\n","        # create a vector containing the in-degree of each node\n","        in_degree = self.dag.sum(axis=0)\n","        # logging.debug(\"degree {}\".format(in_degree))\n","\n","        # find the nodes with in-degree 0 (leaves) and add them to the queue\n","        queue = np.nonzero(in_degree == 0)[0].tolist()\n","        # logging.debug(\"queue {}\".format(queue))\n","\n","        # for each element of the queue increment visits, add them to the list of ordered nodes\n","        # and decrease the in-degree of the neighbor nodes\n","        # and add them to the queue if they reach in-degree == 0\n","        while queue:\n","            visited += 1\n","            idx = queue.pop(0)\n","            indexes.append(idx)\n","            in_degree[idx] -= 1\n","            l = self.terms_list[idx]['adj']\n","            if len(l) > 0:\n","                for j in l:\n","                    in_degree[j] -= 1\n","                    if in_degree[j] == 0:\n","                        queue.append(j)\n","\n","        # if visited is equal to the number of nodes in the graph then the sorting is complete\n","        # otherwise the graph can't be sorted with topological order\n","        if visited == rows:\n","            self.order = indexes\n","        else:\n","            raise Exception(\"The sparse matrix doesn't represent an acyclic graph\")\n","\n","    def set_ia(self, ia_dict):\n","        self.ia = np.zeros(self.idxs, dtype='float')\n","        for term_id in self.terms_dict:\n","            if ia_dict.get(term_id):\n","                self.ia[self.terms_dict[term_id]['index']] = ia_dict.get(term_id)\n","            else:\n","                logging.debug('Missing IA for term: {}'.format(term_id))\n","        # Convert inf to zero\n","        np.nan_to_num(self.ia, copy=False, nan=0, posinf=0, neginf=0)\n","        self.toi = np.nonzero(self.ia > 0)[0]\n","\n","\n","class Prediction:\n","    \"\"\"\n","    The score matrix contains the scores given by the predictor for every node of the ontology\n","    \"\"\"\n","    def __init__(self, ids, matrix, idx, namespace=None):\n","        self.ids = ids\n","        self.matrix = matrix  # scores\n","        self.next_idx = idx\n","        # self.n_pred_seq = idx + 1\n","        self.namespace = namespace\n","\n","    def __str__(self):\n","        return \"\\n\".join([\"{}\\t{}\\t{}\".format(index, self.matrix[index], self.namespace) for index, _id in enumerate(self.ids)])\n","\n","\n","class GroundTruth:\n","    def __init__(self, ids, matrix, namespace=None):\n","        self.ids = ids\n","        self.matrix = matrix\n","        self.namespace = namespace\n","\n","\n","def propagate(matrix, ont, order, mode='max'):\n","    \"\"\"\n","    Update inplace the score matrix (proteins x terms) up to the root taking the max between children and parents\n","    \"\"\"\n","    if matrix.shape[0] == 0:\n","        raise Exception(\"Empty matrix\")\n","\n","    deepest = np.where(np.sum(matrix[:, order], axis=0) > 0)[0][0]\n","    if deepest.size == 0:\n","        raise Exception(\"The matrix is empty\")\n","\n","    # Remove leaves\n","    order_ = np.delete(order, [range(0, deepest)])\n","\n","    for i in order_:\n","        # Get direct children\n","        children = np.where(ont.dag[:, i] != 0)[0]\n","        if children.size > 0:\n","            cols = np.concatenate((children, [i]))\n","            if mode == 'max':\n","                matrix[:, i] = matrix[:, cols].max(axis=1)\n","            elif mode == 'fill':\n","                rows = np.where(matrix[:, i] == 0)[0]\n","                if rows.size:\n","                    idx = np.ix_(rows, cols)\n","                    if flag_correct_metric_computation_bug_found_by_Anton:\n","                        # Corrected way (see https://www.kaggle.com/competitions/cafa-5-protein-function-prediction/discussion/420241 )\n","                        matrix[rows, i] = matrix[idx].max(axis=1) #  matrix[idx].max(axis=1)[0] # Correction: https://www.kaggle.com/competitions/cafa-5-protein-function-prediction/discussion/420241\n","                    else:\n","                        # Old way - not corrected\n","                        matrix[rows, i] = matrix[idx].max(axis=1)[0] # Correction: https://www.kaggle.com/competitions/cafa-5-protein-function-prediction/discussion/420241\n","                        \n","    return\n","\n","\n","def obo_parser(obo_file, valid_rel=(\"is_a\", \"part_of\")):\n","    \"\"\"\n","    Parse a OBO file and returns a list of ontologies, one for each namespace.\n","    Obsolete terms are excluded as well as external namespaces.\n","    \"\"\"\n","    term_dict = {}\n","    term_id = None\n","    namespace = None\n","    name = None\n","    term_def = None\n","    alt_id = []\n","    rel = []\n","    obsolete = True\n","    with open(obo_file) as f:\n","        for line in f:\n","            line = line.strip().split(\": \")\n","            if line and len(line) > 1:\n","                k = line[0]\n","                v = \": \".join(line[1:])\n","                if k == \"id\":\n","                    # Populate the dictionary with the previous entry\n","                    if term_id is not None and obsolete is False and namespace is not None:\n","                        term_dict.setdefault(namespace, {})[term_id] = {'name': name,\n","                                                                       'namespace': namespace,\n","                                                                       'def': term_def,\n","                                                                       'alt_id': alt_id,\n","                                                                       'rel': rel}\n","                    # Assign current term ID\n","                    term_id = v\n","\n","                    # Reset optional fields\n","                    alt_id = []\n","                    rel = []\n","                    obsolete = False\n","                    namespace = None\n","\n","                elif k == \"alt_id\":\n","                    alt_id.append(v)\n","                elif k == \"name\":\n","                    name = v\n","                elif k == \"namespace\" and v != 'external':\n","                    namespace = v\n","                elif k == \"def\":\n","                    term_def = v\n","                elif k == 'is_obsolete':\n","                    obsolete = True\n","                elif k == \"is_a\" and k in valid_rel:\n","                    s = v.split('!')[0].strip()\n","                    rel.append(s)\n","                elif k == \"relationship\" and v.startswith(\"part_of\") and \"part_of\" in valid_rel:\n","                    s = v.split()[1].strip()\n","                    rel.append(s)\n","\n","        # Last record\n","        if obsolete is False and namespace is not None:\n","            term_dict.setdefault(namespace, {})[term_id] = {'name': name,\n","                                                          'namespace': namespace,\n","                                                          'def': term_def,\n","                                                          'alt_id': alt_id,\n","                                                          'rel': rel}\n","    return term_dict\n","\n","\n","def gt_parser(gt_file, ontologies):\n","    \"\"\"\n","    Parse ground truth file. Discard terms not included in the ontology.\n","    \"\"\"\n","    gt_dict = {}\n","    with open(gt_file) as f:\n","        for line in f:\n","            line = line.strip().split()\n","            if line:\n","                p_id, term_id = line[:2]\n","                for ont in ontologies:\n","                    if term_id in ont.terms_dict:\n","                        gt_dict.setdefault(ont.namespace, {}).setdefault(p_id, []).append(term_id)\n","                        break\n","\n","    gts = {}\n","    for ont in ontologies:\n","        if gt_dict.get(ont.namespace):\n","            matrix = np.zeros((len(gt_dict[ont.namespace]), ont.idxs), dtype='bool')\n","            ids = {}\n","            for i, p_id in enumerate(gt_dict[ont.namespace]):\n","                ids[p_id] = i\n","                for term_id in gt_dict[ont.namespace][p_id]:\n","                    matrix[i, ont.terms_dict[term_id]['index']] = 1\n","            propagate(matrix, ont, ont.order, mode='max')\n","            gts[ont.namespace] = GroundTruth(ids, matrix, ont.namespace)\n","\n","    return gts\n","\n","\n","def pred_parser(f, ontologies, gts, prop_mode, max_terms=None):\n","    \"\"\"\n","    Parse a prediction file and returns a list of prediction objects, one for each namespace.\n","    If a predicted is predicted multiple times for the same target, it stores the max.\n","    This is the slow step if the input file is huge, ca. 1 minute for 5GB input on SSD disk.\n","    \"\"\"\n","    ids = {}\n","    matrix = {}\n","    ns_dict = {}  # {namespace: term}\n","    onts = {ont.namespace: ont for ont in ontologies}\n","    for ns in gts:\n","        matrix[ns] = np.zeros(gts[ns].matrix.shape, dtype='float')\n","        ids[ns] = {}\n","        for term in onts[ns].terms_dict:\n","            ns_dict[term] = ns\n","\n","    for line in f:\n","        p_id, term_id, prob = line\n","        ns = ns_dict.get(term_id)\n","        if ns in gts and p_id in gts[ns].ids:\n","            i = gts[ns].ids[p_id]\n","            if max_terms is None or np.count_nonzero(matrix[ns][i]) <= max_terms:\n","                j = onts[ns].terms_dict.get(term_id)['index']\n","                ids[ns][p_id] = i\n","                matrix[ns][i, j] = max(matrix[ns][i, j], float(prob))\n","\n","    predictions = []\n","    for ns in ids:\n","        if ids[ns]:\n","            propagate(matrix[ns], onts[ns], onts[ns].order, mode=prop_mode)\n","            predictions.append(Prediction(ids[ns], matrix[ns], len(ids[ns]), ns))\n","\n","    if not predictions:\n","        raise Exception(\"Empty prediction, check format\")\n","\n","    return predictions\n","\n","\n","def ia_parser(file):\n","    ia_dict = {}\n","    with open(file) as f:\n","        for line in f:\n","            if line:\n","                term, ia = line.strip().split()\n","                ia_dict[term] = float(ia)\n","    return ia_dict\n","\n","# Computes the root terms in the dag\n","def get_roots_idx(dag):\n","    return np.where(dag.sum(axis=1) == 0)[0]\n","\n","\n","# Computes the leaf terms in the dag\n","def get_leafs_idx(dag):\n","    return np.where(dag.sum(axis=0) == 0)[0]\n","\n","\n","# Return a mask for all the predictions (matrix) >= tau\n","def solidify_prediction(pred, tau):\n","    return pred >= tau\n","\n","\n","# computes the f metric for each precision and recall in the input arrays\n","def compute_f(pr, rc):\n","    n = 2 * pr * rc\n","    d = pr + rc\n","    return np.divide(n, d, out=np.zeros_like(n, dtype=float), where=d != 0)\n","\n","\n","def compute_s(ru, mi):\n","    return np.sqrt(ru**2 + mi**2)\n","    # return np.where(np.isnan(ru), mi, np.sqrt(ru + np.nan_to_num(mi)))\n","\n","import time\n","from scipy.sparse import csr_matrix\n","\n","def compute_metrics_(tau_arr, g, pred, toi, n_gt, wn_gt=None, ic_arr=None):\n","\n","    verbose = 0;         \n","\n","    if verbose >= 10:\n","        t0 = time.time()\n","    \n","    metrics = np.zeros((len(tau_arr), 7), dtype='float')  # cov, pr, rc, wpr, wrc, ru, mi\n","\n","    if verbose >= 10:\n","        print('type(toi), toi', type(toi), toi )\n","    tmp = pred.matrix[:, toi]\n","    if verbose >= 10:\n","        print('type(tmp), tmp.shape', type(tmp), tmp.shape )\n","    p_s = csr_matrix(tmp )\n","    ic_arr_toi = ic_arr[toi]\n","    if verbose >= 10:\n","        print('type(ic_arr_toi), ic_arr_toi.shape', type(ic_arr_toi), ic_arr_toi.shape )\n","\n","    \n","    g_s = csr_matrix( g )\n","    \n","    if verbose >= 10:\n","        print( 'csr_matrix done %.1f'%(time.time( ) - t0 ), 'p_s.shape, g_s.shape:', p_s.shape, g_s.shape )    \n","\n","\n","    \n","    for i, tau in enumerate(tau_arr):\n","\n","        if verbose >= 100:\n","            t0 = time.time()\n","            print()\n","            print(i, tau, 'Start %.1f'%(time.time( ) - t0 ) )\n","        p = p_s > tau # solidify_prediction(p, tau)\n","        if verbose >= 100:\n","            print(i, tau, 'solidify done %.1f'%(time.time( ) - t0 ), 'p.shape:', p.shape,  )\n","\n","        # number of proteins with at least one term predicted with score >= tau\n","        metrics[i, 0] = (p.sum(axis=1) > 0).sum()\n","\n","        # Terms subsets\n","        intersection = p.multiply( g_s)  # TP\n","\n","\n","        if ic_arr is not None:\n","            \n","            # Weighted precision, recall\n","            wn_pred = p.dot( ic_arr_toi)\n","            wn_intersection =  intersection.dot( ic_arr_toi )\n","            \n","            if verbose >= 100:\n","                print(i, tau, 'After w_pred wn_intersection  %.1f'%(time.time( ) - t0 ) )\n","            \n","            metrics[i, 3] = np.divide(wn_intersection, wn_pred, out=np.zeros( wn_intersection.shape, dtype='float'),\n","                                      where=wn_pred > 0).sum()\n","            metrics[i, 4] = np.divide(wn_intersection, wn_gt, out=np.zeros(wn_intersection.shape, dtype='float'),\n","                                      where=n_gt > 0).sum()\n","            if verbose >= 100:\n","                print(i, tau, 'After metrics 3,4   %.1f'%(time.time( ) - t0 ) )\n","\n","\n","    return metrics\n","\n","\n","def compute_metrics(pred, gt, toi, tau_arr, ic_arr=None, n_cpu=0):\n","    \"\"\"\n","    Takes the prediction and the ground truth and for each threshold in tau_arr\n","    calculates the confusion matrix and returns the coverage,\n","    precision, recall, remaining uncertainty and misinformation.\n","    Toi is the list of terms (indexes) to be considered\n","    \"\"\"\n","    g = gt.matrix[:, toi]\n","    n_gt = g.sum(axis=1)\n","    wn_gt = None\n","    if ic_arr is not None:\n","        wn_gt = (g * ic_arr[toi]).sum(axis=1)\n","\n","    # Parallelization\n","    if n_cpu == 0:\n","        n_cpu = mp.cpu_count()\n","\n","    arg_lists = [[tau_arr, g, pred, toi, n_gt, wn_gt, ic_arr] for tau_arr in np.array_split(tau_arr, n_cpu)]\n","    if 0:\n","        # Original parallel way (# It does not work on Kaggle)\n","        arg_lists = [[tau_arr, g, pred, toi, n_gt, wn_gt, ic_arr] for tau_arr in np.array_split(tau_arr, n_cpu)]\n","        with mp.Pool(processes=n_cpu) as pool:\n","            metrics = np.concatenate(pool.starmap(compute_metrics_, arg_lists), axis=0)\n","    else: \n","        # no-parallel: \n","        metrics = compute_metrics_(tau_arr, g, pred, toi, n_gt, wn_gt, ic_arr )\n","\n","    return pd.DataFrame(metrics, columns=[\"cov\", \"pr\", \"rc\", \"wpr\", \"wrc\", \"ru\", \"mi\"])\n","\n","\n","def evaluate_prediction(prediction, gt, ontologies, tau_arr, normalization='cafa', n_cpu=0):\n","    dfs = []\n","    for p in prediction:\n","        ns = p.namespace\n","        ne = np.full(len(tau_arr), gt[ns].matrix.shape[0])\n","\n","        ont = [o for o in ontologies if o.namespace == ns][0]\n","\n","        # cov, pr, rc, wpr, wrc, ru, mi\n","        metrics = compute_metrics(p, gt[ns], ont.toi, tau_arr, ont.ia, n_cpu)\n","\n","        for column in [\"pr\", \"rc\", \"wpr\", \"wrc\", \"ru\", \"mi\"]:\n","            if normalization == 'gt' or (column in [\"rc\", \"wrc\"] and normalization == 'cafa'):\n","                metrics[column] = np.divide(metrics[column], ne, out=np.zeros_like(metrics[column], dtype='float'), where=ne > 0)\n","            else:\n","                metrics[column] = np.divide(metrics[column], metrics[\"cov\"], out=np.zeros_like(metrics[column], dtype='float'), where=metrics[\"cov\"] > 0)\n","\n","        metrics['ns'] = [ns] * len(tau_arr)\n","        metrics['tau'] = tau_arr\n","        metrics['cov'] = np.divide(metrics['cov'], ne, out=np.zeros_like(metrics['cov'], dtype='float'), where=ne > 0)\n","        metrics['f'] = compute_f(metrics['pr'], metrics['rc'])\n","        metrics['wf'] = compute_f(metrics['wpr'], metrics['wrc'])\n","        metrics['s'] = compute_s(metrics['ru'], metrics['mi'])\n","\n","        dfs.append(metrics)\n","\n","    return pd.concat(dfs)\n","\n","# Tau array, used to compute metrics at different score thresholds\n","th_step = 0.01\n","tau_arr = np.arange(0.01, 1, th_step)\n","#Consider terms without parents, e.g. the root(s), in the evaluation\n","no_orphans = False\n","# Parse and set information accretion (optional)\n","ia_dict = ia_parser('/kaggle/input/cafa-5-protein-function-prediction/IA.txt')\n","\n","# Parse the OBO file and creates a different graph for each namespace\n","ontologies = []\n","obo_file = '/kaggle/input/cafa-5-protein-function-prediction/Train/go-basic.obo'\n","for ns, terms_dict in obo_parser(obo_file).items():\n","    ontologies.append(Graph(ns, terms_dict, ia_dict, not no_orphans))"]},{"cell_type":"markdown","metadata":{},"source":["# Wrapper to call CAFA5 metric computation"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%time\n","import os.path\n","\n","######################################################################################3\n","###############  Load trainTerms\n","######################################################################################3\n","\n","print()\n","fn = '/kaggle/input/cafa-5-protein-function-prediction/Train/train_terms.tsv'\n","print(fn)\n","trainTerms = pd.read_csv(fn, sep=\"\\t\")\n","print(trainTerms.shape)\n","print('trainTerms memory_usage Mb:', trainTerms.memory_usage().sum()/1e6  )\n","display(trainTerms.head(3))\n","\n","def get_F1_etc_scores_official_CAFA_evaluation( Y_pred, IX, cutoff_threshold_low = 0.01,    make_plots = True ,  verbose = 0 ): \n","    '''\n","    Computation of F1-weighted scores are called here. \n","    Here we prepare Y_pred, Y in format required by functions provided by organizers - see github: https://github.com/BioComputingUP/CAFA-evaluator\n","    Y_pred  -  predictions\n","    IX  - indices selecting part which correspond to Y_pred in full Y \n","    Params:\n","    cutoff_threshold_low - predictions lower (strictly) will be dropped (effectively set to zero)\n","        (!) higher cutoff_threshold_low will improve  both RAM/speed. For most models 0.1 is Okay, and even 0.18. Consider using 0.1-0.15.   \n","    make_plots = True  - create plots of  F1,precision,recall (weighted) depending on threshold  \n","    verbose = 0\n","    \n","    Function uses external variables: \n","    trainTerms - training labels provided by orgs: /kaggle/input/cafa-5-protein-function-prediction/Train/train_terms.tsv\n","    vec_train_protein_ids - ids of the proteins in the current train - should correspond to \"X\" - features \n","    ontologies - data from: /kaggle/input/cafa-5-protein-function-prediction/Train/go-basic.obo\n","    tau_arr - array of thresholds \n","    \n","    Note: pay attention - Y_true is NOT an input argument. (Unusual for metric computation). \n","    We will create Y_true here from \"trainTerms\" cutting only those proteins which correspond to Y_pred indexes \"IX\".\n","    Thus it is highly important pass here the correct \"IX\" i.e. corresponding to Y_pred, otherwise results will not be correct\n","    '''\n","\n","    t00 = time.time()\n","    if verbose >= 100:\n","        print('Scoring starts. n_samples:', len(IX) )\n","\n","    ##########################################################################################\n","    # Prepare \"ground truth\" - \"gt\" terms(labels) in required format  \n","    ##########################################################################################\n","\n","    # First save to file, because function \"gt_parser\" works with files as input \n","    # Only part corresponding to providex indices IX will be generated \n","    t0 = time.time()\n","    trainTerms[ trainTerms.EntryID.isin(vec_train_protein_ids[IX]) ].to_csv('valid.tsv', sep='\\t', index=False) # Wall time: 4.11 s  for 28k samples\n","    if verbose >= 1000:\n","        print('save valid.csv %.1f'%(time.time() - t0 )) \n","\n","    # Prepare \"gt\" labels \n","    t0 = time.time()\n","    gt = gt_parser('valid.tsv', ontologies) # Wall time: 1min 22s  for 28k samples\n","    if verbose >= 100:\n","        print('gt_parser %.1f'%(time.time() - t0 ))\n","\n","    ##########################################################################################\n","    # prepare predicitons as list of triples - (protein, term(label), prediction) \n","    ##########################################################################################\n","\n","    t0 = time.time()\n","    vec_train_protein_ids_loc = vec_train_protein_ids[IX]\n","    preds = []\n","    for i in range(len(vec_train_protein_ids_loc)):\n","        for j in range(len(labels_to_consider)):\n","            if Y_pred[i,j] >= cutoff_threshold_low:\n","                preds.append((vec_train_protein_ids_loc[i], \n","                              labels_to_consider[j],\n","                              Y_pred[i,j]                        ))\n","    if verbose >= 1000:            \n","        print('create preds %.1f'%(time.time() - t0 ))       \n","\n","    ##########################################################################################\n","    # Parse predictions - propagation happens here  \n","    ##########################################################################################\n","    t0 = time.time()\n","    preds = pred_parser(preds, ontologies, gt, prop_mode='fill', max_terms=500) # \n","    if verbose >= 1000:            \n","        print('pred_parser %.1f'%(time.time() - t0 ), 'len(preds)', len(preds) )            \n","\n","    gc.collect()\n","\n","    ##########################################################################################\n","    # Main scores calculations happends here: \n","    ##########################################################################################\n","    # %%time\n","    t0 = time.time()\n","    df_metrics = evaluate_prediction(preds, gt, ontologies, tau_arr, n_cpu=1) # Wall time: 37.7 s for 28k samples\n","    if verbose >= 1000:            \n","        print('evaluate_prediction %.1f'%(time.time() - t0 ), 'got df_metrics with shape:', df_metrics.shape )            \n","    if verbose >= 10000:            \n","        display( df_metrics.head(2) )\n","\n","        \n","    ##########################################################################################\n","    # Comptutations finished. Below are optional plots, output preparartions etc.  \n","    ##########################################################################################\n","    \n","    ##########################################################################################\n","    ##########################################################################################\n","    ##########################################################################################\n","    ##########################################################################################\n","    ##########################################################################################\n","    \n","    \n","    if verbose >= 100:\n","        _t = df_metrics.groupby('ns').agg({'wf':'max'})\n","        display( _t )\n","        print( _t.mean() ) \n","\n","    if verbose >= 100:\n","        print('F1-scoring finished. %.1f secs passed'%(time.time() - t00 ))\n","\n","    # %%time\n","    if make_plots:\n","        try:\n","            list_uv = list(df_metrics['ns'].unique() )\n","            #print(list_uv)\n","            fig = plt.figure(figsize = (20,4))\n","            i0 = 0;\n","            for  col in  ['wf', 'wpr', 'wrc' ] :\n","                i0+=1\n","    #             print(i0,col)\n","                fig.add_subplot(1,3,i0)\n","\n","                for uv in list_uv:\n","                    mask = df_metrics['ns'] == uv\n","                    v = df_metrics[mask][col]\n","                    plt.plot(v.values, label = uv)\n","                plt.title(col, fontsize  = 20)\n","                plt.legend()\n","                plt.grid()\n","            plt.show()        \n","        except:\n","            print('Exception in plot')\n","    \n","    ########################################################################################\n","    # Prepare output of scores : \n","    ########################################################################################\n","    _t = {'cellular_component':'CCO', 'biological_process':'BPO','molecular_function':'MFO'}\n","    dict_scores_etc = {}\n","    df_s = df_metrics.groupby('ns').agg({'wf':'max'})\n","    dict_scores_etc['F1w'] = np.round( df_s.mean().iloc[0], 6) \n","    for k in _t:\n","        k2 = _t[k]\n","        # print(k,dict_scores_etc )\n","        if k in  df_s.index:\n","            dict_scores_etc['F1 '+ k2 ] = np.round( df_s.loc[k].iat[0], 6) \n","        else:\n","            dict_scores_etc['F1 '+ k2 ] = 0\n","\n","    ########################################################################################\n","    # Prepare output of thresholds : \n","    ########################################################################################\n","    for k in _t:\n","        k2 = _t[k]\n","        m = df_metrics['ns'] == k\n","        if m.sum()>0:\n","            IX = df_metrics[m]['wf'].argmax()\n","            thres_optimal = df_metrics[m]['tau'].iat[IX]\n","            dict_scores_etc['thres '+ k2 ] = thres_optimal\n","        else:\n","            dict_scores_etc['thres '+ k2 ] = 0\n","            \n","    dict_scores_etc['F-Scores Time'] = np.round( time.time() - t00   ,1)         \n","    if verbose >= 100:\n","        print('Scores: ', dict_scores_etc)        \n","\n","    if os.path.isfile('valid.tsv') :\n","        os.remove('valid.tsv')\n","        \n","    return  dict_scores_etc   "]},{"cell_type":"markdown","metadata":{},"source":["# Auxilliary functions compute/save scores, etc "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%time\n","import time \n","import gc\n","\n","def update_modeling_stat( df_stat, Y_pred,  Y,  flag_compute_cafa_f1 = False ,  str_model_id = '',  dict_optional_info = {}, verbose = 0):\n","    '''\n","    Compute/store/save scores/metrics/statistics on modelling.\n","    '''\n","    if verbose >= 100:\n","        print('Scoring starts')          \n","\n","    list_folds_ix =  np.sort(list ( set(folds))  )                \n","    for ix_fold  in  list_folds_ix[:n_folds_to_process]:\n","        t0 = time.time()\n","        IX_df_stat = len(df_stat)+1\n","        mask_fold = folds == ix_fold\n","        IX_loc = np.where(mask_fold >  0)[0]; \n","        df_stat.loc[IX_df_stat,'Model'] = str_model_id\n","        df_stat.loc[IX_df_stat,'Fold'] = ix_fold\n","\n","\n","        from sklearn.metrics import roc_auc_score\n","        s = roc_auc_score(Y[IX_loc,:].ravel(), Y_pred[IX_loc,:].ravel() )\n","        df_stat.loc[IX_df_stat, 'AUC'] = np.round( s,5)\n","\n","        \n","        ####################################################################\n","        #### Call CAFA-F1 computation - slow and RAM consuming - be careful \n","        ####################################################################\n","        if flag_compute_cafa_f1:\n","            dict_scores_etc = get_F1_etc_scores_official_CAFA_evaluation( Y_pred[IX_loc,:], IX_loc,  cutoff_threshold_low = cutoff_threshold_low ,\n","                                                                         make_plots = True ,  verbose = 10000 )\n","            for k in dict_scores_etc:\n","                df_stat.loc[IX_df_stat,k] = dict_scores_etc[k]\n","\n","            for t in [0.2, 0.3,0.4,0.5]:\n","                _c = (Y_pred >= t).ravel().sum()\n","                df_stat.loc[IX_df_stat,'GE%.1f per prot'%(t)] = np.round(_c/Y.shape[0])\n","        \n","            log_available_ram(f'CAFA-F1 scoring fold {ix_fold} finished. Model {str_model_id}')\n","        \n","        df_stat.loc[IX_df_stat, 'n_targets'] = Y.shape[1]\n","        df_stat.loc[IX_df_stat, 'n_samples Val'] = Y.shape[0]       \n","        df_stat.loc[IX_df_stat, 'Time scoring'] = np.round(time.time() - t0, 1 )    \n","        for k in dict_optional_info:\n","            val = dict_optional_info[k]\n","            df_stat.loc[IX_df_stat, k] = val   \n","        \n","        try:\n","            df_stat.loc[IX_df_stat, 'Features'] = str( list_features_id )    \n","            df_stat.loc[IX_df_stat, 'Model Features'] = str_model_id.split(' ')[-1] + ' ' + str( list_features_id )\n","            \n","        except:\n","            pass\n","        \n","        \n","        torch.cuda.empty_cache()\n","        gc.collect()\n","        df_stat.to_csv('df_stat.csv')     \n","        \n","        \n","    if verbose > 0:\n","        display(df_stat.tail(n_folds_to_process))\n","        print('Scoring finished. Seconds passed:  %.1f'%(time.time() - t0)  )\n","\n","    return df_stat\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print('%.1f seconds passed total '%(time.time()-t0start) )"]},{"cell_type":"markdown","metadata":{},"source":["# Train the model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%time\n","import gc \n","import time \n","import datetime\n","log_available_ram('Before modeling')\n","\n","######################### Params ##################################################3\n","\n","if 0: \n","    mode_submit = True \n","verbose = 0\n","\n","\n","######################### Output ##################################################\n","df_stat = pd.DataFrame()\n","\n","if (mode_submit is not None) and ( mode_submit != False ):\n","    Y_submit = np.zeros( (141865, Y.shape[1] )  , dtype = np.float16 )  # Predictions for submission will be stored here \n","    # Results from all models and all folds will be blended \n","    print('Y_submit mbytes:', Y_submit.nbytes/1024/1024)\n","cnt_blend_submit = 0 ;  \n","\n","if  flag_compute_oof_predictions:\n","    Y_pred_oof_blend  = np.zeros( ( Y.shape )  , dtype = np.float16 )\n","    print('Y_pred_oof_blend mbytes:', Y_pred_oof_blend.nbytes/1024/1024)\n","cnt_blend_oof = -1;\n","\n","\n","########################## Preparations ###########################################\n","log_available_ram('Right before modeling')\n","\n","if flag_compute_stat_for_each_model:  # Predictions OOF for each particular model - will be rewritten for each modelling \n","    Y_pred_oof = np.zeros( ( Y.shape )  , dtype = np.float16 )\n","    print('Y_pred_oof mbytes:', Y_pred_oof.nbytes/1024/1024)\n","\n","i_model = -1 # \n","i_config = -1 # conter for configurations \n","t0modeling = time.time()\n","list_folds_ix =  np.sort(list ( set(folds))  )\n","print(); print('Start training models',datetime.datetime.now()) ; print()\n","########################## Main modelling  ###########################################\n","for main_config_model_feature_etc  in list_main_config_model_feature_etc:\n","    i_config += 1 \n","    model_config = main_config_model_feature_etc['model']\n","    print('model_config.keys() ==== ',model_config.keys())\n","    if ('Keras' in model_config.keys() ) and ( model_config['Keras'] ): continue   # Keras models will be processed in the next cell - RAM leak problem\n","    if 'list_features_id' in main_config_model_feature_etc.keys():\n","        print()\n","        X,vec_train_protein_ids,X_submit,  submit_protein_ids = get_features(main_config_model_feature_etc['list_features_id'], verbose = 100)\n","        gc.collect()\n","        log_available_ram(f\"New features loaded:  {str(main_config_model_feature_etc['list_features_id'])}\" )  \n","        print()\n","        \n","    mode_downsample_train = model_config.get('mode_downsample_train', mode_downsample_train_default)\n","    \n","    n_selfblend = model_config.get( 'n_selfblend' , 1)\n","    if verbose >= 100:\n","        print(); print('Starting model_config:', model_config, f'time from start: {(time.time() - t0modeling ):.1f}' )\n","    for i_selfblend in range( n_selfblend ): # train-predict same model several times and blend predictions - especially useful for NN, but do not fix random seed (!)\n","        i_model += 1 # Models count\n","        t0one_model_all_folds = time.time()\n","        for ix_fold  in  list_folds_ix[:n_folds_to_process]:\n","        \n","            model, str_model_id = get_model(model_config)\n","            str_model_id_pure_save =  str_model_id\n","            str_model_id = str( i_model) + ' ' + str_model_id\n","            if n_selfblend > 1:  str_model_id += ' ' + str( i_selfblend )\n","\n","            ##################### Prepare train data ###################################################\n","            mask_fold = folds == ix_fold\n","            IX_train = np.where(mask_fold ==  0)[0]; \n","            # IX_train = [ix for ix in IX_train if ix in  set_allowed_train_indexes]\n","            IX_train = get_downsampled_IX_train(IX_train, mode_downsample_train )\n","            X_train = X[IX_train,:]; Y_train = Y[IX_train,:]\n","\n","            if verbose >= 10:\n","                print(f'fold {ix_fold}, model: {str_model_id},  X_train.shape: {X_train.shape}, Y_train.shape: {Y_train.shape}, time: { (time.time() - t0modeling):12.1f} ')\n","                print('X_train Mbytes:', X_train.nbytes/1024/1024, 'Y_train Mbytes:', Y_train.nbytes/1024/1024,  )\n","            ##################### Call train model ###################################################\n","            t0 = time.time()\n","            model_fit(model , X_train, Y_train, model_config , str_model_id, verbose = 0 )   \n","            time_fit = time.time() - t0\n","            if verbose >= 1000:\n","                print(f'time_fit {time_fit:.1f}' ) \n","            del X_train, Y_train\n","            torch.cuda.empty_cache()\n","            gc.collect()\n","            log_available_ram(f'After Model fit. ix_fold {ix_fold}, i_selfblend, {i_selfblend}, {str_model_id}')\n","            \n","            ##################### Compute predictions for submission and blend with the previous one ###################################################\n","            if  mode_submit : \n","                t0 = time.time()\n","                Y_submit = (Y_submit * cnt_blend_submit  + model_predict(model , X_submit,  model_config , str_model_id , verbose = 0 ) )/ (cnt_blend_submit + 1);  # Average predictions from different folds/models\n","                cnt_blend_submit += 1 \n","                time_pred_submit = time.time() - t0\n","                torch.cuda.empty_cache()\n","                gc.collect()\n","\n","            #flag_compute_oof_predictions = True                 \n","            if  flag_compute_oof_predictions:\n","                t0 = time.time()\n","                IX_val = np.where(mask_fold > 0 )[0]; \n","                X_val = X[IX_val,:];#  Y_val = Y[IX_val,:]\n","                Y_pred_val = model_predict(model , X_val,  model_config , str_model_id , verbose = 0 )\n","                time_pred_val = time.time() - t0\n","                if verbose >= 10000:\n","                    print('Y_pred_val.shape', Y_pred_val.shape, f'time_pred_val {time_pred_val:.1f}')\n","                    \n","                if ix_fold == 0: cnt_blend_oof += 1 \n","                Y_pred_oof_blend[IX_val,:] = (Y_pred_oof_blend[IX_val,:] * cnt_blend_oof  + Y_pred_val )/ (cnt_blend_oof + 1); \n","            \n","                if  flag_compute_stat_for_each_model:\n","                    Y_pred_oof[IX_val,:] = (Y_pred_val ) \n","                    \n","                del X_val, Y_pred_val                    \n","                torch.cuda.empty_cache()\n","                gc.collect()\n","                \n","            del model \n","            torch.cuda.empty_cache()\n","            gc.collect()\n","            log_available_ram(f'At fold end. Model {str_model_id}, i_selfblend {i_selfblend}, fold {ix_fold} ')\n","\n","        time_one_model = np.round( time.time() - t0one_model_all_folds )\n","        if flag_compute_stat_for_each_model and flag_compute_oof_predictions: \n","            update_modeling_stat(df_stat, Y_pred_oof,  Y, flag_compute_cafa_f1 = flag_compute_cafa_f1_for_each_model , \n","                                 str_model_id = str_model_id, dict_optional_info = {'Time': time_one_model , 'i_selfblend':i_selfblend,\n","                                'ModelID Pure':str_model_id_pure_save, 'i_config':i_config}, verbose = 0)\n","            torch.cuda.empty_cache()\n","            gc.collect()\n","            log_available_ram(f'After OOF-Stat Calculation. Model {str_model_id}, i_selfblend {i_selfblend}' )\n","            \n","        if flag_compute_each_blend_stat and flag_compute_oof_predictions:\n","            update_modeling_stat(df_stat, Y_pred_oof_blend,  Y, flag_compute_cafa_f1 = flag_compute_cafa_f1_for_each_blend , \n","                                 str_model_id = str(cnt_blend_oof )+ 'Blend'+ ' ' +str_model_id, dict_optional_info = {'Time': time_one_model, \n","                                        'Blend': cnt_blend_oof , 'i_selfblend':i_selfblend}, verbose = 0)\n","            torch.cuda.empty_cache()\n","            gc.collect()\n","            log_available_ram(f'After Blend-Stat Calculation. Model {str_model_id}, i_selfblend {i_selfblend}' )\n","                \n","if flag_save_numpy_Y_pred_oof_blend and flag_compute_oof_predictions:\n","    t0 = time.time()\n","    fn = 'Y_pred_oof_blend.npy'\n","    np.save(fn,Y_pred_oof_blend)\n","    print(f'File {fn} saved. Y_pred_oof_blend.shape: {Y_pred_oof_blend.shape}. Time: {(time.time()-t0):.1f}')\n","    t0 = time.time()\n","    fn = 'Y_labels.npy'\n","    np.save(fn,Y_labels)\n","    print(f'File {fn} saved. Time: {(time.time()-t0):.1f}')\n","\n","if flag_save_numpy_Y_submit and mode_submit:\n","    t0 = time.time()\n","    fn = 'Y_submit.npy'\n","    np.save(fn,Y_submit)\n","    print(f'File {fn} saved. Y_submit.shape: {Y_submit.shape}. Time: {(time.time()-t0):.1f}')\n","    t0 = time.time()\n","    fn = 'Y_labels.npy'\n","    np.save(fn,Y_labels)\n","    print(f'File {fn} saved. Time: {(time.time()-t0):.1f}')\n","\n","\n","if flag_compute_final_model_stat and  flag_compute_oof_predictions:  \n","    update_modeling_stat(df_stat, Y_pred_oof_blend,  Y, flag_compute_cafa_f1 = True , str_model_id= 'Final1 Blend', dict_optional_info = { }, verbose = 0)\n","    gc.collect()\n","    log_available_ram('After Final Stat Calculation')\n","            \n","display(df_stat)   \n","\n","print('%.1f seconds passed total '%(time.time()-t0start) )\n","log_available_ram('After Modelling 1 Finished')"]},{"cell_type":"markdown","metadata":{},"source":["## Modeling specific to Keras models\n","\n","Keras models leak RAM in the code above - so here we try to create a specific  code \n","for modelling with Keras \n","\n","The cell below is almost same as the one above, but we simplify things specifically to Keras models setup\n","\n","That is not intended technical modifaction - if we will find the ways to avoid memory leak in Keras models will return to uniform code as in the previous cell. \n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%time\n","######################### Params ##################################################3\n","\n","verbose = 1000\n","\n","######################### Output ##################################################\n","\n","########################## Preparations ###########################################\n","log_available_ram('Right before Keras modeling')\n","\n","\n","t0modeling = time.time()\n","list_folds_ix =  np.sort(list ( set(folds))  )\n","print(); print('Start training Keras models',datetime.datetime.now()) ; print()\n","########################## Main modelling  ###########################################\n","for main_config_model_feature_etc  in list_main_config_model_feature_etc:\n","    model_config = main_config_model_feature_etc['model']\n","    if 'Keras' not in model_config.keys(): continue  \n","    if  model_config['Keras'] == False : continue  \n","    if 'list_features_id' in main_config_model_feature_etc.keys():\n","        print()\n","        X,vec_train_protein_ids,X_submit,  submit_protein_ids = get_features(main_config_model_feature_etc['list_features_id'], verbose = 100)\n","        gc.collect()\n","        log_available_ram(f\"New features loaded:  {str(main_config_model_feature_etc['list_features_id'])}\" )  \n","        print()\n","    \n","    mode_downsample_train = model_config.get('mode_downsample_train', mode_downsample_train_default)\n","        \n","    n_selfblend = model_config.get( 'n_selfblend' , 1)\n","    if verbose >= 100:\n","        print(); print('Starting model_config:', model_config, f'time from start: {(time.time() - t0modeling ):.1f}' )\n","    for i_selfblend in range( n_selfblend ): # train-predict same model several times and blend predictions - especially useful for NN, but do not fix random seed (!)\n","        i_model += 1 # Models count\n","        t0one_model_all_folds = time.time()\n","        for ix_fold  in  list_folds_ix[:n_folds_to_process]:\n","            if verbose >= 10: \n","                print('---------------------------------------------- Fold', ix_fold, '----------------------------------------------')\n","                \n","            model, str_model_id = get_model(model_config)\n","            str_model_id_pure_save = str_model_id\n","            log_available_ram(f'After Keras model init. ix_fold {ix_fold}, n_selfblend, {n_selfblend}, {str_model_id}')\n","            str_model_id = str( i_model) + ' ' + str_model_id\n","            if n_selfblend > 1:  str_model_id += ' ' + str( i_selfblend )\n","                \n","            ##################### Prepare train data ###################################################\n","            mask_fold = folds == ix_fold\n","            IX_train = np.where(mask_fold ==  0)[0]; \n","            # IX_train = [ix for ix in IX_train if ix in  set_allowed_train_indexes]\n","            IX_train = get_downsampled_IX_train(IX_train, mode_downsample_train )\n","\n","            #X_train = X[IX_train,:]; Y_train = Y[IX_train,:]\n","            log_available_ram(f'After setting X_train. ix_fold {ix_fold}, n_selfblend, {n_selfblend}, {str_model_id}')\n","\n","            if verbose >= 10:\n","                print(f'fold {ix_fold}, model: {str_model_id},  X_train.shape: {X[IX_train,:].shape}, Y_train.shape: {Y[IX_train,:].shape}, time: { (time.time() - t0modeling):12.1f} ')\n","\n","            ##################### Call train model ###################################################\n","            t0 = time.time()\n","            #model_fit(model , X_train, Y_train, model_config , str_model_id, verbose = 0 )   \n","            epochs = model_config.get( 'epochs' , 15)\n","            batch_size = model_config.get( 'batch_size' , 128 )\n","            model.fit( X[IX_train,:], Y[IX_train,:] ,epochs = epochs,   batch_size = batch_size, verbose = 0  )\n","            time_fit = time.time() - t0\n","            if verbose >= 1000:\n","                print(f'time_fit {time_fit:.1f}' ) \n","            #del X_train, Y_train\n","            torch.cuda.empty_cache()\n","            gc.collect()\n","            log_available_ram(f'After Model fit. ix_fold {ix_fold}, i_selfblend, {i_selfblend}, {str_model_id}')\n","            \n","            ##################### Compute predictions for submission and blend with the previous one ###################################################\n","            if  mode_submit : \n","                t0 = time.time()\n","                Y_submit = (Y_submit * cnt_blend_submit  + model_predict(model , X_submit,  model_config , str_model_id , verbose = 0 ) )/ (cnt_blend_submit + 1);  # Average predictions from different folds/models\n","                cnt_blend_submit += 1 \n","                time_pred_submit = time.time() - t0\n","                torch.cuda.empty_cache()\n","                gc.collect()\n","                log_available_ram(f'After Predict on submit. ix_fold {ix_fold}, i_selfblend, {i_selfblend}, {str_model_id}')\n","                \n","            #flag_compute_oof_predictions = True                 \n","            if  flag_compute_oof_predictions:\n","                t0 = time.time()\n","                IX_val = np.where(mask_fold > 0 )[0]; \n","                #X_val = X[IX_val,:];#  Y_val = Y[IX_val,:]\n","                Y_pred_val = model_predict(model , X[IX_val,:],  model_config , str_model_id , verbose = 0 )\n","                time_pred_val = time.time() - t0\n","                if verbose >= 10000:\n","                    print('Y_pred_val.shape', Y_pred_val.shape, f'time_pred_val {time_pred_val:.1f}')\n","                if ix_fold == 0: cnt_blend_oof += 1 \n","                Y_pred_oof_blend[IX_val,:] = (Y_pred_oof_blend[IX_val,:] * cnt_blend_oof  + Y_pred_val )/ (cnt_blend_oof + 1); \n","            \n","                if  flag_compute_stat_for_each_model:\n","                    Y_pred_oof[IX_val,:] = (Y_pred_val ) \n","                    \n","                del  Y_pred_val                    \n","                torch.cuda.empty_cache()\n","                gc.collect()\n","                log_available_ram(f'After Predict on OOF. ix_fold {ix_fold}, i_selfblend, {i_selfblend}, {str_model_id}')\n","            \n","            keras.backend.clear_session()\n","            del model\n","            gc.collect()\n","            keras.backend.clear_session()\n","            torch.cuda.empty_cache()\n","            gc.collect()\n","            log_available_ram(f'At fold end. Model {str_model_id}, i_selfblend {i_selfblend}, fold {ix_fold} ')\n","\n","        time_one_model = np.round( time.time() - t0one_model_all_folds )\n","        if flag_compute_stat_for_each_model and flag_compute_oof_predictions: \n","            update_modeling_stat(df_stat, Y_pred_oof,  Y, flag_compute_cafa_f1 = flag_compute_cafa_f1_for_each_model , \n","                 str_model_id = str_model_id, dict_optional_info = {'Time': time_one_model, \n","                'i_selfblend':i_selfblend, 'ModelID Pure':str_model_id_pure_save, 'i_config':i_config }, verbose = 0)\n","            torch.cuda.empty_cache()\n","            gc.collect()\n","            log_available_ram(f'After OOF-Stat Calculation. Model {str_model_id}, i_selfblend {i_selfblend}' )\n","            \n","        if flag_compute_each_blend_stat and flag_compute_oof_predictions:\n","            update_modeling_stat(df_stat, Y_pred_oof_blend,  Y, flag_compute_cafa_f1 = flag_compute_cafa_f1_for_each_blend , \n","                                 str_model_id = str(cnt_blend_oof )+ 'Blend'+ ' ' +str_model_id, dict_optional_info = {'Time': time_one_model, \n","                                'Blend': cnt_blend_oof, 'i_selfblend':i_selfblend}, verbose = 0)\n","            torch.cuda.empty_cache()\n","            gc.collect()\n","            log_available_ram(f'After Blend-Stat Calculation. Model {str_model_id}, i_selfblend {i_selfblend}' )\n","                \n","if flag_save_numpy_Y_pred_oof_blend and flag_compute_oof_predictions:\n","    t0 = time.time()\n","    fn = 'Y_pred_oof_blend.npy'\n","    np.save(fn,Y_pred_oof_blend)\n","    print(f'File {fn} saved. Y_pred_oof_blend.shape: {Y_pred_oof_blend.shape}. Time: {(time.time()-t0):.1f}')\n","    t0 = time.time()\n","    fn = 'Y_labels.npy'\n","    np.save(fn,Y_labels)\n","    print(f'File {fn} saved. Time: {(time.time()-t0):.1f}')\n","\n","if flag_save_numpy_Y_submit and mode_submit:\n","    t0 = time.time()\n","    fn = 'Y_submit.npy'\n","    np.save(fn,Y_submit)\n","    print(f'File {fn} saved. Y_submit.shape: {Y_submit.shape}. Time: {(time.time()-t0):.1f}')\n","    t0 = time.time()\n","    fn = 'Y_labels.npy'\n","    np.save(fn,Y_labels)\n","    print(f'File {fn} saved. Time: {(time.time()-t0):.1f}')\n","    log_available_ram(f'After Save Y_submit' )\n","\n","\n","if flag_compute_final_model_stat and  flag_compute_oof_predictions:  \n","    #time_one_model = np.round( time.time() - t0one_model_all_folds )\n","    update_modeling_stat(df_stat, Y_pred_oof_blend,  Y, flag_compute_cafa_f1 = True , str_model_id= 'FinalKeras Blend', dict_optional_info = { }, verbose = 0)\n","    gc.collect()\n","    log_available_ram('After Final Stat Calculation')\n","            \n","display(df_stat)            \n","\n","if flag_compute_stat_for_each_model: \n","    del Y_pred_oof\n","    \n","torch.cuda.empty_cache()    \n","gc.collect()\n","\n","log_available_ram('After Modelling Keras Finished')"]},{"cell_type":"markdown","metadata":{},"source":["# Show/plot modelling results "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["if ('AUC' in df_stat.columns) and ( 'F1w' in df_stat.columns ):\n","    print(); print('Pearson correlations:')\n","    display( df_stat[['AUC','F1w']].corr() )\n","    \n","    print(); print('Spearman correlations:')\n","    display( df_stat[['AUC','F1w']].corr(method = 'spearman') )\n","\n","print(df_stat.shape)\n","display( df_stat )"]},{"cell_type":"markdown","metadata":{},"source":["## Plots in particular Fold-averaged "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print('Fold averaged')\n","df_stat = df_stat.copy()\n","df_stat['RankTmp'] = range(len(df_stat))\n","d = df_stat.groupby('Model').mean()\n","d = d.sort_values('RankTmp') # Make sure ordering is the same as in the intial df_stat\n","for col2 in ['F1w', 'AUC']:\n","    if  (col2 not in df_stat.columns ): continue \n","    print(col2, 'top  data:', )\n","    display( d.sort_values(col2, ascending = False).head(10) )\n","    plt.figure(figsize = (20,5) )\n","    plt.suptitle(col2 + ' Fold averaged', fontsize = 20 )\n","    \n","    plt.subplot(1,2,1)\n","    m = ['Blend' not in t for t in d.index ]\n","    plt.plot(d[col2][m],'*-')\n","    plt.title('Models (no blend)', fontsize = 20)\n","    plt.grid()# b='on')\n","    plt.xticks(rotation=90)\n","\n","    plt.subplot(1,2,2)\n","    m = ['Blend' in t for t in d.index ]\n","    plt.plot(d[col2][m],'*-')\n","    plt.title('Blend', fontsize = 20)\n","    plt.grid()# b='on')\n","    plt.xticks(rotation=90)\n","    \n","    plt.show()    \n","    \n","    d.sort_values(col2, ascending = False).to_csv(col2+'_sorted_fold_averaged.csv' )\n","\n","print()\n","print('Output top and tail sorted data')    \n","for col2 in ['F1w', 'AUC']:\n","    if  (col2 not in df_stat.columns ): continue \n","    print(col2, 'top 50  data:', )\n","    display( d.sort_values(col2, ascending = False).head(50) )\n","    print(col2, 'Tail 50  data:', )\n","    display( d.sort_values(col2, ascending = False).tail(50) )\n","    print(); print(); print(); print(); print(); print(); \n","    "]},{"cell_type":"markdown","metadata":{},"source":["## Grouping by model type and averageing (i.e. self-blend will be averaged) \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%time\n","print('Group for each model type')\n","print('Beware of possible grouping models on different input features - which may not be desirable - if so look on i_config grouping - below')\n","print('If all models on the same data - not at problem at all')\n","df_stat = df_stat.copy()\n","df_stat['RankTmp'] = range(len(df_stat))\n","if 'ModelID Pure' in df_stat.columns:\n","    d = df_stat.groupby('ModelID Pure').mean()\n","    d = d.sort_values('RankTmp') # Make sure ordering is the same as in the intial df_stat\n","    for col2 in ['F1w', 'AUC']:\n","        if  (col2 not in df_stat.columns ): continue \n","        print(col2, 'top  data:', )\n","        display( d.sort_values(col2, ascending = False).head(10) )\n","        plt.figure(figsize = (20,5) )\n","        plt.suptitle(col2 + ' Fold averaged', fontsize = 20 )\n","\n","        plt.subplot(1,2,1)\n","        m = ['Blend' not in t for t in d.index ]\n","        plt.plot(d[col2][m],'*-')\n","        plt.title('Models (no blend)', fontsize = 20)\n","        plt.grid()# b='on')\n","        plt.xticks(rotation=90)\n","        plt.show()    \n","\n","        d.sort_values(col2, ascending = False).to_csv(col2+'_sorted_groupped_by_model_type.csv' )\n","\n","    print()\n","    print('Output top and tail sorted data')    \n","    for col2 in ['F1w', 'AUC']:\n","        if  (col2 not in df_stat.columns ): continue \n","        print(col2, 'top 50  data:', )\n","        display( d.sort_values(col2, ascending = False).head(50) )\n","        print(col2, 'Tail 50  data:', )\n","        display( d.sort_values(col2, ascending = False).tail(50) )\n","        print(); print(); print(); print(); print(); print(); \n"]},{"cell_type":"markdown","metadata":{},"source":["## Grouping by i_cofing (all self-blends will be averaged) , different features will be preserved\n","\n","by the naming will disapped in groupped dataset - may be will update it later "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%time\n","print('Group for each i_config')\n","df_stat = df_stat.copy()\n","df_stat['RankTmp'] = range(len(df_stat))\n","if 'i_config' in df_stat.columns:\n","    d = df_stat.groupby('i_config').mean()\n","    d = d.sort_values('RankTmp') # Make sure ordering is the same as in the intial df_stat\n","    for col2 in ['F1w', 'AUC']:\n","        if  (col2 not in df_stat.columns ): continue \n","        print(col2, 'top  data:', )\n","        display( d.sort_values(col2, ascending = False).head(10) )\n","        plt.figure(figsize = (20,5) )\n","        plt.suptitle(col2 + ' Fold averaged', fontsize = 20 )\n","\n","        plt.subplot(1,2,1)\n","        m = np.ones( len(d) ).astype(bool) # ['Blend' not in t for t in d.index ]\n","        plt.plot(d[col2][m],'*-')\n","        plt.title('Models (no blend)', fontsize = 20)\n","        plt.grid()# b='on')\n","        plt.xticks(rotation=90)\n","        plt.show()    \n","\n","        d.sort_values(col2, ascending = False).to_csv(col2+'_sorted_groupped_by_i_config.csv' )\n","\n","    print()\n","    print('Output top and tail sorted data')    \n","    for col2 in ['F1w', 'AUC']:\n","        if  (col2 not in df_stat.columns ): continue \n","        print(col2, 'top 50  data:', )\n","        display( d.sort_values(col2, ascending = False).head(50) )\n","        print(col2, 'Tail 50  data:', )\n","        display( d.sort_values(col2, ascending = False).tail(50) )\n","        print(); print(); print(); print(); print(); print(); \n"]},{"cell_type":"markdown","metadata":{},"source":["## Plots without averaging by folds"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print('Plots without averaging by folds')\n","col = 'Model'\n","for col2 in ['F1w', 'AUC']:\n","    if ( col not in df_stat.columns) or (col2 not in df_stat.columns ): continue \n","    print(col2, 'top  data:', )\n","    display( df_stat.sort_values(col2, ascending = False).head(5) )\n","    plt.figure(figsize = (20,5) )\n","    plt.suptitle(col2, fontsize = 20 )\n","    \n","    plt.subplot(1,2,1)\n","    m = ['Blend' not in t for t in df_stat[col]]\n","    plt.plot(df_stat[col2][m],'*-')\n","    plt.title('Models (no blend)', fontsize = 20)\n","    plt.grid()# b='on')\n","    plt.xticks(rotation=90)\n","\n","    \n","    plt.subplot(1,2,2)\n","    m = ['Blend' in t for t in df_stat[col]]\n","    plt.plot(df_stat[col2][m],'*-')\n","    plt.title('Blend', fontsize = 20)\n","    plt.grid()# b='on')\n","    plt.xticks(rotation=90)\n","    \n","    plt.show()    \n","    \n","    \n","print()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["if 'AUC' in df_stat.columns:\n","    plt.plot(df_stat['AUC'].values,'*-')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%time\n","col = 'AUC' \n","col2 = 'Fold'\n","if (col in df_stat.columns) and ( col2 in df_stat.columns ):\n","    d2 = df_stat.groupby(col2).mean()\n","    display(d2)\n","    d2.to_csv('df_stat_folds_mean.csv')\n","    plt.plot(d2[col].values,'*-')\n","    plt.xlabel(col2,fontsize = 20 )\n","    plt.title(col,fontsize = 20)\n","    plt.show()\n","\n","col2 = 'Model'\n","if (col in df_stat.columns) and ( col2 in df_stat.columns ):\n","    d3 = df_stat.groupby(col2).mean()\n","    display(d3)\n","    d3.to_csv('df_stat_models_mean.csv')\n","    plt.plot(d3[col].values,'*-')\n","    plt.title(col,fontsize = 20)\n","    plt.xlabel(col2,fontsize = 20 )\n","    plt.show()\n","    display()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["m = df_stat['Model'] == 'Final'\n","if m.sum() == 0:\n","    m =  ( df_stat['Model'] == 'FinalKeras Blend' )\n","if m.sum() == 0:\n","    m = ( df_stat['Model'] == 'Final1 Blend' )\n","\n","\n","display(df_stat[m] )\n","display(df_stat[m].mean() )\n","if 'F1w' in df_stat.columns:\n","    print( df_stat[m].mean().loc['F1w']  )\n","df_stat[m].mean().to_csv('final_means.csv')\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# Prepare submission"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%time\n","import gc\n","if 0:\n","    del X_train,Y_train, X_val, Y_val, train_dataset, train_dataloader, X, Y\n","gc.collect()\n","torch.cuda.empty_cache()\n","\n","log_available_ram()"]},{"cell_type":"markdown","metadata":{},"source":["## Prepare submission tsv file"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%time\n","\n","mode_submit_prepare = 'slow_less_RAM_consuming'\n","# 'slow_less_RAM_consuming' - works slower but consumes less RAM\n","\n","import time \n","t0 = time.time()\n","\n","print( mode_submit_prepare , mode_submit)\n","if (mode_submit ) and ( flag_save_final_submit_file ):\n","    print(Y_submit.shape)\n","    if mode_submit_prepare == 'slow_less_RAM_consuming':\n","\n","\n","        file_path = \"submission.tsv\"\n","        cc = 0\n","        cc2 = 0\n","        with open(file_path, 'w') as file:\n","            for i in range(Y_submit.shape[0]):\n","                for j in range(Y_submit.shape[1]):\n","                    val = Y_submit[i,j]\n","                    if val >= cutoff_threshold_low:\n","                        str_go_term = str(Y_labels[j])\n","                        str_protein_id = str( submit_protein_ids[i] )\n","                        str_save = str_protein_id+'\\t'+str_go_term + '\\t' + '%.3f'%val + '\\n'\n","                        file.write(str_save)   \n","                        cc2 +=1\n","                        if cc2 <= 10:\n","                            if cc2 == 1: print('First 10 examples of the saved data:')\n","                            print(str_save)\n","                    cc += 1\n","                    if cc % 30_000_000  == 0: \n","                        sz = Y_submit.shape[0]*Y_submit.shape[1]\n","                        print(cc, 'out of',sz, 'percent %.2f'%(cc/sz*100), 'saved:'  ,cc2, 'time %.1f'%(time.time() - t0 ))\n","\n","        print(cc2,'results saved to submission file', 'time  %.1f'%(time.time() - t0 )  )                \n","\n","    else:\n","\n","        # That is widely used way to preparase submission , but it might crash by RAM \n","\n","        df_submission = pd.DataFrame(columns = ['Protein Id', 'GO Term Id','Prediction'])\n","\n","        n_targets_predicted = Y_submit.shape[1]\n","        n_samples_predicted = Y_submit.shape[0]\n","        print('n_samples_predicted, n_targets_predicted',  n_samples_predicted, n_targets_predicted )\n","\n","\n","        protein_list = []\n","        for k in list(submit_protein_ids):\n","            protein_list += [k] * n_targets_predicted\n","        df_submission['Protein Id'] = protein_list\n","\n","        df_submission['GO Term Id'] = list(Y_labels) * n_samples_predicted\n","        df_submission['Prediction'] = Y_submit.ravel()\n","\n","        df_submission = df_submission.round(3)\n","        df_submission = df_submission[ df_submission['Prediction'] >= cutoff_threshold_low  ]\n","\n","        memory_usage_per_column = df_submission.memory_usage(deep=True)\n","        total_memory_usage = memory_usage_per_column.sum()\n","        print(\"\\nTotal memory usage:\", total_memory_usage/1e6, \"Megabytes\")\n","\n","        print(df_submission.shape)\n","        display(df_submission)\n","\n","        import gc\n","        if 0:\n","            del preds \n","\n","        gc.collect()\n","\n","        df_submission.to_csv(\"submission.tsv\",header=False, index=False,sep='\\t')\n","\n","\n","    log_available_ram('After saving submission')    "]},{"cell_type":"markdown","metadata":{},"source":["## Plot histograms on submission predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%time\n","try:\n","    print(df_submission.shape)\n","    plt.figure(figsize = (15,4))\n","    plt.hist(df_submission['Prediction'].values, bins = 1000 )\n","    plt.show()\n","    print(df_submission.shape)\n","    display(df_submission.describe())\n","\n","    for t in [0.1,0.2, 0.25,0.28, 0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]:\n","        m = df_submission['Prediction'] > t\n","        print(t, m.sum(), m.sum()/ (n_samples_predicted * n_targets_predicted ) )\n","\n","    print()    \n","    try:\n","        print( Y.sum(),  Y.sum()/ (Y.shape[0] * Y.shape[1]) )\n","    except:\n","        pass    \n","\n","    print('Here is fast rationale why we should think of threshold for F1 is around 0.28 - number of 1 in that case corresponds to train data')\n","except:\n","    pass\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print('%.1f seconds passed total '%(time.time()-t0start) )"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":5521661,"sourceId":41875,"sourceType":"competition"},{"datasetId":2673536,"sourceId":4584547,"sourceType":"datasetVersion"},{"datasetId":3167603,"sourceId":5499219,"sourceType":"datasetVersion"},{"datasetId":3225525,"sourceId":5607816,"sourceType":"datasetVersion"},{"datasetId":3207078,"sourceId":5806679,"sourceType":"datasetVersion"},{"datasetId":3207084,"sourceId":5806682,"sourceType":"datasetVersion"},{"datasetId":3207096,"sourceId":5806692,"sourceType":"datasetVersion"},{"datasetId":3211581,"sourceId":5807641,"sourceType":"datasetVersion"},{"datasetId":3207113,"sourceId":5807722,"sourceType":"datasetVersion"},{"datasetId":3189532,"sourceId":6278150,"sourceType":"datasetVersion"},{"datasetId":3614490,"sourceId":6285621,"sourceType":"datasetVersion"}],"dockerImageVersionId":30476,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
