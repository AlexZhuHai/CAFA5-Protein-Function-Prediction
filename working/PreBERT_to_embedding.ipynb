{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e4d974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo\n",
    "import numpy as np\n",
    "# Load the .npy file\n",
    "data = np.load('filename.npy')\n",
    "\n",
    "print(\"Load ProtBERT Model...\")\n",
    "# PROT BERT LOADING :\n",
    "from transformers import BertModel, BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False )\n",
    "model = BertModel.from_pretrained(\"Rostlab/prot_bert\").to(config.device)\n",
    "\n",
    "def get_bert_embedding(\n",
    "    sequence : str,\n",
    "    len_seq_limit : int\n",
    "):\n",
    "    '''\n",
    "    Function to collect last hidden state embedding vector from pre-trained ProtBERT Model\n",
    "\n",
    "    INPUTS:\n",
    "    - sequence (str) : protein sequence (ex : AAABBB) from fasta file\n",
    "    - len_seq_limit (int) : maximum sequence lenght (i.e nb of letters) for truncation\n",
    "\n",
    "    OUTPUTS:\n",
    "    - output_hidden : last hidden state embedding vector for input sequence of length 1024\n",
    "    '''\n",
    "    sequence_w_spaces = ' '.join(list(sequence))\n",
    "    encoded_input = tokenizer(\n",
    "        sequence_w_spaces,\n",
    "        truncation=True,\n",
    "        max_length=len_seq_limit,\n",
    "        padding='max_length',\n",
    "        return_tensors='pt').to(config.device)\n",
    "    output = model(**encoded_input)\n",
    "    output_hidden = output['last_hidden_state'][:,0][0].detach().cpu().numpy()\n",
    "    assert len(output_hidden)==1024\n",
    "    return output_hidden\n",
    "\n",
    "### COLLECTING FOR TRAIN SAMPLES :\n",
    "print(\"Loading train set ProtBERT Embeddings...\")\n",
    "fasta_train = SeqIO.parse(config.train_sequences_path, \"fasta\")\n",
    "print(\"Total Nb of Elements : \", len(list(fasta_train)))\n",
    "fasta_train = SeqIO.parse(config.train_sequences_path, \"fasta\")\n",
    "ids_list = []\n",
    "embed_vects_list = []\n",
    "t0 = time.time()\n",
    "checkpoint = 0\n",
    "for item in tqdm(fasta_train):\n",
    "    ids_list.append(item.id)\n",
    "    embed_vects_list.append(\n",
    "        get_bert_embedding(sequence = item.seq, len_seq_limit = 1200))\n",
    "    checkpoint+=1\n",
    "    if checkpoint>=100:\n",
    "        df_res = pd.DataFrame(data={\"id\" : ids_list, \"embed_vect\" : embed_vects_list})\n",
    "        np.save('/kaggle/working/train_ids.npy',np.array(ids_list))\n",
    "        np.save('/kaggle/working/train_embeddings.npy',np.array(embed_vects_list))\n",
    "        checkpoint=0\n",
    "\n",
    "np.save('/kaggle/working/train_ids.npy',np.array(ids_list))\n",
    "np.save('/kaggle/working/train_embeddings.npy',np.array(embed_vects_list))\n",
    "print('Total Elapsed Time:',time.time()-t0)\n",
    "\n",
    "### COLLECTING FOR TEST SAMPLES :\n",
    "print(\"Loading test set ProtBERT Embeddings...\")\n",
    "fasta_test = SeqIO.parse(config.test_sequences_path, \"fasta\")\n",
    "print(\"Total Nb of Elements : \", len(list(fasta_test)))\n",
    "fasta_test = SeqIO.parse(config.test_sequences_path, \"fasta\")\n",
    "ids_list = []\n",
    "embed_vects_list = []\n",
    "t0 = time.time()\n",
    "checkpoint=0\n",
    "for item in tqdm(fasta_test):\n",
    "    ids_list.append(item.id)\n",
    "    embed_vects_list.append(\n",
    "        get_bert_embedding(sequence = item.seq, len_seq_limit = 1200))\n",
    "    checkpoint+=1\n",
    "    if checkpoint>=100:\n",
    "        np.save('/kaggle/working/test_ids.npy',np.array(ids_list))\n",
    "        np.save('/kaggle/working/test_embeddings.npy',np.array(embed_vects_list))\n",
    "        checkpoint=0\n",
    "\n",
    "np.save('/kaggle/working/test_ids.npy',np.array(ids_list))\n",
    "np.save('/kaggle/working/test_embeddings.npy',np.array(embed_vects_list))\n",
    "print('Total Elasped Time:',time.time()-t0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
