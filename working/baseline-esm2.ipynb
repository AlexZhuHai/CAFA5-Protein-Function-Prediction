{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c87e4fa-c372-48a6-8be2-f04a76eb6d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch import nn\n",
    "from torch.utils.data import random_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchmetrics.classification import MultilabelF1Score\n",
    "from torchmetrics.classification import MultilabelAccuracy\n",
    "import pytorch_lightning as pl\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# sub = pd.read_csv(\"../input/cafa-5-protein-function-prediction/sample_submission.tsv\", sep= \"\\t\", header = None)\n",
    "# sub.columns = [\"The Protein ID\", \"The Gene Ontology term (GO) ID\", \"Predicted link probability that GO appear in Protein\"]\n",
    "# print(sub.head(5))\n",
    "\n",
    "MAIN_DIR = \"../input/cafa-5-protein-function-prediction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7758d3b-75bb-4e32-8811-dffd358c6224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n",
      "BEGIN TRAINING...\n",
      "EPOCH  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████████████████████████████▏        | 779/1001 [00:14<00:04, 52.12it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 235\u001b[0m\n\u001b[1;32m    231\u001b[0m     scores_history \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m : train_f1score_history, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m : val_f1score_history}\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, losses_history, scores_history\n\u001b[0;32m--> 235\u001b[0m ems2_model, ems2_losses, ems2_scores \u001b[38;5;241m=\u001b[39m train_model(embeddings_source\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEMS2\u001b[39m\u001b[38;5;124m\"\u001b[39m,model_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvolutional\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    236\u001b[0m \u001b[38;5;66;03m# ems2_model, ems2_losses, ems2_scores = train_model(embeddings_source=\"EMS2\",model_type=\"convolutional\")\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m#t5_model, t5_losses, t5_scores = train_model(embeddings_source=\"T5\",model_type=\"linear\")\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m#protbert_model, protbert_losses, protbert_scores = train_model(embeddings_source=\"ProtBERT\",model_type=\"linear\")\u001b[39;00m\n\u001b[1;32m    240\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m4\u001b[39m))\n",
      "Cell \u001b[0;32mIn[4], line 194\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(embeddings_source, model_type, train_size)\u001b[0m\n\u001b[1;32m    192\u001b[0m preds \u001b[38;5;241m=\u001b[39m model(embed)\n\u001b[1;32m    193\u001b[0m loss\u001b[38;5;241m=\u001b[39m CrossEntropy(preds, targets)\n\u001b[0;32m--> 194\u001b[0m score\u001b[38;5;241m=\u001b[39mf1_score(preds, targets)\n\u001b[1;32m    195\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem()) \n\u001b[1;32m    196\u001b[0m scores\u001b[38;5;241m.\u001b[39mappend(score\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torchmetrics/metric.py:304\u001b[0m, in \u001b[0;36mMetric.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_full_state_update(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_reduce_state_update(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cache\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torchmetrics/metric.py:373\u001b[0m, in \u001b[0;36mMetric._forward_reduce_state_update\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# allow grads for batch computation\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;66;03m# calculate batch state and compute batch value\u001b[39;00m\n\u001b[0;32m--> 373\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    374\u001b[0m batch_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# reduce batch and global state\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torchmetrics/metric.py:466\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_grad):\n\u001b[1;32m    465\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 466\u001b[0m         update(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    468\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected all tensors to be on\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torchmetrics/classification/stat_scores.py:479\u001b[0m, in \u001b[0;36mMultilabelStatScores.update\u001b[0;34m(self, preds, target)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Update state with predictions and targets.\"\"\"\u001b[39;00m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate_args:\n\u001b[0;32m--> 479\u001b[0m     _multilabel_stat_scores_tensor_validation(\n\u001b[1;32m    480\u001b[0m         preds, target, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultidim_average, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index\n\u001b[1;32m    481\u001b[0m     )\n\u001b[1;32m    482\u001b[0m preds, target \u001b[38;5;241m=\u001b[39m _multilabel_stat_scores_format(\n\u001b[1;32m    483\u001b[0m     preds, target, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthreshold, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index\n\u001b[1;32m    484\u001b[0m )\n\u001b[1;32m    485\u001b[0m tp, fp, tn, fn \u001b[38;5;241m=\u001b[39m _multilabel_stat_scores_update(preds, target, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultidim_average)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torchmetrics/functional/classification/stat_scores.py:615\u001b[0m, in \u001b[0;36m_multilabel_stat_scores_tensor_validation\u001b[0;34m(preds, target, num_labels, multidim_average, ignore_index)\u001b[0m\n\u001b[1;32m    609\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    610\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected both `target.shape[1]` and `preds.shape[1]` to be equal to the number of labels\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    611\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpreds\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_labels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    612\u001b[0m     )\n\u001b[1;32m    614\u001b[0m \u001b[38;5;66;03m# Check that target only contains [0,1] values or value in ignore_index\u001b[39;00m\n\u001b[0;32m--> 615\u001b[0m unique_values \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39munique(target)\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ignore_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    617\u001b[0m     check \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39many((unique_values \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m&\u001b[39m (unique_values \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/_jit_internal.py:499\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_false(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/_jit_internal.py:499\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_false(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/functional.py:991\u001b[0m, in \u001b[0;36m_return_output\u001b[0;34m(input, sorted, return_inverse, return_counts, dim)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unique_impl(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28msorted\u001b[39m, return_inverse, return_counts, dim)\n\u001b[0;32m--> 991\u001b[0m output, _, _ \u001b[38;5;241m=\u001b[39m _unique_impl(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28msorted\u001b[39m, return_inverse, return_counts, dim)\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/functional.py:905\u001b[0m, in \u001b[0;36m_unique_impl\u001b[0;34m(input, sorted, return_inverse, return_counts, dim)\u001b[0m\n\u001b[1;32m    897\u001b[0m     output, inverse_indices, counts \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39munique_dim(\n\u001b[1;32m    898\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    899\u001b[0m         dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    902\u001b[0m         return_counts\u001b[38;5;241m=\u001b[39mreturn_counts,\n\u001b[1;32m    903\u001b[0m     )\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 905\u001b[0m     output, inverse_indices, counts \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_unique2(\n\u001b[1;32m    906\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;28msorted\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28msorted\u001b[39m,\n\u001b[1;32m    908\u001b[0m         return_inverse\u001b[38;5;241m=\u001b[39mreturn_inverse,\n\u001b[1;32m    909\u001b[0m         return_counts\u001b[38;5;241m=\u001b[39mreturn_counts,\n\u001b[1;32m    910\u001b[0m     )\n\u001b[1;32m    911\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output, inverse_indices, counts\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class config:\n",
    "    train_sequences_path = MAIN_DIR  + \"../Train/train_sequences.fasta\"\n",
    "    train_labels_path = MAIN_DIR + \"/Train/train_terms.tsv\"\n",
    "    test_sequences_path = MAIN_DIR + \"../Test (Targets)/testsuperset.fasta\"\n",
    "    \n",
    "    num_labels = 500\n",
    "    n_epochs = 10\n",
    "    batch_size = 128\n",
    "    lr = 0.001\n",
    "\n",
    "    device = torch.device('mps')\n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(config.device)\n",
    "\n",
    "# Directories for the different embedding vectors : \n",
    "embeds_map = {\n",
    "    \"T5\" : \"t5embeds\",\n",
    "    \"ProtBERT\" : \"protbert-embeddings-for-cafa5\",\n",
    "    \"EMS2\" : \"cafa-5-ems-2-embeddings-numpy\"\n",
    "}\n",
    "\n",
    "# Length of the different embedding vectors :\n",
    "embeds_dim = {\n",
    "    \"T5\" : 1024,\n",
    "    \"ProtBERT\" : 1024,\n",
    "    \"EMS2\" : 1280\n",
    "}\n",
    "\n",
    "class ProteinSequenceDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, datatype, embeddings_source):\n",
    "        super(ProteinSequenceDataset).__init__()\n",
    "        self.datatype = datatype\n",
    "        \n",
    "        if embeddings_source in [\"ProtBERT\", \"EMS2\"]:\n",
    "            embeds = np.load(\"../input/\"+embeds_map[embeddings_source]+\"/\"+datatype+\"_embeddings.npy\")\n",
    "            ids = np.load(\"../input/\"+embeds_map[embeddings_source]+\"/\"+datatype+\"_ids.npy\")\n",
    "        \n",
    "        if embeddings_source == \"T5\":\n",
    "            embeds = np.load(\"../input/\"+embeds_map[embeddings_source]+\"/\"+datatype+\"_embeds.npy\")\n",
    "            ids = np.load(\"../input/\"+embeds_map[embeddings_source]+\"/\"+datatype+\"_ids.npy\")\n",
    "            \n",
    "        embeds_list = []\n",
    "        for l in range(embeds.shape[0]):\n",
    "            embeds_list.append(embeds[l,:])\n",
    "        self.df = pd.DataFrame(data={\"EntryID\": ids, \"embed\" : embeds_list})\n",
    "        \n",
    "        if datatype==\"train\":\n",
    "            np_labels = np.load(\n",
    "                \"../input/train-targets-top\"+str(config.num_labels)+ \\\n",
    "                \"/train_targets_top\"+str(config.num_labels)+\".npy\")\n",
    "            df_labels = pd.DataFrame(self.df['EntryID'])\n",
    "            df_labels['labels_vect']=[row for row in np_labels]\n",
    "            self.df = self.df.merge(df_labels, on=\"EntryID\")\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        embed = torch.tensor(self.df.iloc[index][\"embed\"] , dtype = torch.float32)\n",
    "        if self.datatype==\"train\":\n",
    "            targets = torch.tensor(self.df.iloc[index][\"labels_vect\"], dtype = torch.float32)\n",
    "            return embed, targets\n",
    "        if self.datatype==\"test\":\n",
    "            id = self.df.iloc[index][\"EntryID\"]\n",
    "            return embed, id\n",
    "\n",
    "\n",
    "class MultiLayerPerceptron(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(input_dim, 864)\n",
    "        self.activation1 = torch.nn.ReLU() \n",
    "        self.linear2 = torch.nn.Linear(864, 712)\n",
    "        self.activation2 = torch.nn.ReLU()\n",
    "        self.linear3 = torch.nn.Linear(712, num_classes)\n",
    "      \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "        \n",
    "\n",
    "# class CNN1D(nn.Module):\n",
    "#     def __init__(self, input_dim, num_classes):\n",
    "#         super(CNN1D, self).__init__()\n",
    "#         # (batch_size, channels, embed_size)\n",
    "#         self.conv1 = nn.Conv1d(in_channels=1, out_channels=3, kernel_size=3, dilation=1, padding=1, stride=1)\n",
    "#         # (batch_size, 3, embed_size)\n",
    "#         self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "#         # (batch_size, 3, embed_size/2 = 512)\n",
    "#         self.conv2 = nn.Conv1d(in_channels=3, out_channels=8, kernel_size=3, dilation=1, padding=1, stride=1)\n",
    "#         # (batch_size, 8, embed_size/2 = 512)\n",
    "#         self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "#         # (batch_size, 8, embed_size/4 = 256)\n",
    "#         self.fc1 = nn.Linear(in_features=int(8 * input_dim/4), out_features=864)\n",
    "#         self.fc2 = nn.Linear(in_features=864, out_features=num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.reshape(x.shape[0], 1, x.shape[1])\n",
    "#         x = self.pool1(nn.functional.tanh(self.conv1(x)))\n",
    "#         x = self.pool2(nn.functional.tanh(self.conv2(x)))\n",
    "#         x = torch.flatten(x, 1)\n",
    "#         x = nn.functional.tanh(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AttentionModule(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(AttentionModule, self).__init__()\n",
    "        self.attention_weights = nn.Sequential(\n",
    "            nn.Conv1d(channels, channels, kernel_size=3, padding=1),  # 可以是更复杂的结构\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(channels),\n",
    "            nn.Conv1d(channels, 1, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        weights = self.attention_weights(x)\n",
    "        return x * weights\n",
    "\n",
    "class CNN1D(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(CNN1D, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=3, kernel_size=3, dilation=1, padding=1, stride=1)\n",
    "        self.attention1 = AttentionModule(3)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(in_channels=3, out_channels=8, kernel_size=3, dilation=1, padding=1, stride=1)\n",
    "        self.attention2 = AttentionModule(8)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=int(8 * input_dim / 4), out_features=128)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(x.shape[0], 1, x.shape[1])\n",
    "        x = self.attention1(self.conv1(x))\n",
    "        x = self.pool1(F.relu(x))\n",
    "        x = self.attention2(self.conv2(x))\n",
    "        x = self.pool2(F.relu(x))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        \n",
    "def train_model(embeddings_source, model_type=\"linear\", train_size=0.9):\n",
    "    \n",
    "    train_dataset = ProteinSequenceDataset(datatype=\"train\", embeddings_source = embeddings_source)\n",
    "    \n",
    "    train_set, val_set = random_split(train_dataset, lengths = [int(len(train_dataset)*train_size), len(train_dataset)-int(len(train_dataset)*train_size)])\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=config.batch_size, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_set, batch_size=config.batch_size, shuffle=True)\n",
    "\n",
    "    if model_type == \"linear\":\n",
    "        model = MultiLayerPerceptron(input_dim=embeds_dim[embeddings_source], num_classes=config.num_labels).to(config.device)\n",
    "    if model_type == \"convolutional\":\n",
    "        model = CNN1D(input_dim=embeds_dim[embeddings_source], num_classes=config.num_labels).to(config.device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = config.lr)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, factor=0.1, patience=1)\n",
    "    CrossEntropy = torch.nn.CrossEntropyLoss()\n",
    "    f1_score = MultilabelF1Score(num_labels=config.num_labels).to(config.device)\n",
    "    n_epochs = config.n_epochs\n",
    "\n",
    "    print(\"BEGIN TRAINING...\")\n",
    "    train_loss_history=[]\n",
    "    val_loss_history=[]\n",
    "    \n",
    "    train_f1score_history=[]\n",
    "    val_f1score_history=[]\n",
    "    for epoch in range(n_epochs):\n",
    "        print(\"EPOCH \", epoch+1)\n",
    "        ## TRAIN PHASE :\n",
    "        losses = []\n",
    "        scores = []\n",
    "        for embed, targets in tqdm(train_dataloader):\n",
    "            embed, targets = embed.to(config.device), targets.to(config.device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(embed)\n",
    "            loss= CrossEntropy(preds, targets)\n",
    "            score=f1_score(preds, targets)\n",
    "            losses.append(loss.item()) \n",
    "            scores.append(score.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        avg_loss = np.mean(losses)\n",
    "        avg_score = np.mean(scores)\n",
    "        print(\"Running Average TRAIN Loss : \", avg_loss)\n",
    "        print(\"Running Average TRAIN F1-Score : \", avg_score)\n",
    "        train_loss_history.append(avg_loss)\n",
    "        train_f1score_history.append(avg_score)\n",
    "        \n",
    "        ## VALIDATION PHASE : \n",
    "        losses = []\n",
    "        scores = []\n",
    "        for embed, targets in val_dataloader:\n",
    "            embed, targets = embed.to(config.device), targets.to(config.device)\n",
    "            preds = model(embed)\n",
    "            loss= CrossEntropy(preds, targets)\n",
    "            score=f1_score(preds, targets)\n",
    "            losses.append(loss.item())\n",
    "            scores.append(score.item())\n",
    "        avg_loss = np.mean(losses)\n",
    "        avg_score = np.mean(scores)\n",
    "        print(\"Running Average VAL Loss : \", avg_loss)\n",
    "        print(\"Running Average VAL F1-Score : \", avg_score)\n",
    "        val_loss_history.append(avg_loss)\n",
    "        val_f1score_history.append(avg_score)\n",
    "        \n",
    "        scheduler.step(avg_loss)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    print(\"TRAINING FINISHED\")\n",
    "    print(\"FINAL TRAINING SCORE : \", train_f1score_history[-1])\n",
    "    print(\"FINAL VALIDATION SCORE : \", val_f1score_history[-1])\n",
    "    \n",
    "    losses_history = {\"train\" : train_loss_history, \"val\" : val_loss_history}\n",
    "    scores_history = {\"train\" : train_f1score_history, \"val\" : val_f1score_history}\n",
    "    \n",
    "    return model, losses_history, scores_history\n",
    "\n",
    "ems2_model, ems2_losses, ems2_scores = train_model(embeddings_source=\"EMS2\",model_type=\"convolutional\")\n",
    "# ems2_model, ems2_losses, ems2_scores = train_model(embeddings_source=\"EMS2\",model_type=\"convolutional\")\n",
    "#t5_model, t5_losses, t5_scores = train_model(embeddings_source=\"T5\",model_type=\"linear\")\n",
    "#protbert_model, protbert_losses, protbert_scores = train_model(embeddings_source=\"ProtBERT\",model_type=\"linear\")\n",
    "\n",
    "plt.figure(figsize = (10, 4))\n",
    "#plt.plot(ems2_losses[\"val\"], label = \"EMS2\")\n",
    "#plt.plot(t5_losses[\"val\"], label = \"T5\")\n",
    "plt.plot(ems2_losses[\"val\"], label = \"EMS2\") \n",
    "plt.title(\"Validation Losses for # Vector Embeddings\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Average Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (10, 4))\n",
    "#plt.plot(ems2_scores[\"val\"], label = \"EMS2\")\n",
    "#plt.plot(t5_scores[\"val\"], label = \"T5\")\n",
    "plt.plot(ems2_scores[\"val\"], label = \"EMS2\")\n",
    "plt.title(\"Validation F1-Scores for # Vector Embeddings\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Average F1-Score\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54f5d5d6-de86-4ff8-98fe-d5298721e90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENERATE PREDICTION FOR TEST SET...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "141864it [02:00, 1175.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTIONS DONE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "70932000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(embeddings_source):\n",
    "    \n",
    "    test_dataset = ProteinSequenceDataset(datatype=\"test\", embeddings_source = embeddings_source)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "    \n",
    "    if embeddings_source == \"T5\":\n",
    "        model = t5_model\n",
    "    if embeddings_source == \"ProtBERT\":\n",
    "        model = protbert_model\n",
    "    if embeddings_source == \"EMS2\":\n",
    "        model = ems2_model\n",
    "        \n",
    "    model.eval()\n",
    "    \n",
    "    labels = pd.read_csv(config.train_labels_path, sep = \"\\t\")\n",
    "    top_terms = labels.groupby(\"term\")[\"EntryID\"].count().sort_values(ascending=False)\n",
    "    labels_names = top_terms[:config.num_labels].index.values\n",
    "    print(\"GENERATE PREDICTION FOR TEST SET...\")\n",
    "\n",
    "    ids_ = np.empty(shape=(len(test_dataloader)*config.num_labels,), dtype=object)\n",
    "    go_terms_ = np.empty(shape=(len(test_dataloader)*config.num_labels,), dtype=object)\n",
    "    confs_ = np.empty(shape=(len(test_dataloader)*config.num_labels,), dtype=np.float32)\n",
    "\n",
    "    for i, (embed, id) in tqdm(enumerate(test_dataloader)):\n",
    "        embed = embed.to(config.device)\n",
    "        confs_[i*config.num_labels:(i+1)*config.num_labels] = torch.nn.functional.sigmoid(model(embed)).squeeze().detach().cpu().numpy()\n",
    "        ids_[i*config.num_labels:(i+1)*config.num_labels] = id[0]\n",
    "        go_terms_[i*config.num_labels:(i+1)*config.num_labels] = labels_names\n",
    "\n",
    "    submission_df = pd.DataFrame(data={\"Id\" : ids_, \"GO term\" : go_terms_, \"Confidence\" : confs_})\n",
    "    print(\"PREDICTIONS DONE\")\n",
    "    return submission_df\n",
    "\n",
    "submission_df = predict(\"EMS2\")\n",
    "len(submission_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838ebc0d-3071-4f57-9a96-817d0b734b0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
